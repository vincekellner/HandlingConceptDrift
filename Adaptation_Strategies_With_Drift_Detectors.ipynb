{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides three different strategies how incremental drift can be handled by applying explicit drift detectors. Four different drift detectors are implemented: ADWIN, HDDDM, STEPD, MK. However, in this notebook only ADWIN & HDDDM are used.\n",
    "The strategies comprise: \n",
    "- 1) incremental training/updating of a model after a drift was detected \n",
    "- 2) training of a new model and discard old model\n",
    "- 3) a combination of incremental training and new training after drift was detected (referred to as \"Switching\")\n",
    "\n",
    "All strategies are applied with a custom feedforward MLP model which was trained to predict taxi demand in different areas in New York City at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:44:03.290048Z",
     "start_time": "2020-01-23T11:44:03.282942Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#import keras specific functions for storing and loading models\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "#load custom deep Models (MLP)\n",
    "from complex_mlp import ComplexMLP\n",
    "\n",
    "#import custom functions to store all kinds of results on disk:\n",
    "import save_files_collection as sv_files\n",
    "\n",
    "#import all functions to perform drift detection & retraining\n",
    "import drift_retraining_collection as dft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T10:12:16.843651Z",
     "start_time": "2020-01-23T10:12:11.686741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 date  1  2  3    4  5  6   7  8  9  ...  254  255  256  257  \\\n",
      "0 2009-01-01 05:00:00  0  0  0   91  0  0  30  0  0  ...    0   50   39    3   \n",
      "1 2009-01-01 06:00:00  1  0  0  105  0  0  62  0  0  ...    0   77   67    5   \n",
      "2 2009-01-01 07:00:00  0  0  0   96  0  0  79  0  0  ...    0   90   83    4   \n",
      "3 2009-01-01 08:00:00  0  0  0   91  0  0  84  0  0  ...    0   54   77    3   \n",
      "4 2009-01-01 09:00:00  2  0  0   82  0  0  85  0  1  ...    0   66   54    4   \n",
      "\n",
      "   258  259  260  261  262  263  \n",
      "0    1    0    3   52  127  326  \n",
      "1    0    0   15   65  166  476  \n",
      "2    0    0   19   39  125  460  \n",
      "3    1    0   19   54   79  313  \n",
      "4    0    0   13   24   47  224  \n",
      "\n",
      "[5 rows x 264 columns]\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "TRAIN_PATH = '/media/...'\n",
    "Store_PATH = '/media/...'\n",
    "file_final = 'preprocessed_data.csv'\n",
    "\n",
    "df_m = pd.read_csv(TRAIN_PATH + file_final, header=0)\n",
    "\n",
    "#convert to datetime format:\n",
    "df_m['date'] = pd.to_datetime(df_m['date'], utc=True)\n",
    "df_m['date'] = df_m['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_m['date'] = pd.to_datetime(df_m['date'])\n",
    "#df_m = df_m.set_index(\"date\") -> set index later, since we need \"date\" column to find highest demand columns..\n",
    "print(df_m.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T10:12:16.935716Z",
     "start_time": "2020-01-23T10:12:16.930353Z"
    }
   },
   "outputs": [],
   "source": [
    "'''filter areas with highest demand '''\n",
    "#get time series with highest \"demand patterns\":\n",
    "\n",
    "#function filters nlargest areas:\n",
    "def get_nlargest_areas(nlargest, org_dataset = df_m):\n",
    "    \n",
    "    #get time series with highest \"demand patterns\":\n",
    "    df_sum = org_dataset.copy(deep=True).drop(columns=[\"date\"],axis=1)\n",
    "    df_sum = df_sum.sum(axis=0,numeric_only=True)\n",
    "\n",
    "    #store nlargest values:\n",
    "    df_sum = df_sum.nlargest(nlargest) \n",
    "    idx_filter = list(df_sum.index.values)\n",
    "    #append \"date\" column\n",
    "    idx_filter.append(\"date\")\n",
    "\n",
    "    del df_sum\n",
    "    \n",
    "    #filter columns with largest values:\n",
    "    ts_largest = org_dataset[idx_filter].copy(deep=True)\n",
    "    ts_largest = ts_largest.set_index(\"date\")\n",
    "\n",
    "    #shift datetimeindex to use local NYC time not UTC:\n",
    "    ts_largest.index = ts_largest.index.shift(-5,freq='H')\n",
    "\n",
    "    return ts_largest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T10:12:17.694787Z",
     "start_time": "2020-01-23T10:12:16.972574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 largest areas:  (83231, 20)\n",
      "10 largest areas:  (83231, 10)\n"
     ]
    }
   ],
   "source": [
    "ts_20largest = get_nlargest_areas(20)\n",
    "ts_10largest = get_nlargest_areas(10)\n",
    "\n",
    "print('20 largest areas: ', ts_20largest.shape)\n",
    "print('10 largest areas: ', ts_10largest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models from Disk (trained on 2009-2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T08:56:44.171522Z",
     "start_time": "2019-11-02T08:56:44.161050Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model_from_disk(model_type):\n",
    "    \n",
    "    '''\n",
    "    Returns pre-trained model from disk\n",
    "    '''\n",
    "\n",
    "\n",
    "    model_PATH = '/media/...'    \n",
    "    #complex MLP:\n",
    "    model_architecture_complex_MLP_PATH = '/media/...'   \n",
    "    complex_MLP_model_file = 'complex_MLP_early_stopping_W168_20areas__y2012.json'\n",
    "    complex_MLP_weights = 'complex_MLP_early_stopping_W168_20areas__y2012_weights.h5'\n",
    "    \n",
    "    \n",
    "    #multivar LSTM without additional features:\n",
    "    model_architecture_multivar_20 = '/media/...'   \n",
    "    multivar_LSTM_file = 'Model_Architectures_multivar20/multivariate_stacked_lstm_non_st_model_2H_256_32_batch512_drop03_clip_norm_shuffle_scaling_tanh_W168_20largest_areas__y2012.json'\n",
    "    multivar_LSTM_weights = 'multivariate_stacked_lstm_non_st_model_2H_256_32_batch512_drop03_clip_norm_shuffle_scaling_tanh_W168_20largest_areas__y2012_weights.h5'\n",
    "\n",
    "    \n",
    "    \n",
    "    #multivar LSTM with additional features:\n",
    "    #prepare files for models:\n",
    "    model_architecture_multivar_20 = '/media/...'   \n",
    "    multivar_LSTM_feat_file = 'Model_Architectures_multivar20/multivariate_stacked_lstm_non_st_model_2H_256_128_batch512_drop03_clip_norm_shuffle_scaling_tanh_encoded_lag_feats_added_W168_20largest_areas__y2012.json'\n",
    "    multivar_LSTM_feat_weights = 'multivariate_stacked_lstm_non_st_model_2H_256_128_batch512_drop03_clip_norm_shuffle_scaling_tanh_encoded_lag_feats_added_W168_20largest_areas__y2012_weights.h5'\n",
    "\n",
    "    \n",
    "    \n",
    "    #XGBoost 2year n_estimators=2000:\n",
    "    multivar_XGBoost_2000_model_file = 'multivar_XGBoost_est2000_maxdep7_s1_2009_e12_2010_2year_train'\n",
    "    model_architecture_multivar_XGBoost_path = '/media/...'   \n",
    "    \n",
    "    model_architecture_multivar_XGBoost_path_new = '/media/...'   \n",
    "    \n",
    "    #XGBoost 2year n_estimators=1000:\n",
    "    multivar_XGBoost_1000_model_file = 'multivar_XGBoost_est1000_maxdep7_20largest_areas_s1_2009_e12_2010_2year_train.pickle.dat'\n",
    "\n",
    "    \n",
    "    instances_dict = {'SingleMLP': (),\n",
    "                  'SingleLSTM': (),\n",
    "                  'ComplexMLP': (complex_MLP_model_file, model_architecture_complex_MLP_PATH, complex_MLP_weights),\n",
    "                  'MultivarLSTM': (multivar_LSTM_file, model_architecture_multivar_20, multivar_LSTM_weights),\n",
    "                  'MultivarLSTM_lag_feat': (multivar_LSTM_feat_file, model_architecture_multivar_20, multivar_LSTM_feat_weights),\n",
    "                  'MultivarXGBoost_2year_est2000': (multivar_XGBoost_2000_model_file, model_architecture_multivar_XGBoost_path),\n",
    "                  'MultivarXGBoost_2year_est1000': (multivar_XGBoost_1000_model_file, model_architecture_multivar_XGBoost_path_new)\n",
    "\n",
    "                 }\n",
    "    \n",
    "       \n",
    "    if 'XGBoost' not in model_type:\n",
    "\n",
    "        #load complexMLP model 20largest areas:\n",
    "        json_file = open(model_PATH + instances_dict[model_type][0], 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        prediction_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        #load weights of best model:\n",
    "        prediction_model.load_weights(instances_dict[model_type][1] + instances_dict[model_type][2])\n",
    "\n",
    "    \n",
    "    else:\n",
    "        model_PATH = instances_dict[model_type][1]\n",
    "\n",
    "        model_name = instances_dict[model_type][0]\n",
    "\n",
    "        #load model:\n",
    "        file_to_load = model_PATH + model_name\n",
    "\n",
    "        #load model into dict:\n",
    "        prediction_model = pickle.load(open(file_to_load, \"rb\"))\n",
    "        \n",
    "     \n",
    "    \n",
    "    return prediction_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function to call multiple instances of a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T10:22:34.167729Z",
     "start_time": "2020-01-23T10:22:34.162051Z"
    }
   },
   "outputs": [],
   "source": [
    "#function needed to create new instances, otherwise same instances is used!!\n",
    "def create_model_instance(model_type, n_epochs_init = 150):\n",
    "    \n",
    "    instances_dict = {'ComplexMLP': ComplexMLP(n_epochs = n_epochs_init),\n",
    "                      \n",
    "                     }\n",
    "    \n",
    "    return instances_dict[model_type]\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADWIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: training of a new model if drift is detected & discarding of old model\n",
    "- Params for test purpose only: number of epochs = 20 instead of 150, end of dataset: 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:06:01.393279Z",
     "start_time": "2020-01-23T10:32:24.382422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 21us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 42us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.25613309572718]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16265807091495166\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16265807091495166\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2392759732209273\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.23927597322092736\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25663277956855934\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2566327795685594\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24101165385569057\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24101165385569057\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1643937515497148\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16439375154971486\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2204314406149268\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22043144061492687\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1643937515497148\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16439375154971486\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17877510538060992\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17877510538060998\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20133895363253163\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20133895363253163\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27919662782048105\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27919662782048105\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21596826183982143\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21596826183982148\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17827919662782044\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1782791966278205\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19687577485742624\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19687577485742624\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20009918175055785\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2000991817505579\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20505826927845272\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20505826927845278\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18770146293082068\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18770146293082074\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15100421522439866\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15100421522439872\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17728737912224146\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17728737912224152\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.36523679642945694\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.365236796429457\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17381601785271505\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1738160178527151\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          1          1          0         0          0   \n",
      "2011-01-01 01:00:00          0          0          0         1          0   \n",
      "2011-01-01 02:00:00          1          0          0         0          0   \n",
      "2011-01-01 03:00:00          1          1          0         0          0   \n",
      "2011-01-01 04:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          0          0          0         0          0   \n",
      "2011-01-01 01:00:00          0          0          0         1          0   \n",
      "2011-01-01 02:00:00          0          1          0         1          1   \n",
      "2011-01-01 03:00:00          0          0          0         0          0   \n",
      "2011-01-01 04:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          0          0          0         0          0   \n",
      "2011-01-01 01:00:00          0          1          1         0          0   \n",
      "2011-01-01 02:00:00          0          1          0         1          0   \n",
      "2011-01-01 03:00:00          0          0          0         1          1   \n",
      "2011-01-01 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-01-01 00:00:00          0          0          0          0         0  \n",
      "2011-01-01 01:00:00          1          0          0          1         0  \n",
      "2011-01-01 02:00:00          1          1          1          1         1  \n",
      "2011-01-01 03:00:00          0          1          1          1         0  \n",
      "2011-01-01 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          0          1          0         1          1   \n",
      "2011-06-17 21:00:00          0          0          0         1          1   \n",
      "2011-06-17 22:00:00          1          0          1         1          0   \n",
      "2011-06-17 23:00:00          1          0          1         1          1   \n",
      "2011-06-18 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          1          1          1         1          0   \n",
      "2011-06-17 21:00:00          1          1          1         1          1   \n",
      "2011-06-17 22:00:00          0          1          1         1          0   \n",
      "2011-06-17 23:00:00          1          1          1         1          1   \n",
      "2011-06-18 00:00:00          0          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          0          0          1         1          0   \n",
      "2011-06-17 21:00:00          0          1          1         1          0   \n",
      "2011-06-17 22:00:00          1          1          0         1          1   \n",
      "2011-06-17 23:00:00          1          1          0         1          1   \n",
      "2011-06-18 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-06-17 20:00:00          1          0          1          1         1  \n",
      "2011-06-17 21:00:00          0          1          1          1         1  \n",
      "2011-06-17 22:00:00          1          1          1          0         1  \n",
      "2011-06-17 23:00:00          1          1          1          0         1  \n",
      "2011-06-18 00:00:00          0          0          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Change detected in area binary170, index: 1631\n",
      "date: 2011-03-09 23:00:00\n",
      "Drift detected at:  2011-03-09 23:00:00\n",
      ">> Current Time:  23/01/2020 11:32:48\n",
      "## ++ previous detected dates:  [Timestamp('2011-01-01 00:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  730\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-03-09 23:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count1__trainsize730_s3_2009_e3_2011__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-03-09 23:00:00\n",
      ">last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2009-03-09 23:00:00\n",
      "last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2009-03-09 23:00:00\n",
      "last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16848, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16848,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (336960, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336960 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.3659 - mean_absolute_error: 0.4350 - val_loss: 0.6912 - val_mean_absolute_error: 0.6583\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2911 - mean_absolute_error: 0.3863 - val_loss: 0.6283 - val_mean_absolute_error: 0.5799\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2719 - mean_absolute_error: 0.3727 - val_loss: 0.6469 - val_mean_absolute_error: 0.6446\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2609 - mean_absolute_error: 0.3644 - val_loss: 0.6643 - val_mean_absolute_error: 0.6022\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2538 - mean_absolute_error: 0.3592 - val_loss: 0.6567 - val_mean_absolute_error: 0.5766\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2489 - mean_absolute_error: 0.3554 - val_loss: 0.6135 - val_mean_absolute_error: 0.5506\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2431 - mean_absolute_error: 0.3514 - val_loss: 0.6734 - val_mean_absolute_error: 0.5545\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2408 - mean_absolute_error: 0.3498 - val_loss: 0.6905 - val_mean_absolute_error: 0.5464\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2367 - mean_absolute_error: 0.3469 - val_loss: 0.6646 - val_mean_absolute_error: 0.5785\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2343 - mean_absolute_error: 0.3451 - val_loss: 0.6686 - val_mean_absolute_error: 0.5727\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2326 - mean_absolute_error: 0.3440 - val_loss: 0.6027 - val_mean_absolute_error: 0.5452\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2305 - mean_absolute_error: 0.3426 - val_loss: 0.6584 - val_mean_absolute_error: 0.5320\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2291 - mean_absolute_error: 0.3416 - val_loss: 0.6229 - val_mean_absolute_error: 0.5603\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2266 - mean_absolute_error: 0.3401 - val_loss: 0.6472 - val_mean_absolute_error: 0.5887\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2256 - mean_absolute_error: 0.3394 - val_loss: 0.6738 - val_mean_absolute_error: 0.6198\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2239 - mean_absolute_error: 0.3381 - val_loss: 0.6486 - val_mean_absolute_error: 0.6256\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2235 - mean_absolute_error: 0.3379 - val_loss: 0.6626 - val_mean_absolute_error: 0.5533\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2227 - mean_absolute_error: 0.3374 - val_loss: 0.6503 - val_mean_absolute_error: 0.5157\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2223 - mean_absolute_error: 0.3368 - val_loss: 0.5646 - val_mean_absolute_error: 0.5027\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2215 - mean_absolute_error: 0.3365 - val_loss: 0.5993 - val_mean_absolute_error: 0.5389\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  1\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00'), Timestamp('2011-03-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 22us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.998517665137136]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1678729037952339\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1678729037952339\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2510150044130627\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2510150044130627\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24942630185348635\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24942630185348633\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24430714916151808\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2443071491615181\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15728155339805827\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15728155339805824\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22453662842012356\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22453662842012356\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1698146513680494\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16981465136804944\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1747572815533981\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17475728155339806\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20688437775816415\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20688437775816418\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2681376875551633\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26813768755516326\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21129744042365406\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.211297440423654\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1721094439541041\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17210944395410416\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.18940864960282433\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.18940864960282436\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1966460723742277\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1966460723742277\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19717563989408649\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19717563989408649\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18517210944395412\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1851721094439541\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15622241835834072\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1562224183583407\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.16946160635481022\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.16946160635481025\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358340688437775\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358340688437775\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1657546337157988\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16575463371579877\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          1          0          0         1          1   \n",
      "2011-03-10 01:00:00          1          0          0         0          1   \n",
      "2011-03-10 02:00:00          1          1          1         0          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          1          0          1         1          0   \n",
      "2011-03-10 01:00:00          1          1          1         1          1   \n",
      "2011-03-10 02:00:00          1          0          1         1          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          0          0          1         1          1   \n",
      "2011-03-10 01:00:00          1          1          1         1          1   \n",
      "2011-03-10 02:00:00          1          1          1         1          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-03-10 00:00:00          0          1          1          0         1  \n",
      "2011-03-10 01:00:00          1          0          0          0         1  \n",
      "2011-03-10 02:00:00          1          1          1          1         1  \n",
      "2011-03-10 03:00:00          1          1          1          1         1  \n",
      "2011-03-10 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          1          0         1          1   \n",
      "2011-08-24 21:00:00          1          1          0         1          0   \n",
      "2011-08-24 22:00:00          1          1          0         1          0   \n",
      "2011-08-24 23:00:00          1          1          1         1          1   \n",
      "2011-08-25 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          0          1         1          0   \n",
      "2011-08-24 21:00:00          1          1          1         1          1   \n",
      "2011-08-24 22:00:00          1          1          1         1          0   \n",
      "2011-08-24 23:00:00          1          0          0         1          1   \n",
      "2011-08-25 00:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          1          1         1          1   \n",
      "2011-08-24 21:00:00          1          1          1         1          1   \n",
      "2011-08-24 22:00:00          1          1          1         1          1   \n",
      "2011-08-24 23:00:00          0          1          1         1          1   \n",
      "2011-08-25 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-08-24 20:00:00          1          1          1          1         1  \n",
      "2011-08-24 21:00:00          1          1          1          0         1  \n",
      "2011-08-24 22:00:00          1          0          1          0         1  \n",
      "2011-08-24 23:00:00          1          1          0          0         1  \n",
      "2011-08-25 00:00:00          1          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary237, index: 895\n",
      "date: 2011-04-16 07:00:00\n",
      "Drift detected at:  2011-04-16 07:00:00\n",
      ">> Current Time:  23/01/2020 11:34:56\n",
      "## ++ previous detected dates:  [Timestamp('2011-03-09 23:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  730\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-04-16 07:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count3__trainsize730_s4_2009_e4_2011__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-04-16 07:00:00\n",
      ">last_train_set_year:  2011-04-16 07:00:00\n",
      "start_validation_set_year:  2011-04-16 07:00:00\n",
      "end_validation_set_year:  2011-04-16 07:00:00\n",
      "start_test_set_year:  2011-04-16 07:00:00\n",
      "end_test_set_year:  2011-04-16 07:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2009-04-16 07:00:00\n",
      "last_train_set_year:  2011-04-16 07:00:00\n",
      "start_validation_set_year:  2011-04-16 07:00:00\n",
      "start_test_set_year:  2011-04-16 07:00:00\n",
      "end_validation_set_year:  2011-04-16 07:00:00\n",
      "end_test_set_year:  2011-04-16 07:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# adjusted dates..\n",
      "start_train_year:  2009-04-16 07:00:00\n",
      "last_train_set_year:  2011-04-16 07:00:00\n",
      "start_validation_set_year:  2011-04-16 07:00:00\n",
      "start_test_set_year:  2011-04-16 07:00:00\n",
      "end_validation_set_year:  2011-04-16 07:00:00\n",
      "end_test_set_year:  2011-04-16 07:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16848, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16848,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (336960, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336960 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "336960/336960 [==============================] - 5s 15us/step - loss: 0.3793 - mean_absolute_error: 0.4410 - val_loss: 0.0856 - val_mean_absolute_error: 0.2410\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2938 - mean_absolute_error: 0.3892 - val_loss: 0.0689 - val_mean_absolute_error: 0.2205\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2764 - mean_absolute_error: 0.3764 - val_loss: 0.0645 - val_mean_absolute_error: 0.2037\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2651 - mean_absolute_error: 0.3681 - val_loss: 0.0666 - val_mean_absolute_error: 0.2039\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2569 - mean_absolute_error: 0.3620 - val_loss: 0.0582 - val_mean_absolute_error: 0.1941\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2511 - mean_absolute_error: 0.3578 - val_loss: 0.0592 - val_mean_absolute_error: 0.1937\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2466 - mean_absolute_error: 0.3544 - val_loss: 0.0634 - val_mean_absolute_error: 0.1932\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2414 - mean_absolute_error: 0.3511 - val_loss: 0.0603 - val_mean_absolute_error: 0.2005\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2380 - mean_absolute_error: 0.3485 - val_loss: 0.0727 - val_mean_absolute_error: 0.2137\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2351 - mean_absolute_error: 0.3463 - val_loss: 0.0646 - val_mean_absolute_error: 0.2012\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2324 - mean_absolute_error: 0.3446 - val_loss: 0.0588 - val_mean_absolute_error: 0.1870\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2301 - mean_absolute_error: 0.3429 - val_loss: 0.0634 - val_mean_absolute_error: 0.2076\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2285 - mean_absolute_error: 0.3418 - val_loss: 0.0665 - val_mean_absolute_error: 0.2217\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2274 - mean_absolute_error: 0.3412 - val_loss: 0.0541 - val_mean_absolute_error: 0.1857\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2262 - mean_absolute_error: 0.3402 - val_loss: 0.0571 - val_mean_absolute_error: 0.1962\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2252 - mean_absolute_error: 0.3397 - val_loss: 0.0571 - val_mean_absolute_error: 0.1925\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2237 - mean_absolute_error: 0.3387 - val_loss: 0.0565 - val_mean_absolute_error: 0.1921\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2226 - mean_absolute_error: 0.3378 - val_loss: 0.0504 - val_mean_absolute_error: 0.1825\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2226 - mean_absolute_error: 0.3382 - val_loss: 0.0527 - val_mean_absolute_error: 0.1887\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2209 - mean_absolute_error: 0.3372 - val_loss: 0.0705 - val_mean_absolute_error: 0.2253\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  2\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-04-16 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 24us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 45us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.964813368117554]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1716201798506325\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17162017985063252\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24919981710105166\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24919981710105166\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24752324340801712\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24752324340801707\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24203627495808566\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24203627495808566\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16034141137021796\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16034141137021796\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22252705380277393\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22252705380277396\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1716201798506325\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17162017985063252\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1763450693491846\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17634506934918456\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2089620484682213\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2089620484682213\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2716049382716049\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2716049382716049\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21109586953208348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  142\n",
      "Share of wrongly classified observations:  0.21109586953208354\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1800030483158055\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18000304831580552\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19387288523090995\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19387288523090992\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1970736168267032\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19707361682670324\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2014936747447036\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20149367474470356\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18533760097546104\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18533760097546106\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15622618503276942\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1562261850327694\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1775643956713916\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17756439567139157\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38454503886602653\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38454503886602653\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17207742722146013\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17207742722146013\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-04-16 08:00:00          1          1          1         1          0   \n",
      "2011-04-16 09:00:00          1          1          0         1          1   \n",
      "2011-04-16 10:00:00          0          1          1         1          1   \n",
      "2011-04-16 11:00:00          1          0          0         0          1   \n",
      "2011-04-16 12:00:00          0          0          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-04-16 08:00:00          1          0          1         1          1   \n",
      "2011-04-16 09:00:00          1          1          0         1          1   \n",
      "2011-04-16 10:00:00          0          1          0         1          0   \n",
      "2011-04-16 11:00:00          0          1          1         1          0   \n",
      "2011-04-16 12:00:00          1          0          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-04-16 08:00:00          1          0          1         1          0   \n",
      "2011-04-16 09:00:00          0          1          1         1          1   \n",
      "2011-04-16 10:00:00          0          1          1         0          1   \n",
      "2011-04-16 11:00:00          1          1          1         1          1   \n",
      "2011-04-16 12:00:00          1          1          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-04-16 08:00:00          1          1          1          0         1  \n",
      "2011-04-16 09:00:00          1          0          1          0         0  \n",
      "2011-04-16 10:00:00          0          1          1          0         0  \n",
      "2011-04-16 11:00:00          1          1          1          0         1  \n",
      "2011-04-16 12:00:00          1          1          0          1         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-01 04:00:00          1          1          1         0          1   \n",
      "2011-10-01 05:00:00          1          1          1         1          1   \n",
      "2011-10-01 06:00:00          1          1          1         0          0   \n",
      "2011-10-01 07:00:00          1          1          1         1          1   \n",
      "2011-10-01 08:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-01 04:00:00          1          1          1         1          1   \n",
      "2011-10-01 05:00:00          1          1          1         1          1   \n",
      "2011-10-01 06:00:00          1          1          1         0          0   \n",
      "2011-10-01 07:00:00          0          1          1         1          1   \n",
      "2011-10-01 08:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-01 04:00:00          1          1          1         1          1   \n",
      "2011-10-01 05:00:00          1          1          1         1          1   \n",
      "2011-10-01 06:00:00          1          1          1         1          1   \n",
      "2011-10-01 07:00:00          1          1          1         1          1   \n",
      "2011-10-01 08:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-01 04:00:00          1          1          0          1         1  \n",
      "2011-10-01 05:00:00          1          1          1          1         1  \n",
      "2011-10-01 06:00:00          1          0          1          1         1  \n",
      "2011-10-01 07:00:00          1          1          1          0         1  \n",
      "2011-10-01 08:00:00          0          1          0          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-10-01 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 42us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.702679635480195]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1719167904903418\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17191679049034175\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24947994056463596\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24947994056463596\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24710252600297178\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24710252600297178\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24026745913818726\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24026745913818723\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16121842496285288\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1612184249628529\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2240713224368499\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22407132243684993\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17117384843982164\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1711738484398217\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17622585438335814\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1762258543833581\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20757800891530465\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20757800891530462\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27147102526002975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  186\n",
      "Share of wrongly classified observations:  0.2714710252600297\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21188707280832098\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21188707280832095\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18023774145616644\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18023774145616642\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19316493313521543\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19316493313521546\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19747399702823176\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1974739970282318\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20297176820208018\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20297176820208024\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18588410104011888\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18588410104011888\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15705794947994056\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15705794947994056\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17667161961367017\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17667161961367014\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38499257057949476\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3849925705794948\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17280832095096588\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17280832095096582\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-01 09:00:00          1          1          1         1          1   \n",
      "2011-10-01 10:00:00          0          1          1         0          1   \n",
      "2011-10-01 11:00:00          1          1          1         1          1   \n",
      "2011-10-01 12:00:00          1          1          1         1          1   \n",
      "2011-10-01 13:00:00          1          0          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-01 09:00:00          1          0          1         1          1   \n",
      "2011-10-01 10:00:00          1          1          0         1          1   \n",
      "2011-10-01 11:00:00          1          1          0         1          0   \n",
      "2011-10-01 12:00:00          0          1          0         1          1   \n",
      "2011-10-01 13:00:00          0          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-01 09:00:00          1          1          1         1          1   \n",
      "2011-10-01 10:00:00          1          0          1         0          1   \n",
      "2011-10-01 11:00:00          1          0          1         1          1   \n",
      "2011-10-01 12:00:00          1          1          1         1          0   \n",
      "2011-10-01 13:00:00          0          0          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-01 09:00:00          1          1          1          1         1  \n",
      "2011-10-01 10:00:00          1          0          1          0         1  \n",
      "2011-10-01 11:00:00          0          1          1          0         1  \n",
      "2011-10-01 12:00:00          1          0          0          0         0  \n",
      "2011-10-01 13:00:00          0          0          1          0         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-08 05:00:00          1          1          1         0          1   \n",
      "2011-10-08 06:00:00          1          1          1         1          1   \n",
      "2011-10-08 07:00:00          1          1          1         1          0   \n",
      "2011-10-08 08:00:00          0          0          0         1          1   \n",
      "2011-10-08 09:00:00          0          1          1         1          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-08 05:00:00          1          1          1         0          1   \n",
      "2011-10-08 06:00:00          1          1          1         1          1   \n",
      "2011-10-08 07:00:00          1          1          1         1          1   \n",
      "2011-10-08 08:00:00          1          1          1         1          0   \n",
      "2011-10-08 09:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-08 05:00:00          1          1          1         1          1   \n",
      "2011-10-08 06:00:00          1          1          1         1          1   \n",
      "2011-10-08 07:00:00          1          1          1         1          0   \n",
      "2011-10-08 08:00:00          1          1          1         1          0   \n",
      "2011-10-08 09:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-08 05:00:00          1          1          1          1         1  \n",
      "2011-10-08 06:00:00          1          1          1          1         1  \n",
      "2011-10-08 07:00:00          1          1          1          1         1  \n",
      "2011-10-08 08:00:00          1          1          1          1         1  \n",
      "2011-10-08 09:00:00          1          0          1          1         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-10-08 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 39us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.628772594713936]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17132917814175963\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17132917814175969\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2491665458762139\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24916654587621395\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2458327293810697\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24583272938106973\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23858530221771268\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2385853022177127\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16161762574286131\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1616176257428613\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22264096245832732\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2226409624583273\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1700246412523554\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17002464125235542\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17538773735323965\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1753877373532396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  48\n",
      "Share of wrongly classified observations:  0.2062617770691405\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20626177706914045\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2704739817364835\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27047398173648357\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2120597187998261\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21205971879982607\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17959124510798663\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17959124510798666\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19336135671836496\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.193361356718365\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19756486447311206\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19756486447311206\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20263806348746194\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20263806348746194\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18364980431946654\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1836498043194666\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15567473546890853\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15567473546890853\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17640237715610962\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17640237715610957\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38469343383099\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38469343383099\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1723438179446296\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17234381794462966\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-08 10:00:00          1          1          1         1          1   \n",
      "2011-10-08 11:00:00          0          1          0         1          0   \n",
      "2011-10-08 12:00:00          1          1          1         1          1   \n",
      "2011-10-08 13:00:00          1          0          1         1          1   \n",
      "2011-10-08 14:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-08 10:00:00          1          1          1         1          0   \n",
      "2011-10-08 11:00:00          1          0          1         1          1   \n",
      "2011-10-08 12:00:00          1          0          1         1          1   \n",
      "2011-10-08 13:00:00          1          1          1         1          1   \n",
      "2011-10-08 14:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-08 10:00:00          1          1          1         1          1   \n",
      "2011-10-08 11:00:00          1          1          1         0          0   \n",
      "2011-10-08 12:00:00          0          1          1         1          1   \n",
      "2011-10-08 13:00:00          0          1          1         1          1   \n",
      "2011-10-08 14:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-08 10:00:00          0          1          1          1         1  \n",
      "2011-10-08 11:00:00          1          0          0          0         1  \n",
      "2011-10-08 12:00:00          1          1          0          0         1  \n",
      "2011-10-08 13:00:00          0          1          1          1         1  \n",
      "2011-10-08 14:00:00          0          1          0          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-15 06:00:00          1          1          1         1          1   \n",
      "2011-10-15 07:00:00          1          0          0         0          0   \n",
      "2011-10-15 08:00:00          0          1          0         1          0   \n",
      "2011-10-15 09:00:00          1          0          1         0          1   \n",
      "2011-10-15 10:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-15 06:00:00          1          1          1         0          1   \n",
      "2011-10-15 07:00:00          1          0          1         1          0   \n",
      "2011-10-15 08:00:00          1          1          1         0          0   \n",
      "2011-10-15 09:00:00          0          1          1         1          1   \n",
      "2011-10-15 10:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-15 06:00:00          1          1          1         1          0   \n",
      "2011-10-15 07:00:00          0          0          1         1          0   \n",
      "2011-10-15 08:00:00          0          1          1         0          0   \n",
      "2011-10-15 09:00:00          1          1          1         1          0   \n",
      "2011-10-15 10:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-15 06:00:00          1          1          1          1         1  \n",
      "2011-10-15 07:00:00          1          0          0          1         0  \n",
      "2011-10-15 08:00:00          1          1          1          1         1  \n",
      "2011-10-15 09:00:00          1          1          1          0         1  \n",
      "2011-10-15 10:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-10-15 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 43us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[65.95412841091088]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17034521788341828\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17034521788341822\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24915110356536507\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24915110356536502\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24731182795698925\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24731182795698925\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23839841539332196\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23839841539332202\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.161431805319751\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.161431805319751\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22368421052631582\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2236842105263158\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16949632144878324\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16949632144878324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  234\n",
      "Share of wrongly classified observations:  0.1762874929258631\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17628749292586304\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20741369552914546\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20741369552914543\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2705149971703452\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2705149971703452\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21123372948500285\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21123372948500282\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17968307866440292\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17968307866440295\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19170911148839842\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19170911148839842\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1977928692699491\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19779286926994907\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2041595925297114\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2041595925297114\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18336162988115445\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1833616298811545\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15478211658177699\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.154782116581777\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17713638936049803\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17713638936049803\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3848330503678551\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38483305036785515\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17232597623089985\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17232597623089982\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-15 11:00:00          1          1          0         1          1   \n",
      "2011-10-15 12:00:00          1          1          1         1          1   \n",
      "2011-10-15 13:00:00          1          1          1         1          1   \n",
      "2011-10-15 14:00:00          1          1          0         1          0   \n",
      "2011-10-15 15:00:00          1          1          1         0          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-15 11:00:00          0          1          1         0          1   \n",
      "2011-10-15 12:00:00          0          1          1         1          0   \n",
      "2011-10-15 13:00:00          1          0          1         1          1   \n",
      "2011-10-15 14:00:00          1          1          1         1          1   \n",
      "2011-10-15 15:00:00          0          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-15 11:00:00          1          1          1         1          1   \n",
      "2011-10-15 12:00:00          1          1          1         1          1   \n",
      "2011-10-15 13:00:00          1          1          1         1          0   \n",
      "2011-10-15 14:00:00          1          1          1         1          1   \n",
      "2011-10-15 15:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-15 11:00:00          1          1          0          1         1  \n",
      "2011-10-15 12:00:00          1          0          1          0         1  \n",
      "2011-10-15 13:00:00          1          1          1          0         1  \n",
      "2011-10-15 14:00:00          1          1          0          1         1  \n",
      "2011-10-15 15:00:00          1          0          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-22 07:00:00          0          1          0         0          0   \n",
      "2011-10-22 08:00:00          1          1          0         1          1   \n",
      "2011-10-22 09:00:00          1          1          1         0          1   \n",
      "2011-10-22 10:00:00          1          1          1         1          1   \n",
      "2011-10-22 11:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-22 07:00:00          1          0          1         1          1   \n",
      "2011-10-22 08:00:00          1          1          0         1          0   \n",
      "2011-10-22 09:00:00          1          1          1         1          1   \n",
      "2011-10-22 10:00:00          1          1          1         1          1   \n",
      "2011-10-22 11:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-22 07:00:00          1          0          1         1          0   \n",
      "2011-10-22 08:00:00          1          0          1         1          0   \n",
      "2011-10-22 09:00:00          1          0          0         1          1   \n",
      "2011-10-22 10:00:00          1          1          1         1          1   \n",
      "2011-10-22 11:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-22 07:00:00          1          0          0          1         1  \n",
      "2011-10-22 08:00:00          1          1          1          1         1  \n",
      "2011-10-22 09:00:00          1          1          0          1         1  \n",
      "2011-10-22 10:00:00          1          1          1          0         1  \n",
      "2011-10-22 11:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-10-22 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 43us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.72299573942348]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17078900096725158\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17078900096725164\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24844548846206993\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24844548846206993\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24623462760812487\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24623462760812492\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23835843581594585\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23835843581594585\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16111648473124218\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16111648473124224\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2241260190686749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  162\n",
      "Share of wrongly classified observations:  0.22412601906867485\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1699599281470222\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16995992814702224\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17645433190548565\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1764543319054857\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20643913223711485\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20643913223711483\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26986320298466215\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26986320298466215\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21086085394500487\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21086085394500484\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1800469807931463\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18004698079314632\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19193035788310076\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19193035788310073\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19621390078761913\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1962139007876192\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2032610197595689\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2032610197595689\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18363962968080694\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18363962968080697\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15517479618626506\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15517479618626503\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1752107226751416\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17521072267514162\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358435815945835\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358435815945835\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17272350421445348\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1727235042144535\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-22 12:00:00          1          1          1         1          1   \n",
      "2011-10-22 13:00:00          1          1          1         1          1   \n",
      "2011-10-22 14:00:00          1          1          1         1          0   \n",
      "2011-10-22 15:00:00          0          1          1         0          0   \n",
      "2011-10-22 16:00:00          0          1          1         0          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-22 12:00:00          0          1          1         1          1   \n",
      "2011-10-22 13:00:00          1          0          1         1          0   \n",
      "2011-10-22 14:00:00          1          1          1         1          1   \n",
      "2011-10-22 15:00:00          0          1          0         1          1   \n",
      "2011-10-22 16:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-22 12:00:00          1          1          1         1          1   \n",
      "2011-10-22 13:00:00          1          1          1         1          1   \n",
      "2011-10-22 14:00:00          1          1          0         1          1   \n",
      "2011-10-22 15:00:00          1          1          1         1          1   \n",
      "2011-10-22 16:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-22 12:00:00          0          1          1          1         1  \n",
      "2011-10-22 13:00:00          1          1          1          0         1  \n",
      "2011-10-22 14:00:00          1          1          1          0         1  \n",
      "2011-10-22 15:00:00          1          1          1          1         0  \n",
      "2011-10-22 16:00:00          1          1          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-29 08:00:00          0          0          0         0          0   \n",
      "2011-10-29 09:00:00          0          0          1         1          1   \n",
      "2011-10-29 10:00:00          1          0          1         0          1   \n",
      "2011-10-29 11:00:00          1          1          1         1          1   \n",
      "2011-10-29 12:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-29 08:00:00          0          0          0         1          0   \n",
      "2011-10-29 09:00:00          0          1          0         1          0   \n",
      "2011-10-29 10:00:00          1          0          1         1          1   \n",
      "2011-10-29 11:00:00          0          0          1         1          1   \n",
      "2011-10-29 12:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-29 08:00:00          0          0          0         1          0   \n",
      "2011-10-29 09:00:00          0          1          0         0          1   \n",
      "2011-10-29 10:00:00          1          0          0         0          0   \n",
      "2011-10-29 11:00:00          1          1          1         1          1   \n",
      "2011-10-29 12:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-29 08:00:00          0          0          1          1         1  \n",
      "2011-10-29 09:00:00          1          0          1          0         0  \n",
      "2011-10-29 10:00:00          1          1          0          0         0  \n",
      "2011-10-29 11:00:00          0          1          1          0         1  \n",
      "2011-10-29 12:00:00          0          1          1          0         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-10-29 13:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 44us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.05901485550489]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1694571968674048\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16945719686740482\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2480421280043208\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24804212800432082\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24588171752633003\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24588171752633\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24007561436672964\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24007561436672967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  236\n",
      "Share of wrongly classified observations:  0.15987037537132054\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15987037537132054\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22427761274642177\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22427761274642183\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16905211990278157\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16905211990278152\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17674858223062384\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1767485822306238\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20537402106400215\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20537402106400215\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2688360788549824\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26883607885498245\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21050499594923033\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21050499594923036\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1798541722927356\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17985417229273562\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19065622468268972\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19065622468268972\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1980826357007831\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19808263570078316\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2029435592762625\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20294355927626248\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18417499324871722\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18417499324871725\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15446934917634347\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1544693491763435\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17742371050499595\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17742371050499595\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38333783418849576\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3833378341884958\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17310288954901432\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17310288954901432\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-29 13:00:00          0          1          0         1          1   \n",
      "2011-10-29 14:00:00          1          1          1         1          0   \n",
      "2011-10-29 15:00:00          1          1          1         0          1   \n",
      "2011-10-29 16:00:00          1          1          1         1          1   \n",
      "2011-10-29 17:00:00          0          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-29 13:00:00          1          1          1         1          1   \n",
      "2011-10-29 14:00:00          1          1          1         1          0   \n",
      "2011-10-29 15:00:00          1          0          1         1          1   \n",
      "2011-10-29 16:00:00          1          0          1         1          1   \n",
      "2011-10-29 17:00:00          1          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-29 13:00:00          1          1          1         0          1   \n",
      "2011-10-29 14:00:00          1          1          0         1          1   \n",
      "2011-10-29 15:00:00          0          0          0         1          1   \n",
      "2011-10-29 16:00:00          0          1          1         1          0   \n",
      "2011-10-29 17:00:00          0          1          1         0          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-29 13:00:00          0          1          1          0         1  \n",
      "2011-10-29 14:00:00          1          1          0          0         1  \n",
      "2011-10-29 15:00:00          0          1          1          1         1  \n",
      "2011-10-29 16:00:00          0          1          1          0         0  \n",
      "2011-10-29 17:00:00          1          1          1          1         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-05 09:00:00          1          1          1         1          1   \n",
      "2011-11-05 10:00:00          1          1          1         1          1   \n",
      "2011-11-05 11:00:00          1          1          1         1          1   \n",
      "2011-11-05 12:00:00          1          1          1         1          1   \n",
      "2011-11-05 13:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-05 09:00:00          1          1          1         1          0   \n",
      "2011-11-05 10:00:00          0          1          1         1          1   \n",
      "2011-11-05 11:00:00          1          0          0         1          1   \n",
      "2011-11-05 12:00:00          1          1          1         1          1   \n",
      "2011-11-05 13:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-05 09:00:00          1          1          1         0          1   \n",
      "2011-11-05 10:00:00          1          1          1         1          1   \n",
      "2011-11-05 11:00:00          0          1          1         0          1   \n",
      "2011-11-05 12:00:00          1          1          1         1          0   \n",
      "2011-11-05 13:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-05 09:00:00          1          1          1          1         1  \n",
      "2011-11-05 10:00:00          1          1          1          1         1  \n",
      "2011-11-05 11:00:00          1          1          1          1         0  \n",
      "2011-11-05 12:00:00          1          0          1          1         1  \n",
      "2011-11-05 13:00:00          0          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-11-05 14:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 20us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 40us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[83.7913537471047]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17267326732673272\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17267326732673266\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.25042904290429047\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2504290429042904\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24884488448844888\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24884488448844885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  79\n",
      "Share of wrongly classified observations:  0.2418481848184818\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24184818481848186\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16264026402640264\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16264026402640264\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22640264026402646\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2264026402640264\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1712211221122112\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17122112211221122\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17980198019801985\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1798019801980198\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20871287128712868\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2087128712871287\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27036303630363034\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27036303630363034\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21267326732673264\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21267326732673267\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1811221122112211\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18112211221122113\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1956435643564356\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19564356435643565\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2010561056105611\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20105610561056106\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20567656765676567\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20567656765676567\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1871947194719472\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18719471947194719\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1570957095709571\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1570957095709571\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18046204620462047\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18046204620462047\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.386006600660066\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.386006600660066\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17623762376237628\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17623762376237623\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-05 14:00:00          1          1          1         1          1   \n",
      "2011-11-05 15:00:00          0          0          1         1          1   \n",
      "2011-11-05 16:00:00          1          1          0         1          1   \n",
      "2011-11-05 17:00:00          1          1          0         1          1   \n",
      "2011-11-05 18:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-05 14:00:00          1          0          1         1          1   \n",
      "2011-11-05 15:00:00          1          1          1         0          1   \n",
      "2011-11-05 16:00:00          1          1          1         1          0   \n",
      "2011-11-05 17:00:00          1          1          0         1          0   \n",
      "2011-11-05 18:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-05 14:00:00          1          1          1         1          1   \n",
      "2011-11-05 15:00:00          0          1          1         0          1   \n",
      "2011-11-05 16:00:00          1          1          0         1          1   \n",
      "2011-11-05 17:00:00          1          1          1         1          1   \n",
      "2011-11-05 18:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-05 14:00:00          1          1          0          1         1  \n",
      "2011-11-05 15:00:00          1          1          1          1         1  \n",
      "2011-11-05 16:00:00          1          1          1          0         1  \n",
      "2011-11-05 17:00:00          0          1          0          0         1  \n",
      "2011-11-05 18:00:00          1          1          1          0         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-12 10:00:00          0          0          1         1          1   \n",
      "2011-11-12 11:00:00          1          1          1         1          1   \n",
      "2011-11-12 12:00:00          1          1          1         1          1   \n",
      "2011-11-12 13:00:00          1          1          1         1          1   \n",
      "2011-11-12 14:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-12 10:00:00          1          1          1         1          0   \n",
      "2011-11-12 11:00:00          1          1          1         1          1   \n",
      "2011-11-12 12:00:00          1          1          1         1          0   \n",
      "2011-11-12 13:00:00          1          1          0         1          1   \n",
      "2011-11-12 14:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-12 10:00:00          1          1          1         1          1   \n",
      "2011-11-12 11:00:00          1          1          0         1          1   \n",
      "2011-11-12 12:00:00          1          1          1         1          1   \n",
      "2011-11-12 13:00:00          1          1          0         1          1   \n",
      "2011-11-12 14:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-12 10:00:00          1          1          1          1         1  \n",
      "2011-11-12 11:00:00          1          1          1          1         0  \n",
      "2011-11-12 12:00:00          0          1          1          0         0  \n",
      "2011-11-12 13:00:00          1          1          1          0         1  \n",
      "2011-11-12 14:00:00          1          0          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-11-12 15:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 21us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 36us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[64.22847089982122]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17329545454545459\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17329545454545456\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24987086776859502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  161\n",
      "Share of wrongly classified observations:  0.24987086776859505\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24948347107438018\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24948347107438015\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24108987603305787\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24108987603305784\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1628357438016529\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1628357438016529\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22804752066115708\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22804752066115702\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17135847107438018\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17135847107438015\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18001033057851235\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1800103305785124\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20854855371900827\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20854855371900827\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2719524793388429\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.271952479338843\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21306818181818177\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21306818181818182\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1815599173553719\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1815599173553719\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19563533057851235\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1956353305785124\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20067148760330578\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20067148760330578\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20454545454545459\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20454545454545456\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1875\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1875\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15792871900826444\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15792871900826447\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18052685950413228\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18052685950413222\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38494318181818177\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3849431818181818\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1783316115702479\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17833161157024793\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-12 15:00:00          1          0          1         1          1   \n",
      "2011-11-12 16:00:00          0          0          0         0          0   \n",
      "2011-11-12 17:00:00          0          0          1         0          1   \n",
      "2011-11-12 18:00:00          1          1          1         1          0   \n",
      "2011-11-12 19:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-12 15:00:00          1          1          1         1          0   \n",
      "2011-11-12 16:00:00          0          0          0         1          0   \n",
      "2011-11-12 17:00:00          1          1          0         0          0   \n",
      "2011-11-12 18:00:00          1          1          1         1          1   \n",
      "2011-11-12 19:00:00          1          0          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-12 15:00:00          1          1          1         0          1   \n",
      "2011-11-12 16:00:00          0          0          0         0          0   \n",
      "2011-11-12 17:00:00          1          1          0         1          1   \n",
      "2011-11-12 18:00:00          0          1          0         1          1   \n",
      "2011-11-12 19:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-12 15:00:00          1          0          1          1         1  \n",
      "2011-11-12 16:00:00          0          0          0          0         0  \n",
      "2011-11-12 17:00:00          0          1          0          1         0  \n",
      "2011-11-12 18:00:00          1          1          0          0         1  \n",
      "2011-11-12 19:00:00          1          1          1          1         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-19 11:00:00          1          0          1         1          1   \n",
      "2011-11-19 12:00:00          0          1          0         1          1   \n",
      "2011-11-19 13:00:00          1          0          1         1          1   \n",
      "2011-11-19 14:00:00          1          1          1         0          1   \n",
      "2011-11-19 15:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-19 11:00:00          1          1          0         1          1   \n",
      "2011-11-19 12:00:00          1          1          1         1          0   \n",
      "2011-11-19 13:00:00          1          1          1         1          1   \n",
      "2011-11-19 14:00:00          1          1          1         1          1   \n",
      "2011-11-19 15:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-19 11:00:00          1          1          1         1          1   \n",
      "2011-11-19 12:00:00          1          1          1         1          1   \n",
      "2011-11-19 13:00:00          1          1          1         1          1   \n",
      "2011-11-19 14:00:00          1          1          1         1          1   \n",
      "2011-11-19 15:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-19 11:00:00          1          1          1          1         0  \n",
      "2011-11-19 12:00:00          1          1          1          0         0  \n",
      "2011-11-19 13:00:00          1          1          1          1         1  \n",
      "2011-11-19 14:00:00          1          1          1          0         1  \n",
      "2011-11-19 15:00:00          1          0          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-11-19 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 20us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 35us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[66.83935426021004]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17464931125995198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  237\n",
      "Share of wrongly classified observations:  0.17464931125995198\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.25123214962719576\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.25123214962719576\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25199039555162395\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2519903955516239\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24263869581701003\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24263869581700998\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1639074939972197\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16390749399721977\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2300012637432074\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23000126374320737\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17224819916592948\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17224819916592948\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1808416529761152\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18084165297611526\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21155061291545563\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21155061291545557\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2739795273600404\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27397952736004044\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2143308479716921\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21433084797169216\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18071527865537718\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18071527865537723\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19701756603058262\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1970175660305826\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2021989131808416\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20219891318084166\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20573739416150638\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20573739416150638\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18867686086187285\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18867686086187288\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15670415771515223\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15670415771515228\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1819790218627575\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1819790218627575\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3843043093643371\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3843043093643372\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17742954631618857\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17742954631618854\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-19 16:00:00          1          1          1         0          1   \n",
      "2011-11-19 17:00:00          1          1          0         1          1   \n",
      "2011-11-19 18:00:00          1          1          1         1          1   \n",
      "2011-11-19 19:00:00          0          1          1         1          0   \n",
      "2011-11-19 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-19 16:00:00          1          0          1         1          1   \n",
      "2011-11-19 17:00:00          0          0          1         1          1   \n",
      "2011-11-19 18:00:00          1          1          0         1          1   \n",
      "2011-11-19 19:00:00          0          1          1         1          1   \n",
      "2011-11-19 20:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-19 16:00:00          1          1          1         1          1   \n",
      "2011-11-19 17:00:00          0          1          0         1          0   \n",
      "2011-11-19 18:00:00          0          1          0         1          1   \n",
      "2011-11-19 19:00:00          1          1          1         1          1   \n",
      "2011-11-19 20:00:00          1          1          0         1          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-19 16:00:00          0          1          1          0         1  \n",
      "2011-11-19 17:00:00          1          1          1          1         0  \n",
      "2011-11-19 18:00:00          1          1          1          0         1  \n",
      "2011-11-19 19:00:00          0          1          0          1         1  \n",
      "2011-11-19 20:00:00          1          1          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-26 12:00:00          0          1          1         1          1   \n",
      "2011-11-26 13:00:00          1          1          1         1          1   \n",
      "2011-11-26 14:00:00          1          0          0         1          1   \n",
      "2011-11-26 15:00:00          1          1          1         0          0   \n",
      "2011-11-26 16:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-26 12:00:00          1          1          1         1          1   \n",
      "2011-11-26 13:00:00          1          1          1         1          1   \n",
      "2011-11-26 14:00:00          0          1          1         1          1   \n",
      "2011-11-26 15:00:00          1          0          1         1          0   \n",
      "2011-11-26 16:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-26 12:00:00          1          1          0         1          0   \n",
      "2011-11-26 13:00:00          1          0          1         1          1   \n",
      "2011-11-26 14:00:00          1          1          1         1          1   \n",
      "2011-11-26 15:00:00          1          1          1         0          1   \n",
      "2011-11-26 16:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-26 12:00:00          1          1          1          1         1  \n",
      "2011-11-26 13:00:00          1          1          0          1         1  \n",
      "2011-11-26 14:00:00          1          1          1          0         1  \n",
      "2011-11-26 15:00:00          1          1          0          1         1  \n",
      "2011-11-26 16:00:00          1          1          0          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-11-26 17:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 18us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 45us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[68.06127531421508]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17446176688938386\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1744617668893838\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.251051719871319\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.25105171987131897\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2532788913635239\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2532788913635239\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24226676565206628\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2422667656520663\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16419203167532792\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1641920316753279\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2312546399406088\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23125463994060877\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1727295223954467\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17272952239544667\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18200940361296714\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18200940361296708\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21244741400643408\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21244741400643405\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27418955704033654\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27418955704033654\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21529324424647367\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21529324424647364\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1813907448651324\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1813907448651324\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19784706755753523\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19784706755753526\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20267260579064583\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2026726057906459\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20576589952981938\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20576589952981936\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18943330858698337\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18943330858698343\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1563969314526107\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15639693145261074\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18275179411036868\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1827517941103687\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3838158871566444\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3838158871566444\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17817371937639204\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17817371937639198\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-11-26 17:00:00          1          1          1         1          1   \n",
      "2011-11-26 18:00:00          1          1          0         1          1   \n",
      "2011-11-26 19:00:00          1          1          1         1          1   \n",
      "2011-11-26 20:00:00          0          1          1         0          0   \n",
      "2011-11-26 21:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-11-26 17:00:00          1          0          0         1          0   \n",
      "2011-11-26 18:00:00          1          0          0         1          0   \n",
      "2011-11-26 19:00:00          1          1          1         0          1   \n",
      "2011-11-26 20:00:00          1          0          1         1          0   \n",
      "2011-11-26 21:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-11-26 17:00:00          0          1          1         1          1   \n",
      "2011-11-26 18:00:00          1          1          1         0          1   \n",
      "2011-11-26 19:00:00          1          1          1         0          1   \n",
      "2011-11-26 20:00:00          1          1          0         0          1   \n",
      "2011-11-26 21:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-11-26 17:00:00          1          1          1          0         1  \n",
      "2011-11-26 18:00:00          0          0          0          1         0  \n",
      "2011-11-26 19:00:00          1          1          1          1         1  \n",
      "2011-11-26 20:00:00          0          1          0          1         1  \n",
      "2011-11-26 21:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-12-03 13:00:00          0          1          1         1          1   \n",
      "2011-12-03 14:00:00          1          1          1         1          1   \n",
      "2011-12-03 15:00:00          1          1          1         1          1   \n",
      "2011-12-03 16:00:00          0          0          0         0          1   \n",
      "2011-12-03 17:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-12-03 13:00:00          1          0          1         1          1   \n",
      "2011-12-03 14:00:00          1          1          1         1          1   \n",
      "2011-12-03 15:00:00          1          1          1         1          1   \n",
      "2011-12-03 16:00:00          1          1          1         1          0   \n",
      "2011-12-03 17:00:00          0          1          1         0          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-12-03 13:00:00          1          1          0         1          1   \n",
      "2011-12-03 14:00:00          1          1          1         1          1   \n",
      "2011-12-03 15:00:00          1          1          1         1          1   \n",
      "2011-12-03 16:00:00          1          1          0         1          1   \n",
      "2011-12-03 17:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-12-03 13:00:00          1          1          1          1         1  \n",
      "2011-12-03 14:00:00          1          1          1          0         0  \n",
      "2011-12-03 15:00:00          1          1          1          0         1  \n",
      "2011-12-03 16:00:00          0          1          1          1         0  \n",
      "2011-12-03 17:00:00          0          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-16 07:00:00'), Timestamp('2011-04-16 07:00:00'), Timestamp('2011-12-03 18:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 20us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 53us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.68370074919236]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17391831293176585\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17391831293176585\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.25184826081687073\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2518482608168707\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2531814325536299\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25318143255362985\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24312204581262875\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24312204581262878\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16313174160707788\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16313174160707794\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23112350018179617\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23112350018179614\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17210035147254876\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1721003514725488\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18276572536662228\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18276572536662222\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2127014907283965\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21270149072839656\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2730578111744031\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2730578111744031\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21536783420191496\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2153678342019149\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.181432553629863\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.18143255362986305\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1971882196097443\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19718821960974428\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20239970912616656\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20239970912616653\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20542964489152826\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2054296448915283\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18967397891164706\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18967397891164708\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1559810932008241\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15598109320082415\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18337171251969453\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.18337171251969459\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358986789480065\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38358986789480065\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1791298024481881\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1791298024481881\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-12-03 18:00:00          1          1          0         0          1   \n",
      "2011-12-03 19:00:00          1          1          1         1          1   \n",
      "2011-12-03 20:00:00          1          1          1         1          1   \n",
      "2011-12-03 21:00:00          1          1          0         1          1   \n",
      "2011-12-03 22:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-12-03 18:00:00          1          1          1         1          1   \n",
      "2011-12-03 19:00:00          1          1          1         1          1   \n",
      "2011-12-03 20:00:00          1          0          0         1          1   \n",
      "2011-12-03 21:00:00          1          1          1         1          1   \n",
      "2011-12-03 22:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-12-03 18:00:00          0          0          1         1          1   \n",
      "2011-12-03 19:00:00          1          1          1         1          1   \n",
      "2011-12-03 20:00:00          1          0          0         1          1   \n",
      "2011-12-03 21:00:00          1          1          1         1          1   \n",
      "2011-12-03 22:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-12-03 18:00:00          0          1          1          0         1  \n",
      "2011-12-03 19:00:00          1          0          1          0         1  \n",
      "2011-12-03 20:00:00          1          1          0          1         1  \n",
      "2011-12-03 21:00:00          1          1          1          1         0  \n",
      "2011-12-03 22:00:00          0          1          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-12-10 14:00:00          1          1          1         0          1   \n",
      "2011-12-10 15:00:00          1          1          1         1          1   \n",
      "2011-12-10 16:00:00          1          0          0         0          1   \n",
      "2011-12-10 17:00:00          1          0          0         1          1   \n",
      "2011-12-10 18:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-12-10 14:00:00          1          1          0         1          1   \n",
      "2011-12-10 15:00:00          1          0          1         0          1   \n",
      "2011-12-10 16:00:00          0          1          1         1          1   \n",
      "2011-12-10 17:00:00          1          1          1         1          1   \n",
      "2011-12-10 18:00:00          1          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-12-10 14:00:00          1          0          1         1          1   \n",
      "2011-12-10 15:00:00          0          1          1         1          1   \n",
      "2011-12-10 16:00:00          1          1          0         1          0   \n",
      "2011-12-10 17:00:00          1          1          0         1          0   \n",
      "2011-12-10 18:00:00          0          0          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-12-10 14:00:00          0          1          1          0         0  \n",
      "2011-12-10 15:00:00          0          1          1          0         1  \n",
      "2011-12-10 16:00:00          1          1          1          1         1  \n",
      "2011-12-10 17:00:00          1          0          1          1         0  \n",
      "2011-12-10 18:00:00          1          0          0          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area binary230, index: 77\n",
      "date: 2011-12-06 23:00:00\n",
      "Drift detected at:  2011-12-06 23:00:00\n",
      ">> Current Time:  23/01/2020 11:41:01\n",
      "## ++ previous detected dates:  [Timestamp('2011-04-16 07:00:00'), Timestamp('2011-12-06 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  730\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-12-06 23:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-12-06 23:00:00'), Timestamp('2011-12-06 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-12-06 23:00:00'), Timestamp('2011-12-06 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count15__trainsize730_s12_2009_e12_2011__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-12-06 23:00:00\n",
      ">last_train_set_year:  2011-12-06 23:00:00\n",
      "start_validation_set_year:  2011-12-06 23:00:00\n",
      "end_validation_set_year:  2011-12-06 23:00:00\n",
      "start_test_set_year:  2011-12-06 23:00:00\n",
      "end_test_set_year:  2011-12-06 23:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2009-12-06 23:00:00\n",
      "last_train_set_year:  2011-12-06 23:00:00\n",
      "start_validation_set_year:  2011-12-06 23:00:00\n",
      "start_test_set_year:  2011-12-06 23:00:00\n",
      "end_validation_set_year:  2011-12-06 23:00:00\n",
      "end_test_set_year:  2011-12-06 23:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# adjusted dates..\n",
      "start_train_year:  2009-12-06 23:00:00\n",
      "last_train_set_year:  2011-12-06 23:00:00\n",
      "start_validation_set_year:  2011-12-06 23:00:00\n",
      "start_test_set_year:  2011-12-06 23:00:00\n",
      "end_validation_set_year:  2011-12-06 23:00:00\n",
      "end_test_set_year:  2011-12-06 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16848, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16848,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (336960, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336960 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "336960/336960 [==============================] - 5s 15us/step - loss: 0.3639 - mean_absolute_error: 0.4354 - val_loss: 0.7763 - val_mean_absolute_error: 0.7288\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2871 - mean_absolute_error: 0.3861 - val_loss: 0.5731 - val_mean_absolute_error: 0.6549\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2690 - mean_absolute_error: 0.3725 - val_loss: 0.5729 - val_mean_absolute_error: 0.6289\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2575 - mean_absolute_error: 0.3640 - val_loss: 0.5169 - val_mean_absolute_error: 0.6168\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2492 - mean_absolute_error: 0.3579 - val_loss: 0.5204 - val_mean_absolute_error: 0.6103\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2444 - mean_absolute_error: 0.3541 - val_loss: 0.5152 - val_mean_absolute_error: 0.5921\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2392 - mean_absolute_error: 0.3506 - val_loss: 0.5388 - val_mean_absolute_error: 0.6026\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2353 - mean_absolute_error: 0.3473 - val_loss: 0.5765 - val_mean_absolute_error: 0.6381\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2328 - mean_absolute_error: 0.3456 - val_loss: 0.5850 - val_mean_absolute_error: 0.6434\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2287 - mean_absolute_error: 0.3431 - val_loss: 0.4898 - val_mean_absolute_error: 0.5859\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2276 - mean_absolute_error: 0.3416 - val_loss: 0.4501 - val_mean_absolute_error: 0.5686\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2259 - mean_absolute_error: 0.3405 - val_loss: 0.4315 - val_mean_absolute_error: 0.5664\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2237 - mean_absolute_error: 0.3389 - val_loss: 0.4952 - val_mean_absolute_error: 0.5829\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2223 - mean_absolute_error: 0.3380 - val_loss: 0.4691 - val_mean_absolute_error: 0.5628\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2212 - mean_absolute_error: 0.3372 - val_loss: 0.4669 - val_mean_absolute_error: 0.5776\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2189 - mean_absolute_error: 0.3357 - val_loss: 0.4715 - val_mean_absolute_error: 0.5607\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2184 - mean_absolute_error: 0.3353 - val_loss: 0.4789 - val_mean_absolute_error: 0.5697\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2168 - mean_absolute_error: 0.3342 - val_loss: 0.4586 - val_mean_absolute_error: 0.5673\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2167 - mean_absolute_error: 0.3341 - val_loss: 0.4837 - val_mean_absolute_error: 0.5619\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2151 - mean_absolute_error: 0.3331 - val_loss: 0.4167 - val_mean_absolute_error: 0.5408\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  3\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2009-12-06 23:00:00'), Timestamp('2011-12-06 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-12-06 23:00:00'), Timestamp('2011-12-06 23:00:00'), Timestamp('2011-12-07 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 24us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 46us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.1725917548243]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16903141146559497\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16903141146559503\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2452226687443615\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24522266874436152\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2507176248667268\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25071762486672683\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24194209792503896\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24194209792503896\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16591486918723863\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16591486918723858\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22709751496760433\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22709751496760436\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16845731157221355\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16845731157221355\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.177970966948249\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.177970966948249\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2093824325432625\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20938243254326253\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27187730665135734\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27187730665135734\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21520544574756006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  142\n",
      "Share of wrongly classified observations:  0.21520544574756006\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1765767243500369\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1765767243500369\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19322562125809895\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19322562125809892\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19880259165094727\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19880259165094727\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1981464774870828\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19814647748708275\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1858443369146231\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18584433691462315\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1545968998605758\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15459689986057573\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1778069384072829\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17780693840728287\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.37677355859919626\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.37677355859919626\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17485442466989254\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17485442466989257\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-12-07 00:00:00          1          0          1         1          1   \n",
      "2011-12-07 01:00:00          1          1          1         1          1   \n",
      "2011-12-07 02:00:00          1          1          0         1          1   \n",
      "2011-12-07 03:00:00          1          1          1         0          1   \n",
      "2011-12-07 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-12-07 00:00:00          1          1          0         1          0   \n",
      "2011-12-07 01:00:00          0          1          0         1          1   \n",
      "2011-12-07 02:00:00          1          1          1         1          1   \n",
      "2011-12-07 03:00:00          1          1          1         1          1   \n",
      "2011-12-07 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-12-07 00:00:00          0          0          0         0          1   \n",
      "2011-12-07 01:00:00          0          1          1         1          1   \n",
      "2011-12-07 02:00:00          1          1          1         1          1   \n",
      "2011-12-07 03:00:00          1          1          1         1          1   \n",
      "2011-12-07 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-12-07 00:00:00          1          1          0          0         1  \n",
      "2011-12-07 01:00:00          1          1          1          1         0  \n",
      "2011-12-07 02:00:00          1          1          1          1         1  \n",
      "2011-12-07 03:00:00          1          1          1          1         1  \n",
      "2011-12-07 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-05-22 20:00:00          0          1          1         1          1   \n",
      "2012-05-22 21:00:00          1          1          1         1          0   \n",
      "2012-05-22 22:00:00          0          1          1         1          1   \n",
      "2012-05-22 23:00:00          1          0          0         1          0   \n",
      "2012-05-23 00:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-05-22 20:00:00          0          1          1         0          1   \n",
      "2012-05-22 21:00:00          1          1          1         1          1   \n",
      "2012-05-22 22:00:00          1          0          1         0          0   \n",
      "2012-05-22 23:00:00          1          1          1         0          1   \n",
      "2012-05-23 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-05-22 20:00:00          0          1          0         0          0   \n",
      "2012-05-22 21:00:00          1          1          1         1          1   \n",
      "2012-05-22 22:00:00          0          1          1         0          1   \n",
      "2012-05-22 23:00:00          1          0          0         1          1   \n",
      "2012-05-23 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-05-22 20:00:00          1          1          1          0         1  \n",
      "2012-05-22 21:00:00          0          1          0          1         0  \n",
      "2012-05-22 22:00:00          1          1          1          0         1  \n",
      "2012-05-22 23:00:00          1          1          1          0         1  \n",
      "2012-05-23 00:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary237, index: 1471\n",
      "date: 2012-02-06 07:00:00\n",
      "Drift detected at:  2012-02-06 07:00:00\n",
      ">> Current Time:  23/01/2020 11:43:17\n",
      "## ++ previous detected dates:  [Timestamp('2011-12-06 23:00:00'), Timestamp('2012-02-06 07:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  730\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-02-06 07:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-02-06 07:00:00'), Timestamp('2012-02-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2010-02-06 07:00:00'), Timestamp('2012-02-06 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count17__trainsize730_s2_2010_e2_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-02-06 07:00:00\n",
      ">last_train_set_year:  2012-02-06 07:00:00\n",
      "start_validation_set_year:  2012-02-06 07:00:00\n",
      "end_validation_set_year:  2012-02-06 07:00:00\n",
      "start_test_set_year:  2012-02-06 07:00:00\n",
      "end_test_set_year:  2012-02-06 07:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-02-06 07:00:00\n",
      "last_train_set_year:  2012-02-06 07:00:00\n",
      "start_validation_set_year:  2012-02-06 07:00:00\n",
      "start_test_set_year:  2012-02-06 07:00:00\n",
      "end_validation_set_year:  2012-02-06 07:00:00\n",
      "end_test_set_year:  2012-02-06 07:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-02-06 07:00:00\n",
      "last_train_set_year:  2012-02-06 07:00:00\n",
      "start_validation_set_year:  2012-02-06 07:00:00\n",
      "start_test_set_year:  2012-02-06 07:00:00\n",
      "end_validation_set_year:  2012-02-06 07:00:00\n",
      "end_test_set_year:  2012-02-06 07:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16848, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16848,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (336960, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336960 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.3771 - mean_absolute_error: 0.4409 - val_loss: 0.2640 - val_mean_absolute_error: 0.4041\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2852 - mean_absolute_error: 0.3847 - val_loss: 0.2800 - val_mean_absolute_error: 0.4192\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2682 - mean_absolute_error: 0.3720 - val_loss: 0.2787 - val_mean_absolute_error: 0.4217\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2586 - mean_absolute_error: 0.3647 - val_loss: 0.2047 - val_mean_absolute_error: 0.3438\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2531 - mean_absolute_error: 0.3608 - val_loss: 0.2058 - val_mean_absolute_error: 0.3459\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2461 - mean_absolute_error: 0.3552 - val_loss: 0.2274 - val_mean_absolute_error: 0.3890\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2416 - mean_absolute_error: 0.3521 - val_loss: 0.2440 - val_mean_absolute_error: 0.3966\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2366 - mean_absolute_error: 0.3489 - val_loss: 0.2766 - val_mean_absolute_error: 0.4248\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2336 - mean_absolute_error: 0.3466 - val_loss: 0.1818 - val_mean_absolute_error: 0.3445\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2305 - mean_absolute_error: 0.3445 - val_loss: 0.2701 - val_mean_absolute_error: 0.4297\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2288 - mean_absolute_error: 0.3430 - val_loss: 0.2040 - val_mean_absolute_error: 0.3671\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2272 - mean_absolute_error: 0.3421 - val_loss: 0.2254 - val_mean_absolute_error: 0.3794\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2246 - mean_absolute_error: 0.3400 - val_loss: 0.2254 - val_mean_absolute_error: 0.3772\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2239 - mean_absolute_error: 0.3399 - val_loss: 0.1664 - val_mean_absolute_error: 0.3188\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2220 - mean_absolute_error: 0.3384 - val_loss: 0.2545 - val_mean_absolute_error: 0.4221\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2215 - mean_absolute_error: 0.3384 - val_loss: 0.1824 - val_mean_absolute_error: 0.3347\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2189 - mean_absolute_error: 0.3362 - val_loss: 0.2264 - val_mean_absolute_error: 0.3926\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2176 - mean_absolute_error: 0.3353 - val_loss: 0.1597 - val_mean_absolute_error: 0.3214\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2169 - mean_absolute_error: 0.3349 - val_loss: 0.2342 - val_mean_absolute_error: 0.3671\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "336960/336960 [==============================] - 4s 13us/step - loss: 0.2157 - mean_absolute_error: 0.3343 - val_loss: 0.2205 - val_mean_absolute_error: 0.3453\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  4\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  730\n",
      "selected years for training:  [Timestamp('2010-02-06 07:00:00'), Timestamp('2012-02-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2010-02-06 07:00:00'), Timestamp('2012-02-06 07:00:00'), Timestamp('2012-02-06 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 22us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 42us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.38253239858013]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16736187339919506\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16736187339919503\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24105378704720093\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24105378704720087\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24815221368459572\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2481522136845957\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23556531284302962\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23556531284302965\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16253201609952428\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16253201609952433\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22407610684229784\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22407610684229784\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1659714599341383\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1659714599341383\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17519209659714596\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.175192096597146\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2109037687522869\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21090376875228686\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2679107208196122\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26791072081961215\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21317233809001102\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.213172338090011\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17358214416392248\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17358214416392242\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1935601902671057\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19356019026710575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  68\n",
      "Share of wrongly classified observations:  0.19890230515916574\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19890230515916577\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19604829857299666\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19604829857299672\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18580314672521037\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1858031467252104\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.153823637028906\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15382363702890597\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17716794731064767\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17716794731064764\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3754116355653129\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3754116355653128\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17094767654592025\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17094767654592025\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-02-06 08:00:00          1          1          1         1          1   \n",
      "2012-02-06 09:00:00          1          1          1         0          1   \n",
      "2012-02-06 10:00:00          1          0          1         1          1   \n",
      "2012-02-06 11:00:00          1          1          1         1          1   \n",
      "2012-02-06 12:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-02-06 08:00:00          0          1          1         1          1   \n",
      "2012-02-06 09:00:00          0          1          1         1          1   \n",
      "2012-02-06 10:00:00          1          0          1         1          1   \n",
      "2012-02-06 11:00:00          1          1          1         1          0   \n",
      "2012-02-06 12:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-02-06 08:00:00          0          1          1         0          0   \n",
      "2012-02-06 09:00:00          0          1          1         0          0   \n",
      "2012-02-06 10:00:00          1          0          1         0          1   \n",
      "2012-02-06 11:00:00          0          1          1         0          1   \n",
      "2012-02-06 12:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-02-06 08:00:00          1          1          1          0         1  \n",
      "2012-02-06 09:00:00          1          1          0          1         1  \n",
      "2012-02-06 10:00:00          1          0          1          0         1  \n",
      "2012-02-06 11:00:00          1          1          1          1         0  \n",
      "2012-02-06 12:00:00          1          0          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-07-23 04:00:00          1          1          1         0          1   \n",
      "2012-07-23 05:00:00          1          0          1         0          1   \n",
      "2012-07-23 06:00:00          1          1          1         1          1   \n",
      "2012-07-23 07:00:00          1          1          1         0          1   \n",
      "2012-07-23 08:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-07-23 04:00:00          1          1          1         1          1   \n",
      "2012-07-23 05:00:00          0          0          1         1          0   \n",
      "2012-07-23 06:00:00          1          1          0         1          1   \n",
      "2012-07-23 07:00:00          1          1          1         1          1   \n",
      "2012-07-23 08:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-07-23 04:00:00          1          1          1         1          1   \n",
      "2012-07-23 05:00:00          1          1          1         0          0   \n",
      "2012-07-23 06:00:00          1          1          1         0          1   \n",
      "2012-07-23 07:00:00          1          1          1         0          1   \n",
      "2012-07-23 08:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-07-23 04:00:00          1          1          1          1         1  \n",
      "2012-07-23 05:00:00          1          1          1          1         1  \n",
      "2012-07-23 06:00:00          0          1          1          0         1  \n",
      "2012-07-23 07:00:00          1          1          1          1         0  \n",
      "2012-07-23 08:00:00          1          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary237, index: 2143\n",
      "date: 2012-05-05 15:00:00\n",
      "Drift detected at:  2012-05-05 15:00:00\n",
      ">> Current Time:  23/01/2020 11:45:33\n",
      "## ++ previous detected dates:  [Timestamp('2012-02-06 07:00:00'), Timestamp('2012-05-05 15:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-05-05 15:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-05-05 15:00:00'), Timestamp('2012-05-05 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-05-05 15:00:00'), Timestamp('2012-05-05 15:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count19__trainsize731_s5_2010_e5_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-05-05 15:00:00\n",
      ">last_train_set_year:  2012-05-05 15:00:00\n",
      "start_validation_set_year:  2012-05-05 15:00:00\n",
      "end_validation_set_year:  2012-05-05 15:00:00\n",
      "start_test_set_year:  2012-05-05 15:00:00\n",
      "end_test_set_year:  2012-05-05 15:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-05-05 15:00:00\n",
      "last_train_set_year:  2012-05-05 15:00:00\n",
      "start_validation_set_year:  2012-05-05 15:00:00\n",
      "start_test_set_year:  2012-05-05 15:00:00\n",
      "end_validation_set_year:  2012-05-05 15:00:00\n",
      "end_test_set_year:  2012-05-05 15:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-05-05 15:00:00\n",
      "last_train_set_year:  2012-05-05 15:00:00\n",
      "start_validation_set_year:  2012-05-05 15:00:00\n",
      "start_test_set_year:  2012-05-05 15:00:00\n",
      "end_validation_set_year:  2012-05-05 15:00:00\n",
      "end_test_set_year:  2012-05-05 15:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 15us/step - loss: 0.3663 - mean_absolute_error: 0.4347 - val_loss: 0.1569 - val_mean_absolute_error: 0.3075\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2796 - mean_absolute_error: 0.3809 - val_loss: 0.1331 - val_mean_absolute_error: 0.2769\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2626 - mean_absolute_error: 0.3683 - val_loss: 0.1439 - val_mean_absolute_error: 0.2984\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2518 - mean_absolute_error: 0.3603 - val_loss: 0.1178 - val_mean_absolute_error: 0.2509\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2453 - mean_absolute_error: 0.3555 - val_loss: 0.1219 - val_mean_absolute_error: 0.2588\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2391 - mean_absolute_error: 0.3507 - val_loss: 0.1105 - val_mean_absolute_error: 0.2562\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2344 - mean_absolute_error: 0.3471 - val_loss: 0.1209 - val_mean_absolute_error: 0.2644\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2305 - mean_absolute_error: 0.3443 - val_loss: 0.1191 - val_mean_absolute_error: 0.2561\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2279 - mean_absolute_error: 0.3426 - val_loss: 0.1156 - val_mean_absolute_error: 0.2626\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2255 - mean_absolute_error: 0.3409 - val_loss: 0.1147 - val_mean_absolute_error: 0.2507\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2236 - mean_absolute_error: 0.3395 - val_loss: 0.1384 - val_mean_absolute_error: 0.2921\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2220 - mean_absolute_error: 0.3382 - val_loss: 0.1046 - val_mean_absolute_error: 0.2512\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2202 - mean_absolute_error: 0.3372 - val_loss: 0.1112 - val_mean_absolute_error: 0.2563\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2192 - mean_absolute_error: 0.3365 - val_loss: 0.0908 - val_mean_absolute_error: 0.2340\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2181 - mean_absolute_error: 0.3361 - val_loss: 0.1033 - val_mean_absolute_error: 0.2484\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2166 - mean_absolute_error: 0.3348 - val_loss: 0.0924 - val_mean_absolute_error: 0.2465\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2159 - mean_absolute_error: 0.3341 - val_loss: 0.0890 - val_mean_absolute_error: 0.2350\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2141 - mean_absolute_error: 0.3330 - val_loss: 0.0944 - val_mean_absolute_error: 0.2404\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2142 - mean_absolute_error: 0.3331 - val_loss: 0.0877 - val_mean_absolute_error: 0.2290\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2133 - mean_absolute_error: 0.3325 - val_loss: 0.0933 - val_mean_absolute_error: 0.2417\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  5\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-05-05 15:00:00'), Timestamp('2012-05-05 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-05-05 15:00:00'), Timestamp('2012-05-05 15:00:00'), Timestamp('2012-05-05 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 27us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 52us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.23456061426199]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16629767853754196\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1662976785375419\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2428996141438421\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24289961414384212\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24524005313429065\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2452400531342906\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22822442912265162\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22822442912265165\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16231260674299453\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1623126067429945\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22430261243595417\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2243026124359542\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16787905623379085\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16787905623379087\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.172306913783288\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.172306913783288\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20690745777721553\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2069074577772155\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26440635081282815\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26440635081282815\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21253716237586184\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21253716237586184\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17009298500853942\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17009298500853945\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19103042570687578\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19103042570687584\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19419318109937378\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19419318109937378\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19577455879562278\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19577455879562275\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18356632298058073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  164\n",
      "Share of wrongly classified observations:  0.18356632298058068\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1525713201341008\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15257132013410082\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17167436270478842\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17167436270478842\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.37915111645265354\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.37915111645265354\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16674046429249156\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16674046429249162\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-05-05 16:00:00          1          1          1         1          1   \n",
      "2012-05-05 17:00:00          1          1          1         1          1   \n",
      "2012-05-05 18:00:00          1          1          1         1          1   \n",
      "2012-05-05 19:00:00          1          0          1         1          1   \n",
      "2012-05-05 20:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-05-05 16:00:00          1          1          1         1          0   \n",
      "2012-05-05 17:00:00          1          0          1         1          0   \n",
      "2012-05-05 18:00:00          1          1          1         1          1   \n",
      "2012-05-05 19:00:00          1          1          1         1          1   \n",
      "2012-05-05 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-05-05 16:00:00          1          1          0         1          1   \n",
      "2012-05-05 17:00:00          1          1          0         1          1   \n",
      "2012-05-05 18:00:00          1          1          1         1          1   \n",
      "2012-05-05 19:00:00          1          1          1         1          1   \n",
      "2012-05-05 20:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-05-05 16:00:00          1          1          1          1         1  \n",
      "2012-05-05 17:00:00          1          0          1          0         1  \n",
      "2012-05-05 18:00:00          1          1          1          0         1  \n",
      "2012-05-05 19:00:00          1          1          1          0         1  \n",
      "2012-05-05 20:00:00          1          1          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-10-20 12:00:00          1          0          1         1          1   \n",
      "2012-10-20 13:00:00          1          1          1         1          1   \n",
      "2012-10-20 14:00:00          1          1          1         1          0   \n",
      "2012-10-20 15:00:00          1          1          0         0          1   \n",
      "2012-10-20 16:00:00          0          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-10-20 12:00:00          1          1          1         1          0   \n",
      "2012-10-20 13:00:00          1          1          1         1          0   \n",
      "2012-10-20 14:00:00          0          1          0         1          1   \n",
      "2012-10-20 15:00:00          1          0          1         1          1   \n",
      "2012-10-20 16:00:00          1          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-10-20 12:00:00          1          1          1         1          0   \n",
      "2012-10-20 13:00:00          1          1          1         1          1   \n",
      "2012-10-20 14:00:00          1          1          1         0          1   \n",
      "2012-10-20 15:00:00          1          1          0         1          0   \n",
      "2012-10-20 16:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-10-20 12:00:00          0          1          1          0         0  \n",
      "2012-10-20 13:00:00          1          1          1          0         0  \n",
      "2012-10-20 14:00:00          1          1          1          1         1  \n",
      "2012-10-20 15:00:00          1          1          1          1         1  \n",
      "2012-10-20 16:00:00          0          0          1          0         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary90, index: 3071\n",
      "date: 2012-09-10 15:00:00\n",
      "Drift detected at:  2012-09-10 15:00:00\n",
      ">> Current Time:  23/01/2020 11:47:55\n",
      "## ++ previous detected dates:  [Timestamp('2012-05-05 15:00:00'), Timestamp('2012-09-10 15:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-09-10 15:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-09-10 15:00:00'), Timestamp('2012-09-10 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-09-10 15:00:00'), Timestamp('2012-09-10 15:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count21__trainsize731_s9_2010_e9_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-09-10 15:00:00\n",
      ">last_train_set_year:  2012-09-10 15:00:00\n",
      "start_validation_set_year:  2012-09-10 15:00:00\n",
      "end_validation_set_year:  2012-09-10 15:00:00\n",
      "start_test_set_year:  2012-09-10 15:00:00\n",
      "end_test_set_year:  2012-09-10 15:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-09-10 15:00:00\n",
      "last_train_set_year:  2012-09-10 15:00:00\n",
      "start_validation_set_year:  2012-09-10 15:00:00\n",
      "start_test_set_year:  2012-09-10 15:00:00\n",
      "end_validation_set_year:  2012-09-10 15:00:00\n",
      "end_test_set_year:  2012-09-10 15:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-09-10 15:00:00\n",
      "last_train_set_year:  2012-09-10 15:00:00\n",
      "start_validation_set_year:  2012-09-10 15:00:00\n",
      "start_test_set_year:  2012-09-10 15:00:00\n",
      "end_validation_set_year:  2012-09-10 15:00:00\n",
      "end_test_set_year:  2012-09-10 15:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 16us/step - loss: 0.3637 - mean_absolute_error: 0.4365 - val_loss: 0.3272 - val_mean_absolute_error: 0.4694\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2760 - mean_absolute_error: 0.3800 - val_loss: 0.2733 - val_mean_absolute_error: 0.4272\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2576 - mean_absolute_error: 0.3667 - val_loss: 0.2578 - val_mean_absolute_error: 0.4112\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2488 - mean_absolute_error: 0.3595 - val_loss: 0.1802 - val_mean_absolute_error: 0.3522\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2398 - mean_absolute_error: 0.3525 - val_loss: 0.1823 - val_mean_absolute_error: 0.3542\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2346 - mean_absolute_error: 0.3485 - val_loss: 0.1750 - val_mean_absolute_error: 0.3521\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2295 - mean_absolute_error: 0.3444 - val_loss: 0.1646 - val_mean_absolute_error: 0.3333\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2251 - mean_absolute_error: 0.3412 - val_loss: 0.1569 - val_mean_absolute_error: 0.3244\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2226 - mean_absolute_error: 0.3392 - val_loss: 0.1750 - val_mean_absolute_error: 0.3399\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2194 - mean_absolute_error: 0.3373 - val_loss: 0.1580 - val_mean_absolute_error: 0.3216\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2179 - mean_absolute_error: 0.3359 - val_loss: 0.1341 - val_mean_absolute_error: 0.2995\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2162 - mean_absolute_error: 0.3347 - val_loss: 0.1416 - val_mean_absolute_error: 0.3075\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2140 - mean_absolute_error: 0.3333 - val_loss: 0.1294 - val_mean_absolute_error: 0.3008\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2127 - mean_absolute_error: 0.3325 - val_loss: 0.1395 - val_mean_absolute_error: 0.3155\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2115 - mean_absolute_error: 0.3313 - val_loss: 0.1294 - val_mean_absolute_error: 0.3044\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2103 - mean_absolute_error: 0.3305 - val_loss: 0.1323 - val_mean_absolute_error: 0.3044\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2095 - mean_absolute_error: 0.3299 - val_loss: 0.1250 - val_mean_absolute_error: 0.2963\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2079 - mean_absolute_error: 0.3291 - val_loss: 0.1645 - val_mean_absolute_error: 0.3280\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2077 - mean_absolute_error: 0.3285 - val_loss: 0.1256 - val_mean_absolute_error: 0.3035\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2067 - mean_absolute_error: 0.3280 - val_loss: 0.1682 - val_mean_absolute_error: 0.3319\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  6\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-09-10 15:00:00'), Timestamp('2012-09-10 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-09-10 15:00:00'), Timestamp('2012-09-10 15:00:00'), Timestamp('2012-09-10 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 28us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 58us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[66.0890343034897]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17112441078332719\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17112441078332716\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24622636512896567\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24622636512896562\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25247603410836295\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2524760341083629\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23558074254541606\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23558074254541603\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1667814204756104\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1667814204756104\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23007255971611673\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23007255971611673\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1724484931942164\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1724484931942164\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17779778613420905\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.177797786134209\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21598432286425506\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21598432286425506\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27138393093586144\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27138393093586144\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21921508394682487\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21921508394682485\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17552036438747942\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17552036438747948\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19871828822625925\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1987182882262592\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2006779301943753\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2006779301943753\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1959641968116096\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19596419681160956\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18902600497854982\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18902600497854988\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1564006143742387\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15640061437423866\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17626185053757748\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17626185053757745\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3808061013717494\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38080610137174936\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17091255759758484\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17091255759758486\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-09-10 16:00:00          0          1          1         1          1   \n",
      "2012-09-10 17:00:00          1          0          1         1          1   \n",
      "2012-09-10 18:00:00          1          0          1         1          1   \n",
      "2012-09-10 19:00:00          1          0          0         1          1   \n",
      "2012-09-10 20:00:00          0          0          1         0          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-09-10 16:00:00          0          1          1         0          1   \n",
      "2012-09-10 17:00:00          1          1          1         1          0   \n",
      "2012-09-10 18:00:00          0          0          1         0          1   \n",
      "2012-09-10 19:00:00          1          0          1         1          1   \n",
      "2012-09-10 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-09-10 16:00:00          1          1          0         1          1   \n",
      "2012-09-10 17:00:00          1          1          1         1          1   \n",
      "2012-09-10 18:00:00          0          1          1         0          1   \n",
      "2012-09-10 19:00:00          0          0          0         1          1   \n",
      "2012-09-10 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-09-10 16:00:00          1          1          1          0         1  \n",
      "2012-09-10 17:00:00          1          0          1          0         1  \n",
      "2012-09-10 18:00:00          1          1          0          0         1  \n",
      "2012-09-10 19:00:00          1          1          1          1         1  \n",
      "2012-09-10 20:00:00          1          0          0          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-02-25 12:00:00          1          1          1         1          1   \n",
      "2013-02-25 13:00:00          0          1          1         1          1   \n",
      "2013-02-25 14:00:00          1          1          1         1          1   \n",
      "2013-02-25 15:00:00          1          1          1         1          0   \n",
      "2013-02-25 16:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-02-25 12:00:00          0          1          1         1          1   \n",
      "2013-02-25 13:00:00          1          1          1         1          1   \n",
      "2013-02-25 14:00:00          1          1          1         1          0   \n",
      "2013-02-25 15:00:00          1          1          1         1          1   \n",
      "2013-02-25 16:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-02-25 12:00:00          1          0          1         1          1   \n",
      "2013-02-25 13:00:00          1          1          1         1          1   \n",
      "2013-02-25 14:00:00          1          1          1         1          1   \n",
      "2013-02-25 15:00:00          1          1          1         0          1   \n",
      "2013-02-25 16:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-02-25 12:00:00          1          0          1          0         1  \n",
      "2013-02-25 13:00:00          1          0          1          1         1  \n",
      "2013-02-25 14:00:00          1          1          1          0         1  \n",
      "2013-02-25 15:00:00          1          1          1          0         1  \n",
      "2013-02-25 16:00:00          1          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Change detected in area binary186, index: 959\n",
      "date: 2012-10-20 15:00:00\n",
      "Drift detected at:  2012-10-20 15:00:00\n",
      ">> Current Time:  23/01/2020 11:50:20\n",
      "## ++ previous detected dates:  [Timestamp('2012-09-10 15:00:00'), Timestamp('2012-10-20 15:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-10-20 15:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-10-20 15:00:00'), Timestamp('2012-10-20 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-10-20 15:00:00'), Timestamp('2012-10-20 15:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count23__trainsize731_s10_2010_e10_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-10-20 15:00:00\n",
      ">last_train_set_year:  2012-10-20 15:00:00\n",
      "start_validation_set_year:  2012-10-20 15:00:00\n",
      "end_validation_set_year:  2012-10-20 15:00:00\n",
      "start_test_set_year:  2012-10-20 15:00:00\n",
      "end_test_set_year:  2012-10-20 15:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-10-20 15:00:00\n",
      "last_train_set_year:  2012-10-20 15:00:00\n",
      "start_validation_set_year:  2012-10-20 15:00:00\n",
      "start_test_set_year:  2012-10-20 15:00:00\n",
      "end_validation_set_year:  2012-10-20 15:00:00\n",
      "end_test_set_year:  2012-10-20 15:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-10-20 15:00:00\n",
      "last_train_set_year:  2012-10-20 15:00:00\n",
      "start_validation_set_year:  2012-10-20 15:00:00\n",
      "start_test_set_year:  2012-10-20 15:00:00\n",
      "end_validation_set_year:  2012-10-20 15:00:00\n",
      "end_test_set_year:  2012-10-20 15:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 15us/step - loss: 0.3506 - mean_absolute_error: 0.4299 - val_loss: 0.2686 - val_mean_absolute_error: 0.4347\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2740 - mean_absolute_error: 0.3784 - val_loss: 0.1991 - val_mean_absolute_error: 0.3693\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2572 - mean_absolute_error: 0.3659 - val_loss: 0.2060 - val_mean_absolute_error: 0.3797\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2461 - mean_absolute_error: 0.3575 - val_loss: 0.1918 - val_mean_absolute_error: 0.3637\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2387 - mean_absolute_error: 0.3520 - val_loss: 0.1593 - val_mean_absolute_error: 0.3161\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2336 - mean_absolute_error: 0.3477 - val_loss: 0.1839 - val_mean_absolute_error: 0.3571\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2287 - mean_absolute_error: 0.3442 - val_loss: 0.1894 - val_mean_absolute_error: 0.3612\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2252 - mean_absolute_error: 0.3417 - val_loss: 0.1630 - val_mean_absolute_error: 0.3300\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2219 - mean_absolute_error: 0.3391 - val_loss: 0.1413 - val_mean_absolute_error: 0.3009\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2193 - mean_absolute_error: 0.3371 - val_loss: 0.1545 - val_mean_absolute_error: 0.3134\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2170 - mean_absolute_error: 0.3353 - val_loss: 0.1511 - val_mean_absolute_error: 0.3266\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 4s 13us/step - loss: 0.2140 - mean_absolute_error: 0.3331 - val_loss: 0.1472 - val_mean_absolute_error: 0.3274\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2123 - mean_absolute_error: 0.3321 - val_loss: 0.1569 - val_mean_absolute_error: 0.3133\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2106 - mean_absolute_error: 0.3310 - val_loss: 0.1626 - val_mean_absolute_error: 0.3474\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2101 - mean_absolute_error: 0.3303 - val_loss: 0.1881 - val_mean_absolute_error: 0.3657\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2087 - mean_absolute_error: 0.3294 - val_loss: 0.1285 - val_mean_absolute_error: 0.2952\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2078 - mean_absolute_error: 0.3287 - val_loss: 0.1464 - val_mean_absolute_error: 0.3155\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2065 - mean_absolute_error: 0.3277 - val_loss: 0.1414 - val_mean_absolute_error: 0.3060\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2054 - mean_absolute_error: 0.3269 - val_loss: 0.1387 - val_mean_absolute_error: 0.2998\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2049 - mean_absolute_error: 0.3267 - val_loss: 0.1566 - val_mean_absolute_error: 0.3218\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  7\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-10-20 15:00:00'), Timestamp('2012-10-20 15:00:00')]\n",
      "year_list given:  [Timestamp('2010-10-20 15:00:00'), Timestamp('2012-10-20 15:00:00'), Timestamp('2012-10-20 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80660/80660 [==============================] - 2s 28us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 41us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[67.57617479809649]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17468877576735042\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17468877576735042\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24686255733077966\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2468625573307797\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25250743410110377\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25250743410110377\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23612721133007408\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23612721133007408\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16743107706264804\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16743107706264806\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23214555717957763\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23214555717957763\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1731263545184214\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17312635451842145\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18033365253767453\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1803336525376745\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21435411521596692\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21435411521596695\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2720124993699914\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27201249936999145\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21969658787359503\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21969658787359508\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17655360112897533\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17655360112897536\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1994859130084169\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19948591300841692\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20230835139357894\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20230835139357894\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19812509450128524\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1981250945012852\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18829696083866743\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1882969608386674\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15538531324026006\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15538531324026006\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17851922786149887\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1785192278614989\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38092838062597656\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3809283806259765\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1709591250441006\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1709591250441006\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-10-20 16:00:00          0          0          1         1          1   \n",
      "2012-10-20 17:00:00          1          0          1         0          1   \n",
      "2012-10-20 18:00:00          0          0          1         1          1   \n",
      "2012-10-20 19:00:00          0          1          0         1          1   \n",
      "2012-10-20 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-10-20 16:00:00          1          1          0         1          0   \n",
      "2012-10-20 17:00:00          0          1          0         1          0   \n",
      "2012-10-20 18:00:00          0          0          1         0          0   \n",
      "2012-10-20 19:00:00          1          0          0         0          0   \n",
      "2012-10-20 20:00:00          0          1          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-10-20 16:00:00          1          0          0         0          1   \n",
      "2012-10-20 17:00:00          1          1          1         1          0   \n",
      "2012-10-20 18:00:00          1          1          0         1          0   \n",
      "2012-10-20 19:00:00          1          1          0         1          0   \n",
      "2012-10-20 20:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-10-20 16:00:00          0          0          1          0         1  \n",
      "2012-10-20 17:00:00          1          1          1          1         0  \n",
      "2012-10-20 18:00:00          0          0          1          0         1  \n",
      "2012-10-20 19:00:00          0          1          1          0         0  \n",
      "2012-10-20 20:00:00          1          0          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-04-06 12:00:00          1          0          1         1          1   \n",
      "2013-04-06 13:00:00          1          0          0         1          1   \n",
      "2013-04-06 14:00:00          1          1          1         1          1   \n",
      "2013-04-06 15:00:00          0          0          0         1          0   \n",
      "2013-04-06 16:00:00          1          0          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-04-06 12:00:00          1          1          1         1          0   \n",
      "2013-04-06 13:00:00          1          1          1         1          0   \n",
      "2013-04-06 14:00:00          1          1          0         0          1   \n",
      "2013-04-06 15:00:00          0          0          0         0          0   \n",
      "2013-04-06 16:00:00          1          0          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-04-06 12:00:00          0          1          1         1          1   \n",
      "2013-04-06 13:00:00          1          1          0         1          1   \n",
      "2013-04-06 14:00:00          1          1          0         1          1   \n",
      "2013-04-06 15:00:00          0          0          0         0          0   \n",
      "2013-04-06 16:00:00          0          0          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-04-06 12:00:00          0          1          1          1         1  \n",
      "2013-04-06 13:00:00          1          1          1          0         1  \n",
      "2013-04-06 14:00:00          0          1          1          0         1  \n",
      "2013-04-06 15:00:00          0          1          1          1         1  \n",
      "2013-04-06 16:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary249, index: 447\n",
      "date: 2012-11-08 07:00:00\n",
      "Drift detected at:  2012-11-08 07:00:00\n",
      ">> Current Time:  23/01/2020 11:52:44\n",
      "## ++ previous detected dates:  [Timestamp('2012-10-20 15:00:00'), Timestamp('2012-11-08 07:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00'), Timestamp('2010-10-20 15:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-11-08 07:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-11-08 07:00:00'), Timestamp('2012-11-08 07:00:00')]\n",
      "year_list given:  [Timestamp('2010-11-08 07:00:00'), Timestamp('2012-11-08 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count25__trainsize731_s11_2010_e11_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-11-08 07:00:00\n",
      ">last_train_set_year:  2012-11-08 07:00:00\n",
      "start_validation_set_year:  2012-11-08 07:00:00\n",
      "end_validation_set_year:  2012-11-08 07:00:00\n",
      "start_test_set_year:  2012-11-08 07:00:00\n",
      "end_test_set_year:  2012-11-08 07:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-11-08 07:00:00\n",
      "last_train_set_year:  2012-11-08 07:00:00\n",
      "start_validation_set_year:  2012-11-08 07:00:00\n",
      "start_test_set_year:  2012-11-08 07:00:00\n",
      "end_validation_set_year:  2012-11-08 07:00:00\n",
      "end_test_set_year:  2012-11-08 07:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# adjusted dates..\n",
      "start_train_year:  2010-11-08 07:00:00\n",
      "last_train_set_year:  2012-11-08 07:00:00\n",
      "start_validation_set_year:  2012-11-08 07:00:00\n",
      "start_test_set_year:  2012-11-08 07:00:00\n",
      "end_validation_set_year:  2012-11-08 07:00:00\n",
      "end_test_set_year:  2012-11-08 07:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 15us/step - loss: 0.3481 - mean_absolute_error: 0.4269 - val_loss: 0.9056 - val_mean_absolute_error: 0.8148\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2722 - mean_absolute_error: 0.3768 - val_loss: 0.7985 - val_mean_absolute_error: 0.7670\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2551 - mean_absolute_error: 0.3642 - val_loss: 0.5277 - val_mean_absolute_error: 0.6050\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2449 - mean_absolute_error: 0.3564 - val_loss: 0.6077 - val_mean_absolute_error: 0.6693\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2373 - mean_absolute_error: 0.3507 - val_loss: 0.5841 - val_mean_absolute_error: 0.6710\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2321 - mean_absolute_error: 0.3466 - val_loss: 0.5919 - val_mean_absolute_error: 0.6707\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2277 - mean_absolute_error: 0.3430 - val_loss: 0.3787 - val_mean_absolute_error: 0.5346\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2250 - mean_absolute_error: 0.3411 - val_loss: 0.2959 - val_mean_absolute_error: 0.4749\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2221 - mean_absolute_error: 0.3393 - val_loss: 0.2968 - val_mean_absolute_error: 0.4632\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2199 - mean_absolute_error: 0.3378 - val_loss: 0.3557 - val_mean_absolute_error: 0.5220\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2177 - mean_absolute_error: 0.3362 - val_loss: 0.3766 - val_mean_absolute_error: 0.5421\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2158 - mean_absolute_error: 0.3346 - val_loss: 0.2582 - val_mean_absolute_error: 0.4344\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2145 - mean_absolute_error: 0.3337 - val_loss: 0.3596 - val_mean_absolute_error: 0.5280\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2131 - mean_absolute_error: 0.3327 - val_loss: 0.2874 - val_mean_absolute_error: 0.4653\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2119 - mean_absolute_error: 0.3318 - val_loss: 0.2218 - val_mean_absolute_error: 0.4024\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2113 - mean_absolute_error: 0.3312 - val_loss: 0.2541 - val_mean_absolute_error: 0.4305\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2098 - mean_absolute_error: 0.3302 - val_loss: 0.2396 - val_mean_absolute_error: 0.4227\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2090 - mean_absolute_error: 0.3296 - val_loss: 0.1910 - val_mean_absolute_error: 0.3641\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2086 - mean_absolute_error: 0.3300 - val_loss: 0.2854 - val_mean_absolute_error: 0.4518\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2072 - mean_absolute_error: 0.3286 - val_loss: 0.2239 - val_mean_absolute_error: 0.3896\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  8\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-11-08 07:00:00'), Timestamp('2012-11-08 07:00:00')]\n",
      "year_list given:  [Timestamp('2010-11-08 07:00:00'), Timestamp('2012-11-08 07:00:00'), Timestamp('2012-11-08 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 26us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 54us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[66.5033298090801]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17314801123761647\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17314801123761644\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24579821578195082\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24579821578195082\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25131844842032625\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2513184484203263\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23638424762186405\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23638424762186405\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16708561289368629\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16708561289368623\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22992754694662132\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22992754694662132\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1725072699492336\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17250726994923357\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17911183399871855\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17911183399871852\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21361328798856527\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21361328798856524\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2700478091576717\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2700478091576716\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21864064271280004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  142\n",
      "Share of wrongly classified observations:  0.21864064271280004\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17852038050174968\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1785203805017497\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19853122381586086\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1985312238158608\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20134062792646257\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20134062792646262\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1963625609936419\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19636256099364188\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18877224111587565\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1887722411158756\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15658731332248998\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15658731332249\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1788161072502341\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1788161072502341\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3812410665878062\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3812410665878062\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17186652866085073\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1718665286608507\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-11-08 08:00:00          1          1          1         1          1   \n",
      "2012-11-08 09:00:00          1          1          1         0          1   \n",
      "2012-11-08 10:00:00          1          0          1         1          1   \n",
      "2012-11-08 11:00:00          0          1          0         0          1   \n",
      "2012-11-08 12:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-11-08 08:00:00          1          0          1         1          1   \n",
      "2012-11-08 09:00:00          1          1          0         0          1   \n",
      "2012-11-08 10:00:00          1          0          1         1          1   \n",
      "2012-11-08 11:00:00          0          1          1         0          0   \n",
      "2012-11-08 12:00:00          0          1          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-11-08 08:00:00          1          1          1         0          0   \n",
      "2012-11-08 09:00:00          1          0          1         1          0   \n",
      "2012-11-08 10:00:00          0          0          1         1          1   \n",
      "2012-11-08 11:00:00          0          0          1         1          1   \n",
      "2012-11-08 12:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-11-08 08:00:00          1          0          1          0         1  \n",
      "2012-11-08 09:00:00          0          1          0          0         0  \n",
      "2012-11-08 10:00:00          1          1          0          1         1  \n",
      "2012-11-08 11:00:00          1          0          0          0         1  \n",
      "2012-11-08 12:00:00          1          1          1          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-04-25 04:00:00          1          1          1         1          1   \n",
      "2013-04-25 05:00:00          1          0          1         0          1   \n",
      "2013-04-25 06:00:00          1          1          1         1          1   \n",
      "2013-04-25 07:00:00          1          1          1         1          1   \n",
      "2013-04-25 08:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-04-25 04:00:00          1          1          1         1          1   \n",
      "2013-04-25 05:00:00          1          1          1         1          1   \n",
      "2013-04-25 06:00:00          1          1          1         1          1   \n",
      "2013-04-25 07:00:00          1          1          1         1          1   \n",
      "2013-04-25 08:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-04-25 04:00:00          1          1          1         1          1   \n",
      "2013-04-25 05:00:00          1          1          1         0          1   \n",
      "2013-04-25 06:00:00          1          1          1         1          1   \n",
      "2013-04-25 07:00:00          1          1          1         1          1   \n",
      "2013-04-25 08:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-04-25 04:00:00          1          1          1          1         1  \n",
      "2013-04-25 05:00:00          1          0          1          1         1  \n",
      "2013-04-25 06:00:00          1          1          0          0         1  \n",
      "2013-04-25 07:00:00          1          1          1          0         1  \n",
      "2013-04-25 08:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary249, index: 639\n",
      "date: 2012-12-04 23:00:00\n",
      "Drift detected at:  2012-12-04 23:00:00\n",
      ">> Current Time:  23/01/2020 11:55:10\n",
      "## ++ previous detected dates:  [Timestamp('2012-11-08 07:00:00'), Timestamp('2012-12-04 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00'), Timestamp('2010-10-20 15:00:00'), Timestamp('2010-11-08 07:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-12-04 23:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-12-04 23:00:00'), Timestamp('2012-12-04 23:00:00')]\n",
      "year_list given:  [Timestamp('2010-12-04 23:00:00'), Timestamp('2012-12-04 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count27__trainsize731_s12_2010_e12_2012__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-12-04 23:00:00\n",
      ">last_train_set_year:  2012-12-04 23:00:00\n",
      "start_validation_set_year:  2012-12-04 23:00:00\n",
      "end_validation_set_year:  2012-12-04 23:00:00\n",
      "start_test_set_year:  2012-12-04 23:00:00\n",
      "end_test_set_year:  2012-12-04 23:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-12-04 23:00:00\n",
      "last_train_set_year:  2012-12-04 23:00:00\n",
      "start_validation_set_year:  2012-12-04 23:00:00\n",
      "start_test_set_year:  2012-12-04 23:00:00\n",
      "end_validation_set_year:  2012-12-04 23:00:00\n",
      "end_test_set_year:  2012-12-04 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-12-04 23:00:00\n",
      "last_train_set_year:  2012-12-04 23:00:00\n",
      "start_validation_set_year:  2012-12-04 23:00:00\n",
      "start_test_set_year:  2012-12-04 23:00:00\n",
      "end_validation_set_year:  2012-12-04 23:00:00\n",
      "end_test_set_year:  2012-12-04 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 6s 16us/step - loss: 0.3599 - mean_absolute_error: 0.4342 - val_loss: 0.3832 - val_mean_absolute_error: 0.4385\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2749 - mean_absolute_error: 0.3787 - val_loss: 0.4369 - val_mean_absolute_error: 0.4495\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2584 - mean_absolute_error: 0.3660 - val_loss: 0.3397 - val_mean_absolute_error: 0.3965\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2474 - mean_absolute_error: 0.3583 - val_loss: 0.3803 - val_mean_absolute_error: 0.4213\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2401 - mean_absolute_error: 0.3524 - val_loss: 0.3326 - val_mean_absolute_error: 0.4207\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2349 - mean_absolute_error: 0.3482 - val_loss: 0.3624 - val_mean_absolute_error: 0.4096\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2304 - mean_absolute_error: 0.3450 - val_loss: 0.3770 - val_mean_absolute_error: 0.4122\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2268 - mean_absolute_error: 0.3423 - val_loss: 0.3852 - val_mean_absolute_error: 0.4187\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2228 - mean_absolute_error: 0.3394 - val_loss: 0.3786 - val_mean_absolute_error: 0.4097\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2207 - mean_absolute_error: 0.3379 - val_loss: 0.3670 - val_mean_absolute_error: 0.4105\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2185 - mean_absolute_error: 0.3361 - val_loss: 0.3411 - val_mean_absolute_error: 0.3876\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2166 - mean_absolute_error: 0.3352 - val_loss: 0.3478 - val_mean_absolute_error: 0.3896\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2149 - mean_absolute_error: 0.3335 - val_loss: 0.2882 - val_mean_absolute_error: 0.3572\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2131 - mean_absolute_error: 0.3327 - val_loss: 0.3222 - val_mean_absolute_error: 0.3678\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2113 - mean_absolute_error: 0.3312 - val_loss: 0.3225 - val_mean_absolute_error: 0.3753\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2108 - mean_absolute_error: 0.3306 - val_loss: 0.3110 - val_mean_absolute_error: 0.3927\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2092 - mean_absolute_error: 0.3295 - val_loss: 0.2691 - val_mean_absolute_error: 0.3489\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2088 - mean_absolute_error: 0.3292 - val_loss: 0.2677 - val_mean_absolute_error: 0.3493\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2082 - mean_absolute_error: 0.3285 - val_loss: 0.3020 - val_mean_absolute_error: 0.3900\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2068 - mean_absolute_error: 0.3275 - val_loss: 0.2748 - val_mean_absolute_error: 0.3499\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  9\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-12-04 23:00:00'), Timestamp('2012-12-04 23:00:00')]\n",
      "year_list given:  [Timestamp('2010-12-04 23:00:00'), Timestamp('2012-12-04 23:00:00'), Timestamp('2012-12-05 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 31us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 58us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[64.88373778256755]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17205791007692672\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17205791007692675\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2422953796167997\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24229537961679964\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25056142195040376\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25056142195040376\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2322136748052941\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2322136748052941\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16632423909408\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16632423909407998\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23030245114434522\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23030245114434517\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17172344593626065\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17172344593626068\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17769601987672612\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17769601987672606\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2132925605618997\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21329256056189977\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27005590329208273\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27005590329208273\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2183573032634144\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2183573032634144\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17702709159539398\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17702709159539395\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19714272062688132\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19714272062688137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  68\n",
      "Share of wrongly classified observations:  0.19967509197763866\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1996750919776387\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1959004252472646\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19590042524726456\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1881121888288977\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18811218882889771\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15600363132495576\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15600363132495582\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1762626021310144\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17626260213101438\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38162358449997613\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38162358449997613\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17162788475321322\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17162788475321325\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2012-12-05 00:00:00          1          0          1         1          1   \n",
      "2012-12-05 01:00:00          1          1          1         1          1   \n",
      "2012-12-05 02:00:00          1          1          1         1          1   \n",
      "2012-12-05 03:00:00          1          1          1         0          1   \n",
      "2012-12-05 04:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2012-12-05 00:00:00          1          1          1         1          0   \n",
      "2012-12-05 01:00:00          1          1          1         1          1   \n",
      "2012-12-05 02:00:00          1          1          1         1          1   \n",
      "2012-12-05 03:00:00          1          1          1         1          1   \n",
      "2012-12-05 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2012-12-05 00:00:00          0          1          1         1          1   \n",
      "2012-12-05 01:00:00          1          0          1         1          1   \n",
      "2012-12-05 02:00:00          1          1          1         1          1   \n",
      "2012-12-05 03:00:00          1          1          1         1          1   \n",
      "2012-12-05 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2012-12-05 00:00:00          1          1          0          0         1  \n",
      "2012-12-05 01:00:00          1          1          1          0         1  \n",
      "2012-12-05 02:00:00          1          1          1          1         1  \n",
      "2012-12-05 03:00:00          1          1          1          1         1  \n",
      "2012-12-05 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-05-21 20:00:00          1          1          1         1          0   \n",
      "2013-05-21 21:00:00          1          0          0         1          1   \n",
      "2013-05-21 22:00:00          0          0          1         1          1   \n",
      "2013-05-21 23:00:00          1          0          1         1          1   \n",
      "2013-05-22 00:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-05-21 20:00:00          0          1          1         1          1   \n",
      "2013-05-21 21:00:00          1          0          0         1          1   \n",
      "2013-05-21 22:00:00          0          1          0         1          0   \n",
      "2013-05-21 23:00:00          1          1          1         1          0   \n",
      "2013-05-22 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-05-21 20:00:00          1          1          1         1          1   \n",
      "2013-05-21 21:00:00          0          1          0         1          0   \n",
      "2013-05-21 22:00:00          0          1          1         1          0   \n",
      "2013-05-21 23:00:00          0          1          1         1          1   \n",
      "2013-05-22 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-05-21 20:00:00          1          1          1          1         1  \n",
      "2013-05-21 21:00:00          1          1          0          0         0  \n",
      "2013-05-21 22:00:00          1          1          1          0         1  \n",
      "2013-05-21 23:00:00          1          1          1          1         1  \n",
      "2013-05-22 00:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary170, index: 1407\n",
      "date: 2013-02-01 15:00:00\n",
      "Drift detected at:  2013-02-01 15:00:00\n",
      ">> Current Time:  23/01/2020 11:57:38\n",
      "## ++ previous detected dates:  [Timestamp('2012-12-04 23:00:00'), Timestamp('2013-02-01 15:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00'), Timestamp('2010-10-20 15:00:00'), Timestamp('2010-11-08 07:00:00'), Timestamp('2010-12-04 23:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2011-02-01 15:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00')]\n",
      "year_list given:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count29__trainsize731_s2_2011_e2_2013__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2011-02-01 15:00:00\n",
      ">last_train_set_year:  2013-02-01 15:00:00\n",
      "start_validation_set_year:  2013-02-01 15:00:00\n",
      "end_validation_set_year:  2013-02-01 15:00:00\n",
      "start_test_set_year:  2013-02-01 15:00:00\n",
      "end_test_set_year:  2013-02-01 15:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2011-02-01 15:00:00\n",
      "last_train_set_year:  2013-02-01 15:00:00\n",
      "start_validation_set_year:  2013-02-01 15:00:00\n",
      "start_test_set_year:  2013-02-01 15:00:00\n",
      "end_validation_set_year:  2013-02-01 15:00:00\n",
      "end_test_set_year:  2013-02-01 15:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2011-02-01 15:00:00\n",
      "last_train_set_year:  2013-02-01 15:00:00\n",
      "start_validation_set_year:  2013-02-01 15:00:00\n",
      "start_test_set_year:  2013-02-01 15:00:00\n",
      "end_validation_set_year:  2013-02-01 15:00:00\n",
      "end_test_set_year:  2013-02-01 15:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 16us/step - loss: 0.3636 - mean_absolute_error: 0.4332 - val_loss: 0.1431 - val_mean_absolute_error: 0.2876\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2742 - mean_absolute_error: 0.3789 - val_loss: 0.1610 - val_mean_absolute_error: 0.2988\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2580 - mean_absolute_error: 0.3666 - val_loss: 0.1740 - val_mean_absolute_error: 0.3203\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2469 - mean_absolute_error: 0.3581 - val_loss: 0.1326 - val_mean_absolute_error: 0.2721\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2398 - mean_absolute_error: 0.3524 - val_loss: 0.1314 - val_mean_absolute_error: 0.2694\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2342 - mean_absolute_error: 0.3483 - val_loss: 0.1431 - val_mean_absolute_error: 0.2887\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2296 - mean_absolute_error: 0.3450 - val_loss: 0.1354 - val_mean_absolute_error: 0.2730\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2263 - mean_absolute_error: 0.3425 - val_loss: 0.1759 - val_mean_absolute_error: 0.3242\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2232 - mean_absolute_error: 0.3404 - val_loss: 0.1390 - val_mean_absolute_error: 0.2844\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2210 - mean_absolute_error: 0.3379 - val_loss: 0.1460 - val_mean_absolute_error: 0.2911\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2197 - mean_absolute_error: 0.3374 - val_loss: 0.1395 - val_mean_absolute_error: 0.2814\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2173 - mean_absolute_error: 0.3359 - val_loss: 0.1506 - val_mean_absolute_error: 0.3043\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2158 - mean_absolute_error: 0.3347 - val_loss: 0.1397 - val_mean_absolute_error: 0.2783\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2148 - mean_absolute_error: 0.3338 - val_loss: 0.1434 - val_mean_absolute_error: 0.2769\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2129 - mean_absolute_error: 0.3325 - val_loss: 0.1532 - val_mean_absolute_error: 0.2946\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2116 - mean_absolute_error: 0.3315 - val_loss: 0.1208 - val_mean_absolute_error: 0.2533\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2106 - mean_absolute_error: 0.3311 - val_loss: 0.1227 - val_mean_absolute_error: 0.2681\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2099 - mean_absolute_error: 0.3305 - val_loss: 0.1304 - val_mean_absolute_error: 0.2645\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2079 - mean_absolute_error: 0.3289 - val_loss: 0.1391 - val_mean_absolute_error: 0.2777\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2088 - mean_absolute_error: 0.3297 - val_loss: 0.1308 - val_mean_absolute_error: 0.2653\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  10\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> ceiling_flag_test set to \"True\", end of dataset is reached with preds of test_set or in next iteration\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00')]\n",
      "year_list given:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00'), Timestamp('2013-02-01 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "80660/80660 [==============================] - 2s 26us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 50us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.78990561646915]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17110623628956445\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1711062362895644\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24201996687111071\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24201996687111071\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25133187088686937\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2513318708868693\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22930563638805568\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2293056363880557\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16376415812329315\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1637641581232932\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22966378654250796\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22966378654250794\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17074808613511216\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17074808613511214\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18019429645879037\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18019429645879034\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2118458163585083\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2118458163585083\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2680753906075122\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2680753906075122\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2155616242109505\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21556162421095043\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1758517258360568\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17585172583605677\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19895241079822712\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19895241079822715\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20195191834176474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  68\n",
      "Share of wrongly classified observations:  0.2019519183417648\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19429645879034785\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19429645879034785\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18758114339436804\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1875811433943681\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15467609795406723\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15467609795406725\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17607556968258942\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17607556968258942\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3839817343421229\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38398173434212296\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16904687290146392\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16904687290146395\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-02-01 16:00:00          1          1          0         1          1   \n",
      "2013-02-01 17:00:00          1          0          1         1          1   \n",
      "2013-02-01 18:00:00          1          1          1         0          1   \n",
      "2013-02-01 19:00:00          1          0          1         1          1   \n",
      "2013-02-01 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-02-01 16:00:00          1          1          1         1          1   \n",
      "2013-02-01 17:00:00          1          1          1         1          1   \n",
      "2013-02-01 18:00:00          1          1          1         1          1   \n",
      "2013-02-01 19:00:00          1          1          1         1          1   \n",
      "2013-02-01 20:00:00          1          1          0         0          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-02-01 16:00:00          1          1          1         1          0   \n",
      "2013-02-01 17:00:00          0          1          0         1          1   \n",
      "2013-02-01 18:00:00          1          0          0         1          1   \n",
      "2013-02-01 19:00:00          1          1          1         1          1   \n",
      "2013-02-01 20:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-02-01 16:00:00          0          0          1          0         0  \n",
      "2013-02-01 17:00:00          0          0          1          1         1  \n",
      "2013-02-01 18:00:00          1          1          0          0         1  \n",
      "2013-02-01 19:00:00          1          1          1          0         1  \n",
      "2013-02-01 20:00:00          0          1          0          0         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-07-19 12:00:00          1          1          1         1          1   \n",
      "2013-07-19 13:00:00          1          1          1         1          1   \n",
      "2013-07-19 14:00:00          1          1          1         1          1   \n",
      "2013-07-19 15:00:00          1          1          1         1          1   \n",
      "2013-07-19 16:00:00          0          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-07-19 12:00:00          1          1          0         1          1   \n",
      "2013-07-19 13:00:00          1          1          0         1          1   \n",
      "2013-07-19 14:00:00          1          1          1         1          1   \n",
      "2013-07-19 15:00:00          1          1          0         1          0   \n",
      "2013-07-19 16:00:00          0          1          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-07-19 12:00:00          1          1          1         1          1   \n",
      "2013-07-19 13:00:00          1          1          1         0          1   \n",
      "2013-07-19 14:00:00          1          1          1         1          1   \n",
      "2013-07-19 15:00:00          0          1          1         1          1   \n",
      "2013-07-19 16:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-07-19 12:00:00          1          1          1          1         1  \n",
      "2013-07-19 13:00:00          1          1          1          1         1  \n",
      "2013-07-19 14:00:00          1          1          1          1         1  \n",
      "2013-07-19 15:00:00          1          1          1          0         1  \n",
      "2013-07-19 16:00:00          1          0          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00')]\n",
      "year_list given:  [Timestamp('2011-02-01 15:00:00'), Timestamp('2013-02-01 15:00:00'), Timestamp('2013-07-19 17:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 41us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[54.14036312656672]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17088776326312982\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17088776326312982\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24166888829645428\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24166888829645428\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25104416600017776\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2510441660001777\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22860570514529455\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22860570514529457\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16364525015551412\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1636452501555141\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22976095263485297\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22976095263485294\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17026570692259846\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17026570692259843\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17972984981782636\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17972984981782636\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21127699280191947\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.2112769928019195\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2677952545987736\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2677952545987737\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21492046565360345\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21492046565360348\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17550875322136317\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1755087532213632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  163\n",
      "Share of wrongly classified observations:  0.19865813560828227\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19865813560828224\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20167955211943478\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2016795521194348\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19412601084155334\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19412601084155337\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18737225628721232\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1873722562872123\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1543144050475429\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15431440504754287\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17595307917888559\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17595307917888564\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3845196836399183\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38451968363991823\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16875499866702215\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16875499866702212\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-07-19 17:00:00          1          0          1         1          1   \n",
      "2013-07-19 18:00:00          0          1          1         1          1   \n",
      "2013-07-19 19:00:00          1          1          0         1          1   \n",
      "2013-07-19 20:00:00          1          0          0         1          1   \n",
      "2013-07-19 21:00:00          1          1          0         1          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-07-19 17:00:00          0          1          1         0          1   \n",
      "2013-07-19 18:00:00          1          1          1         1          1   \n",
      "2013-07-19 19:00:00          1          1          1         1          0   \n",
      "2013-07-19 20:00:00          0          1          1         0          0   \n",
      "2013-07-19 21:00:00          0          1          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-07-19 17:00:00          1          1          0         0          1   \n",
      "2013-07-19 18:00:00          1          1          1         1          1   \n",
      "2013-07-19 19:00:00          1          1          0         1          1   \n",
      "2013-07-19 20:00:00          1          0          1         1          0   \n",
      "2013-07-19 21:00:00          0          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-07-19 17:00:00          1          0          1          0         1  \n",
      "2013-07-19 18:00:00          1          1          1          0         1  \n",
      "2013-07-19 19:00:00          0          1          1          0         1  \n",
      "2013-07-19 20:00:00          1          1          1          1         1  \n",
      "2013-07-19 21:00:00          1          1          0          0         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-07-26 13:00:00          1          1          1         1          1   \n",
      "2013-07-26 14:00:00          1          1          1         1          1   \n",
      "2013-07-26 15:00:00          1          0          0         1          1   \n",
      "2013-07-26 16:00:00          1          1          1         1          1   \n",
      "2013-07-26 17:00:00          1          0          1         1          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-07-26 13:00:00          1          1          1         1          1   \n",
      "2013-07-26 14:00:00          1          1          1         1          1   \n",
      "2013-07-26 15:00:00          0          1          1         0          1   \n",
      "2013-07-26 16:00:00          0          1          1         1          0   \n",
      "2013-07-26 17:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-07-26 13:00:00          1          1          1         1          1   \n",
      "2013-07-26 14:00:00          1          1          1         1          1   \n",
      "2013-07-26 15:00:00          1          1          1         1          1   \n",
      "2013-07-26 16:00:00          0          1          1         0          1   \n",
      "2013-07-26 17:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-07-26 13:00:00          1          1          1          1         1  \n",
      "2013-07-26 14:00:00          1          1          1          1         1  \n",
      "2013-07-26 15:00:00          1          1          1          0         1  \n",
      "2013-07-26 16:00:00          1          0          1          1         1  \n",
      "2013-07-26 17:00:00          1          1          0          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area binary230, index: 62\n",
      "date: 2013-07-22 07:00:00\n",
      "Drift detected at:  2013-07-22 07:00:00\n",
      ">> Current Time:  23/01/2020 12:00:54\n",
      "## ++ previous detected dates:  [Timestamp('2013-02-01 15:00:00'), Timestamp('2013-07-22 07:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00'), Timestamp('2010-10-20 15:00:00'), Timestamp('2010-11-08 07:00:00'), Timestamp('2010-12-04 23:00:00'), Timestamp('2011-02-01 15:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2011-07-22 07:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2011-07-22 07:00:00'), Timestamp('2013-07-22 07:00:00')]\n",
      "year_list given:  [Timestamp('2011-07-22 07:00:00'), Timestamp('2013-07-22 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count32__trainsize731_s7_2011_e7_2013__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2011-07-22 07:00:00\n",
      ">last_train_set_year:  2013-07-22 07:00:00\n",
      "start_validation_set_year:  2013-07-22 07:00:00\n",
      "end_validation_set_year:  2013-07-22 07:00:00\n",
      "start_test_set_year:  2013-07-22 07:00:00\n",
      "end_test_set_year:  2013-07-22 07:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2011-07-22 07:00:00\n",
      "last_train_set_year:  2013-07-22 07:00:00\n",
      "start_validation_set_year:  2013-07-22 07:00:00\n",
      "start_test_set_year:  2013-07-22 07:00:00\n",
      "end_validation_set_year:  2013-07-22 07:00:00\n",
      "end_test_set_year:  2013-07-22 07:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2011-07-22 07:00:00\n",
      "last_train_set_year:  2013-07-22 07:00:00\n",
      "start_validation_set_year:  2013-07-22 07:00:00\n",
      "start_test_set_year:  2013-07-22 07:00:00\n",
      "end_validation_set_year:  2013-07-22 07:00:00\n",
      "end_test_set_year:  2013-07-22 07:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 6s 17us/step - loss: 0.3524 - mean_absolute_error: 0.4300 - val_loss: 0.1677 - val_mean_absolute_error: 0.3413\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2750 - mean_absolute_error: 0.3792 - val_loss: 0.1528 - val_mean_absolute_error: 0.3194\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2575 - mean_absolute_error: 0.3656 - val_loss: 0.1647 - val_mean_absolute_error: 0.3311\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2478 - mean_absolute_error: 0.3579 - val_loss: 0.1566 - val_mean_absolute_error: 0.3205\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2404 - mean_absolute_error: 0.3520 - val_loss: 0.1466 - val_mean_absolute_error: 0.3237\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2346 - mean_absolute_error: 0.3479 - val_loss: 0.1308 - val_mean_absolute_error: 0.3026\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2311 - mean_absolute_error: 0.3453 - val_loss: 0.1476 - val_mean_absolute_error: 0.3174\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2270 - mean_absolute_error: 0.3423 - val_loss: 0.1358 - val_mean_absolute_error: 0.3167\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2240 - mean_absolute_error: 0.3396 - val_loss: 0.1537 - val_mean_absolute_error: 0.3300\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2216 - mean_absolute_error: 0.3380 - val_loss: 0.1293 - val_mean_absolute_error: 0.3021\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2190 - mean_absolute_error: 0.3364 - val_loss: 0.1245 - val_mean_absolute_error: 0.3063\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2167 - mean_absolute_error: 0.3346 - val_loss: 0.1266 - val_mean_absolute_error: 0.3101\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2158 - mean_absolute_error: 0.3344 - val_loss: 0.1313 - val_mean_absolute_error: 0.3034\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2140 - mean_absolute_error: 0.3326 - val_loss: 0.1324 - val_mean_absolute_error: 0.2995\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2137 - mean_absolute_error: 0.3325 - val_loss: 0.1285 - val_mean_absolute_error: 0.3082\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2117 - mean_absolute_error: 0.3314 - val_loss: 0.1365 - val_mean_absolute_error: 0.3092\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2115 - mean_absolute_error: 0.3307 - val_loss: 0.1253 - val_mean_absolute_error: 0.2966\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2101 - mean_absolute_error: 0.3302 - val_loss: 0.1447 - val_mean_absolute_error: 0.3328\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2089 - mean_absolute_error: 0.3291 - val_loss: 0.1570 - val_mean_absolute_error: 0.3466\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2090 - mean_absolute_error: 0.3291 - val_loss: 0.1395 - val_mean_absolute_error: 0.3268\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  11\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> end of dataset is reached with preds of valid_set --> get last predictions with model\n",
      "current valid date:  2013-07-22 08:00:00\n",
      "current test date:  None\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2011-07-22 07:00:00'), Timestamp('2013-07-22 07:00:00')]\n",
      "year_list given:  [Timestamp('2011-07-22 07:00:00'), Timestamp('2013-07-22 07:00:00'), Timestamp('2013-07-22 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "78080/78080 [==============================] - 2s 30us/step\n",
      "Shape of org. dataset after shift:  (3904, 20)\n",
      "20/20 [==============================] - 0s 52us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.30872018342869]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17282542579075422\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.17282542579075424\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2438412408759124\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2438412408759124\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25171076642335766\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25171076642335766\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2295088199513382\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2295088199513382\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16742700729927007\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16742700729927007\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.23410888077858882\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2341088807785888\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17217913625304138\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17217913625304138\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18088503649635035\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18088503649635038\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21205900243309006\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21205900243309003\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2715556569343066\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27155565693430656\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21772354014598538\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2177235401459854\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1778436739659367\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17784367396593673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  163\n",
      "Share of wrongly classified observations:  0.20217457420924578\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.20217457420924576\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2019084549878345\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20190845498783455\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19286040145985406\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.192860401459854\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18727189781021902\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18727189781021897\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15309458637469586\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15309458637469586\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1754105839416058\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17541058394160583\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3810827250608273\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.38108272506082724\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1671228710462287\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16712287104622872\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (3904, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-07-22 08:00:00          1          1          0         1          1   \n",
      "2013-07-22 09:00:00          1          1          0         1          1   \n",
      "2013-07-22 10:00:00          1          1          1         1          1   \n",
      "2013-07-22 11:00:00          0          0          1         0          0   \n",
      "2013-07-22 12:00:00          1          0          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-07-22 08:00:00          1          1          1         1          1   \n",
      "2013-07-22 09:00:00          1          1          1         1          1   \n",
      "2013-07-22 10:00:00          1          1          1         1          1   \n",
      "2013-07-22 11:00:00          0          0          0         1          1   \n",
      "2013-07-22 12:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-07-22 08:00:00          1          1          1         1          0   \n",
      "2013-07-22 09:00:00          0          0          1         1          0   \n",
      "2013-07-22 10:00:00          1          0          1         0          0   \n",
      "2013-07-22 11:00:00          1          0          0         0          0   \n",
      "2013-07-22 12:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-07-22 08:00:00          1          1          1          1         1  \n",
      "2013-07-22 09:00:00          0          1          1          1         1  \n",
      "2013-07-22 10:00:00          1          1          1          1         1  \n",
      "2013-07-22 11:00:00          0          0          0          1         0  \n",
      "2013-07-22 12:00:00          1          0          0          0         0  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          1         0          0   \n",
      "2013-12-31 20:00:00          0          1          0         0          0   \n",
      "2013-12-31 21:00:00          1          0          0         0          1   \n",
      "2013-12-31 22:00:00          1          0          0         1          0   \n",
      "2013-12-31 23:00:00          1          0          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          1         1          0   \n",
      "2013-12-31 20:00:00          0          0          0         0          1   \n",
      "2013-12-31 21:00:00          1          0          1         1          1   \n",
      "2013-12-31 22:00:00          1          1          1         0          0   \n",
      "2013-12-31 23:00:00          1          0          0         0          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          0         0          0   \n",
      "2013-12-31 20:00:00          0          0          1         0          0   \n",
      "2013-12-31 21:00:00          0          0          0         0          1   \n",
      "2013-12-31 22:00:00          1          1          1         1          1   \n",
      "2013-12-31 23:00:00          1          0          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-12-31 19:00:00          1          0          0          0         0  \n",
      "2013-12-31 20:00:00          0          0          0          0         0  \n",
      "2013-12-31 21:00:00          0          1          0          0         0  \n",
      "2013-12-31 22:00:00          0          1          1          1         0  \n",
      "2013-12-31 23:00:00          0          0          0          0         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary68, index: 3071\n",
      "date: 2013-11-27 07:00:00\n",
      "Drift detected at:  2013-11-27 07:00:00\n",
      ">> Current Time:  23/01/2020 12:03:31\n",
      "## ++ previous detected dates:  [Timestamp('2013-07-22 07:00:00'), Timestamp('2013-11-27 07:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00'), Timestamp('2009-03-09 23:00:00'), Timestamp('2009-04-16 07:00:00'), Timestamp('2009-12-06 23:00:00'), Timestamp('2010-02-06 07:00:00'), Timestamp('2010-05-05 15:00:00'), Timestamp('2010-09-10 15:00:00'), Timestamp('2010-10-20 15:00:00'), Timestamp('2010-11-08 07:00:00'), Timestamp('2010-12-04 23:00:00'), Timestamp('2011-02-01 15:00:00'), Timestamp('2011-07-22 07:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2011-11-27 07:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2011-11-27 07:00:00'), Timestamp('2013-11-27 07:00:00')]\n",
      "year_list given:  [Timestamp('2011-11-27 07:00:00'), Timestamp('2013-11-27 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin_count34__trainsize731_s11_2011_e11_2013__stepsize1__p12_2013 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2011-11-27 07:00:00\n",
      ">last_train_set_year:  2013-11-27 07:00:00\n",
      "start_validation_set_year:  2013-11-27 07:00:00\n",
      "end_validation_set_year:  2013-11-27 07:00:00\n",
      "start_test_set_year:  2013-11-27 07:00:00\n",
      "end_test_set_year:  2013-11-27 07:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2011-11-27 07:00:00\n",
      "last_train_set_year:  2013-11-27 07:00:00\n",
      "start_validation_set_year:  2013-11-27 07:00:00\n",
      "start_test_set_year:  2013-11-27 07:00:00\n",
      "end_validation_set_year:  2013-11-27 07:00:00\n",
      "end_test_set_year:  2013-11-27 07:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2011-11-27 07:00:00\n",
      "last_train_set_year:  2013-11-27 07:00:00\n",
      "start_validation_set_year:  2013-11-27 07:00:00\n",
      "start_test_set_year:  2013-11-27 07:00:00\n",
      "end_validation_set_year:  2013-11-27 07:00:00\n",
      "end_test_set_year:  2013-11-27 07:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      "337440/337440 [==============================] - 5s 16us/step - loss: 0.3404 - mean_absolute_error: 0.4226 - val_loss: 0.3291 - val_mean_absolute_error: 0.4890\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2701 - mean_absolute_error: 0.3744 - val_loss: 0.3291 - val_mean_absolute_error: 0.4735\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2539 - mean_absolute_error: 0.3625 - val_loss: 0.2757 - val_mean_absolute_error: 0.4458\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2438 - mean_absolute_error: 0.3545 - val_loss: 0.2830 - val_mean_absolute_error: 0.4382\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2361 - mean_absolute_error: 0.3482 - val_loss: 0.3669 - val_mean_absolute_error: 0.4990\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2310 - mean_absolute_error: 0.3445 - val_loss: 0.3291 - val_mean_absolute_error: 0.4732\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/20\n",
      "337440/337440 [==============================] - 5s 13us/step - loss: 0.2253 - mean_absolute_error: 0.3402 - val_loss: 0.2015 - val_mean_absolute_error: 0.3660\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2227 - mean_absolute_error: 0.3379 - val_loss: 0.2499 - val_mean_absolute_error: 0.4104\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2190 - mean_absolute_error: 0.3351 - val_loss: 0.2650 - val_mean_absolute_error: 0.4268\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2183 - mean_absolute_error: 0.3344 - val_loss: 0.2244 - val_mean_absolute_error: 0.3756\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 11/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2164 - mean_absolute_error: 0.3332 - val_loss: 0.2939 - val_mean_absolute_error: 0.4387\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 12/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2130 - mean_absolute_error: 0.3311 - val_loss: 0.2579 - val_mean_absolute_error: 0.4128\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 13/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2114 - mean_absolute_error: 0.3295 - val_loss: 0.2799 - val_mean_absolute_error: 0.4132\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 14/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2103 - mean_absolute_error: 0.3291 - val_loss: 0.2902 - val_mean_absolute_error: 0.4520\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 15/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2084 - mean_absolute_error: 0.3274 - val_loss: 0.2358 - val_mean_absolute_error: 0.3816\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 16/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2083 - mean_absolute_error: 0.3277 - val_loss: 0.4587 - val_mean_absolute_error: 0.5562\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 17/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2070 - mean_absolute_error: 0.3266 - val_loss: 0.3831 - val_mean_absolute_error: 0.5098\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 18/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2061 - mean_absolute_error: 0.3261 - val_loss: 0.3144 - val_mean_absolute_error: 0.4642\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 19/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2056 - mean_absolute_error: 0.3258 - val_loss: 0.2721 - val_mean_absolute_error: 0.4263\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 20/20\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2042 - mean_absolute_error: 0.3247 - val_loss: 0.2657 - val_mean_absolute_error: 0.4362\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  12\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> end of dataset is reached with preds of valid_set --> get last predictions with model\n",
      "current valid date:  2013-11-27 08:00:00\n",
      "current test date:  None\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2011-11-27 07:00:00'), Timestamp('2013-11-27 07:00:00')]\n",
      "year_list given:  [Timestamp('2011-11-27 07:00:00'), Timestamp('2013-11-27 07:00:00'), Timestamp('2013-11-27 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin ####\n",
      "16640/16640 [==============================] - 1s 44us/step\n",
      "Shape of org. dataset after shift:  (832, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[69.24672792166777]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1731675790754258\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1731675790754258\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2441073600973236\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2441073600973236\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25163473236009737\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2516347323600973\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.2295468369829684\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.22954683698296838\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16803527980535282\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1680352798053528\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.234375\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.234375\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17274939172749393\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.17274939172749393\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1813792579075426\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18137925790754258\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.212515206812652\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.21251520681265207\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2715176399026764\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2715176399026764\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2176475060827251\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21764750608272507\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.178109793187348\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17810979318734793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  163\n",
      "Share of wrongly classified observations:  0.20209854014598538\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.2020985401459854\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2018324209245742\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2018324209245742\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1933546228710462\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19335462287104624\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1876140510948905\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18761405109489052\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15320863746958635\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15320863746958638\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17598083941605835\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17598083941605838\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3818810827250608\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3818810827250608\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16674270072992703\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.166742700729927\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (832, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-11-27 08:00:00          1          1          1         0          1   \n",
      "2013-11-27 09:00:00          1          1          1         1          1   \n",
      "2013-11-27 10:00:00          1          1          1         1          1   \n",
      "2013-11-27 11:00:00          1          1          1         1          1   \n",
      "2013-11-27 12:00:00          1          1          0         0          0   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-11-27 08:00:00          1          1          1         1          1   \n",
      "2013-11-27 09:00:00          1          1          1         1          1   \n",
      "2013-11-27 10:00:00          1          1          1         1          1   \n",
      "2013-11-27 11:00:00          1          1          1         1          1   \n",
      "2013-11-27 12:00:00          1          1          1         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-11-27 08:00:00          1          1          1         1          0   \n",
      "2013-11-27 09:00:00          1          1          1         1          1   \n",
      "2013-11-27 10:00:00          1          1          1         0          1   \n",
      "2013-11-27 11:00:00          1          1          1         1          1   \n",
      "2013-11-27 12:00:00          1          1          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-11-27 08:00:00          1          1          1          1         0  \n",
      "2013-11-27 09:00:00          1          1          1          1         1  \n",
      "2013-11-27 10:00:00          1          1          1          0         1  \n",
      "2013-11-27 11:00:00          1          1          0          1         1  \n",
      "2013-11-27 12:00:00          0          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          1         0          0   \n",
      "2013-12-31 20:00:00          0          1          0         0          0   \n",
      "2013-12-31 21:00:00          1          0          0         0          1   \n",
      "2013-12-31 22:00:00          0          0          0         1          1   \n",
      "2013-12-31 23:00:00          1          0          1         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          1         1          0   \n",
      "2013-12-31 20:00:00          0          0          0         0          1   \n",
      "2013-12-31 21:00:00          1          0          0         1          1   \n",
      "2013-12-31 22:00:00          1          1          1         0          0   \n",
      "2013-12-31 23:00:00          1          0          0         0          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2013-12-31 19:00:00          1          0          0         0          0   \n",
      "2013-12-31 20:00:00          0          0          1         0          0   \n",
      "2013-12-31 21:00:00          0          0          0         0          1   \n",
      "2013-12-31 22:00:00          1          1          1         1          1   \n",
      "2013-12-31 23:00:00          1          0          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2013-12-31 19:00:00          1          0          0          0         0  \n",
      "2013-12-31 20:00:00          0          0          0          0         0  \n",
      "2013-12-31 21:00:00          0          1          1          0         0  \n",
      "2013-12-31 22:00:00          0          1          1          1         1  \n",
      "2013-12-31 23:00:00          0          1          0          1         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "# >>> End of dataset reached! Stop Predictions\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  0\n",
      ">> Total Number of retrainings:  12\n"
     ]
    }
   ],
   "source": [
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_adwin'\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "retraining_results_tuple = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'ADWIN', \n",
    "                                                    update_retrain_switch=False, first_forecast_range_days=168,\n",
    "                                                    n_epochs_retrain = 20, overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2013-12-31 23:00:00',\n",
    "                                                    verbosity = 2)\n",
    "                               \n",
    "\n",
    "#assign results:\n",
    "all_MODELS_dict_MLP_adwin = retraining_results_tuple[0]\n",
    "all_model_RESULTS_dict_MLP_adwin = retraining_results_tuple[1] \n",
    "all_detected_DATES_dict_MLP_adwin = retraining_results_tuple[2]\n",
    "avg_rmse_resulst_all_adwin = retraining_results_tuple[3]\n",
    "all_retraining_dates_adwin = retraining_results_tuple[4]\n",
    "all_weight_update_dates_adwin = retraining_results_tuple[5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: incremental training/updating of model if drift is detected\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011, verbosity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:30:37.266488Z",
     "start_time": "2020-01-23T11:23:15.011525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "80660/80660 [==============================] - 3s 32us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 54us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary170, index: 1631\n",
      "date: 2011-03-09 23:00:00\n",
      "Drift detected at:  2011-03-09 23:00:00\n",
      ">> Current Time:  23/01/2020 12:23:40\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin_count1__trainsize730_s3_2009_e3_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "336960/336960 [==============================] - 6s 17us/step - loss: 0.2076 - mean_absolute_error: 0.3272\n",
      "Epoch 2/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2053 - mean_absolute_error: 0.3256\n",
      "Epoch 3/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2063 - mean_absolute_error: 0.3264\n",
      "Epoch 4/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2067 - mean_absolute_error: 0.3264\n",
      "Epoch 5/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2056 - mean_absolute_error: 0.3262\n",
      "Epoch 6/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2046 - mean_absolute_error: 0.3252\n",
      "Epoch 7/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2048 - mean_absolute_error: 0.3254\n",
      "Epoch 8/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2050 - mean_absolute_error: 0.3255\n",
      "Epoch 9/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2045 - mean_absolute_error: 0.3255\n",
      "Epoch 10/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2044 - mean_absolute_error: 0.3254\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  1\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-03-09 23:00:00'), Timestamp('2011-03-09 23:00:00'), Timestamp('2011-03-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "80660/80660 [==============================] - 2s 28us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 47us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary79, index: 1023\n",
      "date: 2011-04-21 15:00:00\n",
      "Drift detected at:  2011-04-21 15:00:00\n",
      ">> Current Time:  23/01/2020 12:25:01\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin_count3__trainsize730_s4_2009_e4_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "336960/336960 [==============================] - 6s 16us/step - loss: 0.2048 - mean_absolute_error: 0.3258\n",
      "Epoch 2/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2044 - mean_absolute_error: 0.3251\n",
      "Epoch 3/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2041 - mean_absolute_error: 0.3251\n",
      "Epoch 4/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2045 - mean_absolute_error: 0.3250\n",
      "Epoch 5/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2045 - mean_absolute_error: 0.3257\n",
      "Epoch 6/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2042 - mean_absolute_error: 0.3251\n",
      "Epoch 7/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2037 - mean_absolute_error: 0.3249\n",
      "Epoch 8/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2033 - mean_absolute_error: 0.3247\n",
      "Epoch 9/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2027 - mean_absolute_error: 0.3248\n",
      "Epoch 10/10\n",
      "336960/336960 [==============================] - 5s 13us/step - loss: 0.2038 - mean_absolute_error: 0.3252\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  2\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-04-21 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "80660/80660 [==============================] - 2s 29us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 51us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-10-06 17:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 49us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-10-13 18:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 44us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-10-20 19:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 49us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-10-27 20:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 49us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-11-03 21:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 48us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-11-10 22:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 46us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-21 15:00:00'), Timestamp('2011-04-21 15:00:00'), Timestamp('2011-11-17 23:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 56us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area binary230, index: 72\n",
      "date: 2011-11-20 23:00:00\n",
      "Drift detected at:  2011-11-20 23:00:00\n",
      ">> Current Time:  23/01/2020 12:29:12\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-11-20 23:00:00'), Timestamp('2011-11-20 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-11-20 23:00:00'), Timestamp('2011-11-20 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin_count12__trainsize730_s11_2009_e11_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "336960/336960 [==============================] - 6s 17us/step - loss: 0.2104 - mean_absolute_error: 0.3295\n",
      "Epoch 2/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2098 - mean_absolute_error: 0.3292\n",
      "Epoch 3/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2079 - mean_absolute_error: 0.3276\n",
      "Epoch 4/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2072 - mean_absolute_error: 0.3275\n",
      "Epoch 5/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2071 - mean_absolute_error: 0.3276\n",
      "Epoch 6/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2063 - mean_absolute_error: 0.3266\n",
      "Epoch 7/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2057 - mean_absolute_error: 0.3263\n",
      "Epoch 8/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2058 - mean_absolute_error: 0.3266\n",
      "Epoch 9/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2049 - mean_absolute_error: 0.3261\n",
      "Epoch 10/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2052 - mean_absolute_error: 0.3263\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  3\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-11-20 23:00:00'), Timestamp('2011-11-20 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-11-20 23:00:00'), Timestamp('2011-11-20 23:00:00'), Timestamp('2011-11-21 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin ####\n",
      "19680/19680 [==============================] - 1s 45us/step\n",
      "Shape of org. dataset after shift:  (984, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  0\n",
      ">> Total Number of retrainings:  3\n"
     ]
    }
   ],
   "source": [
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_w_update_adwin'\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "retraining_results_tuple_w_update_adwin = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'ADWIN', \n",
    "                                                    update_retrain_switch=False, first_forecast_range_days=168,\n",
    "                                                    n_epochs_weight = 10, overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2011-12-31 23:00:00',\n",
    "                                                    update_weights_flag = True,\n",
    "                                                    verbosity = 0)\n",
    "                               \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: Switching Scheme \n",
    "- Params: backshift = 2, weight_range == True\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:35:35.070868Z",
     "start_time": "2020-01-23T11:30:38.256542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin ####\n",
      "80660/80660 [==============================] - 3s 32us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 52us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.25613309572718]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16265807091495166\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16265807091495166\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2392759732209273\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.23927597322092736\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.25663277956855934\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2566327795685594\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24101165385569057\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.24101165385569057\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1643937515497148\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.16439375154971486\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2204314406149268\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22043144061492687\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1643937515497148\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16439375154971486\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17877510538060992\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17877510538060998\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20133895363253163\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20133895363253163\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27919662782048105\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.27919662782048105\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21596826183982143\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21596826183982148\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.17827919662782044\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1782791966278205\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19687577485742624\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19687577485742624\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.20009918175055785\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.2000991817505579\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20505826927845272\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20505826927845278\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18770146293082068\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18770146293082074\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15100421522439866\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.15100421522439872\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17728737912224146\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17728737912224152\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.36523679642945694\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.365236796429457\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.17381601785271505\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1738160178527151\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          1          1          0         0          0   \n",
      "2011-01-01 01:00:00          0          0          0         1          0   \n",
      "2011-01-01 02:00:00          1          0          0         0          0   \n",
      "2011-01-01 03:00:00          1          1          0         0          0   \n",
      "2011-01-01 04:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          0          0          0         0          0   \n",
      "2011-01-01 01:00:00          0          0          0         1          0   \n",
      "2011-01-01 02:00:00          0          1          0         1          1   \n",
      "2011-01-01 03:00:00          0          0          0         0          0   \n",
      "2011-01-01 04:00:00          1          0          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-01-01 00:00:00          0          0          0         0          0   \n",
      "2011-01-01 01:00:00          0          1          1         0          0   \n",
      "2011-01-01 02:00:00          0          1          0         1          0   \n",
      "2011-01-01 03:00:00          0          0          0         1          1   \n",
      "2011-01-01 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-01-01 00:00:00          0          0          0          0         0  \n",
      "2011-01-01 01:00:00          1          0          0          1         0  \n",
      "2011-01-01 02:00:00          1          1          1          1         1  \n",
      "2011-01-01 03:00:00          0          1          1          1         0  \n",
      "2011-01-01 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          0          1          0         1          1   \n",
      "2011-06-17 21:00:00          0          0          0         1          1   \n",
      "2011-06-17 22:00:00          1          0          1         1          0   \n",
      "2011-06-17 23:00:00          1          0          1         1          1   \n",
      "2011-06-18 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          1          1          1         1          0   \n",
      "2011-06-17 21:00:00          1          1          1         1          1   \n",
      "2011-06-17 22:00:00          0          1          1         1          0   \n",
      "2011-06-17 23:00:00          1          1          1         1          1   \n",
      "2011-06-18 00:00:00          0          1          0         1          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-06-17 20:00:00          0          0          1         1          0   \n",
      "2011-06-17 21:00:00          0          1          1         1          0   \n",
      "2011-06-17 22:00:00          1          1          0         1          1   \n",
      "2011-06-17 23:00:00          1          1          0         1          1   \n",
      "2011-06-18 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-06-17 20:00:00          1          0          1          1         1  \n",
      "2011-06-17 21:00:00          0          1          1          1         1  \n",
      "2011-06-17 22:00:00          1          1          1          0         1  \n",
      "2011-06-17 23:00:00          1          1          1          0         1  \n",
      "2011-06-18 00:00:00          0          0          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Change detected in area binary170, index: 1631\n",
      "date: 2011-03-09 23:00:00\n",
      "Drift detected at:  2011-03-09 23:00:00\n",
      ">> Current Time:  23/01/2020 12:31:04\n",
      " ->> update_weights_flag set to \"True\" , delta of drift dates: -68\n",
      " >> delta of last start trainset & current drift:  -798\n",
      "## ++ previous detected dates:  [Timestamp('2011-01-01 00:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  759\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-02-09 22:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin_count1__trainsize758_s2_2009_e3_2011__stepsize1__p12_2011 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-02-09 22:00:00\n",
      ">last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "## Existing Model is updated..\n",
      "generate data..\n",
      "start_train_year:  2009-02-09 22:00:00\n",
      "last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2009-02-09 22:00:00\n",
      "last_train_set_year:  2011-03-09 23:00:00\n",
      "start_validation_set_year:  2011-03-09 23:00:00\n",
      "start_test_set_year:  2011-03-09 23:00:00\n",
      "end_validation_set_year:  2011-03-09 23:00:00\n",
      "end_test_set_year:  2011-03-09 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (17521, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (17521,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (350420, 203)\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 17us/step - loss: 0.2079 - mean_absolute_error: 0.3278\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2073 - mean_absolute_error: 0.3270\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2057 - mean_absolute_error: 0.3260\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2059 - mean_absolute_error: 0.3261\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2056 - mean_absolute_error: 0.3260\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2052 - mean_absolute_error: 0.3257\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2052 - mean_absolute_error: 0.3259\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2051 - mean_absolute_error: 0.3255\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2043 - mean_absolute_error: 0.3254\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2042 - mean_absolute_error: 0.3252\n",
      "## Only training history & model are returned\n",
      "Training data for weight updating is used to make predictions with updated model \n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  1\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> ceiling_flag_test set to \"True\", end of dataset is reached with preds of test_set or in next iteration\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00'), Timestamp('2011-03-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin ####\n",
      "80660/80660 [==============================] - 3s 32us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 49us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.54884150978971]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16169461606354807\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1616946160635481\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.23795233892321266\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.23795233892321271\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.23530450132391878\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2353045013239188\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23000882612533102\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23000882612533097\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1519858781994704\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15198587819947043\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2146513680494263\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2146513680494263\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16169461606354807\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1616946160635481\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.16628420123565757\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.16628420123565754\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.19770520741394526\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.1977052074139453\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.26548984995586933\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2654898499558694\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.20247131509267435\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.20247131509267433\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.16187113857016766\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.1618711385701677\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.18623124448367168\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.18623124448367168\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19170344218887914\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19170344218887908\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.1945278022947926\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.19452780229479258\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.1811120917917034\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18111209179170346\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.14510150044130632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  141\n",
      "Share of wrongly classified observations:  0.14510150044130626\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.16716681376875553\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.16716681376875553\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3754633715798764\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3754633715798764\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.15922330097087378\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.15922330097087378\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          1          0          0         1          1   \n",
      "2011-03-10 01:00:00          1          0          0         1          1   \n",
      "2011-03-10 02:00:00          1          1          1         0          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          1          1          1         1          0   \n",
      "2011-03-10 01:00:00          1          1          1         1          1   \n",
      "2011-03-10 02:00:00          1          1          1         1          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-03-10 00:00:00          0          0          1         1          1   \n",
      "2011-03-10 01:00:00          1          1          1         1          1   \n",
      "2011-03-10 02:00:00          1          1          1         1          1   \n",
      "2011-03-10 03:00:00          1          1          1         1          1   \n",
      "2011-03-10 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-03-10 00:00:00          0          1          1          0         0  \n",
      "2011-03-10 01:00:00          1          1          1          0         1  \n",
      "2011-03-10 02:00:00          1          1          1          1         1  \n",
      "2011-03-10 03:00:00          1          1          1          1         1  \n",
      "2011-03-10 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          1          0         1          1   \n",
      "2011-08-24 21:00:00          1          1          1         1          0   \n",
      "2011-08-24 22:00:00          1          0          0         1          0   \n",
      "2011-08-24 23:00:00          1          1          1         1          1   \n",
      "2011-08-25 00:00:00          1          0          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          0          1         1          0   \n",
      "2011-08-24 21:00:00          1          1          1         1          1   \n",
      "2011-08-24 22:00:00          1          1          1         1          0   \n",
      "2011-08-24 23:00:00          1          1          1         1          1   \n",
      "2011-08-25 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-08-24 20:00:00          1          1          1         1          0   \n",
      "2011-08-24 21:00:00          1          1          1         1          1   \n",
      "2011-08-24 22:00:00          1          1          1         1          1   \n",
      "2011-08-24 23:00:00          0          1          1         1          1   \n",
      "2011-08-25 00:00:00          1          1          1         0          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-08-24 20:00:00          1          1          1          1         1  \n",
      "2011-08-24 21:00:00          1          1          1          0         1  \n",
      "2011-08-24 22:00:00          1          0          1          0         1  \n",
      "2011-08-24 23:00:00          1          1          0          0         1  \n",
      "2011-08-25 00:00:00          1          1          1          0         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary234, index: 1343\n",
      "date: 2011-05-04 23:00:00\n",
      "Drift detected at:  2011-05-04 23:00:00\n",
      ">> Current Time:  23/01/2020 12:32:32\n",
      " ->> update_weights_flag set to \"True\" , delta of drift dates: -56\n",
      " >> delta of last start trainset & current drift:  -854\n",
      "## ++ previous detected dates:  [Timestamp('2011-03-09 23:00:00'), Timestamp('2011-05-04 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  759\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-04-06 22:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-04-06 22:00:00'), Timestamp('2011-05-04 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-06 22:00:00'), Timestamp('2011-05-04 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin_count3__trainsize758_s4_2009_e5_2011__stepsize1__p12_2011 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-04-06 22:00:00\n",
      ">last_train_set_year:  2011-05-04 23:00:00\n",
      "start_validation_set_year:  2011-05-04 23:00:00\n",
      "end_validation_set_year:  2011-05-04 23:00:00\n",
      "start_test_set_year:  2011-05-04 23:00:00\n",
      "end_test_set_year:  2011-05-04 23:00:00\n",
      "## Existing Model is updated..\n",
      "generate data..\n",
      "start_train_year:  2009-04-06 22:00:00\n",
      "last_train_set_year:  2011-05-04 23:00:00\n",
      "start_validation_set_year:  2011-05-04 23:00:00\n",
      "start_test_set_year:  2011-05-04 23:00:00\n",
      "end_validation_set_year:  2011-05-04 23:00:00\n",
      "end_test_set_year:  2011-05-04 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2009-04-06 22:00:00\n",
      "last_train_set_year:  2011-05-04 23:00:00\n",
      "start_validation_set_year:  2011-05-04 23:00:00\n",
      "start_test_set_year:  2011-05-04 23:00:00\n",
      "end_validation_set_year:  2011-05-04 23:00:00\n",
      "end_test_set_year:  2011-05-04 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (17521, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (17521,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final concatenated shape of X_train :  (350420, 203)\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 17us/step - loss: 0.2059 - mean_absolute_error: 0.3258\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2052 - mean_absolute_error: 0.3258\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2050 - mean_absolute_error: 0.3256\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2040 - mean_absolute_error: 0.3251\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2048 - mean_absolute_error: 0.3259\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2045 - mean_absolute_error: 0.3256\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2047 - mean_absolute_error: 0.3257\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2038 - mean_absolute_error: 0.3245\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2037 - mean_absolute_error: 0.3245\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2034 - mean_absolute_error: 0.3243\n",
      "## Only training history & model are returned\n",
      "Training data for weight updating is used to make predictions with updated model \n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  2\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> ceiling_flag_test set to \"True\", end of dataset is reached with preds of test_set or in next iteration\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-04-06 22:00:00'), Timestamp('2011-05-04 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-06 22:00:00'), Timestamp('2011-05-04 23:00:00'), Timestamp('2011-05-05 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin ####\n",
      "80660/80660 [==============================] - 3s 34us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 53us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[59.7997942646525]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1619346554430019\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.16193465544300187\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24554144671137113\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2455414467113711\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.23869310886003714\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.23869310886003708\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23369952917677272\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23369952917677272\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15137680125552866\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1513768012555286\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22285632757882723\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22285632757882723\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.1615066343272935\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16150663432729348\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.17420459409330857\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.1742045940933086\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.19646169211014408\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.1964616921101441\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2730774718219432\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2730774718219432\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21001569410757592\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21001569410757598\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.16607219289484942\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.16607219289484948\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1910400913111714\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19104009131117136\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19646169211014408\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1964616921101441\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.2000285347410472\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20002853474104723\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18376373234412902\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.183763732344129\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.14723926380368102\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.147239263803681\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17078042516764158\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.1707804251676416\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3797974033385647\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3797974033385647\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16507347695819663\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1650734769581966\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-05-05 00:00:00          1          1          0         0          1   \n",
      "2011-05-05 01:00:00          1          1          1         1          1   \n",
      "2011-05-05 02:00:00          1          1          1         1          1   \n",
      "2011-05-05 03:00:00          1          1          1         1          1   \n",
      "2011-05-05 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-05-05 00:00:00          1          1          1         1          0   \n",
      "2011-05-05 01:00:00          1          1          1         1          1   \n",
      "2011-05-05 02:00:00          1          1          1         1          1   \n",
      "2011-05-05 03:00:00          1          1          1         1          1   \n",
      "2011-05-05 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-05-05 00:00:00          1          0          1         1          1   \n",
      "2011-05-05 01:00:00          1          1          1         1          1   \n",
      "2011-05-05 02:00:00          1          1          1         1          1   \n",
      "2011-05-05 03:00:00          1          1          1         1          1   \n",
      "2011-05-05 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-05-05 00:00:00          0          1          1          1         1  \n",
      "2011-05-05 01:00:00          1          1          0          1         1  \n",
      "2011-05-05 02:00:00          1          1          1          1         1  \n",
      "2011-05-05 03:00:00          1          1          1          1         1  \n",
      "2011-05-05 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-19 20:00:00          1          0          1         1          0   \n",
      "2011-10-19 21:00:00          0          1          0         0          0   \n",
      "2011-10-19 22:00:00          1          0          0         0          1   \n",
      "2011-10-19 23:00:00          1          0          0         0          1   \n",
      "2011-10-20 00:00:00          1          1          0         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-19 20:00:00          1          1          0         1          1   \n",
      "2011-10-19 21:00:00          0          0          0         0          0   \n",
      "2011-10-19 22:00:00          0          0          0         0          0   \n",
      "2011-10-19 23:00:00          1          0          0         0          1   \n",
      "2011-10-20 00:00:00          1          1          0         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-19 20:00:00          1          1          1         1          0   \n",
      "2011-10-19 21:00:00          0          0          0         0          0   \n",
      "2011-10-19 22:00:00          0          0          0         0          0   \n",
      "2011-10-19 23:00:00          1          0          0         0          1   \n",
      "2011-10-20 00:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-19 20:00:00          1          1          1          1         1  \n",
      "2011-10-19 21:00:00          0          0          0          0         0  \n",
      "2011-10-19 22:00:00          0          0          0          0         0  \n",
      "2011-10-19 23:00:00          1          1          0          0         1  \n",
      "2011-10-20 00:00:00          1          1          1          1         1  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Change detected in area binary162, index: 3839\n",
      "date: 2011-10-11 23:00:00\n",
      "Drift detected at:  2011-10-11 23:00:00\n",
      ">> Current Time:  23/01/2020 12:34:05\n",
      " ->> update_weights_flag set to \"True\" , delta of drift dates: -160\n",
      " >> delta of last start trainset & current drift:  -1014\n",
      "## ++ previous detected dates:  [Timestamp('2011-05-04 23:00:00'), Timestamp('2011-10-11 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  759\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-09-13 22:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-09-13 22:00:00'), Timestamp('2011-10-11 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-13 22:00:00'), Timestamp('2011-10-11 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin_count5__trainsize758_s9_2009_e10_2011__stepsize1__p12_2011 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-09-13 22:00:00\n",
      ">last_train_set_year:  2011-10-11 23:00:00\n",
      "start_validation_set_year:  2011-10-11 23:00:00\n",
      "end_validation_set_year:  2011-10-11 23:00:00\n",
      "start_test_set_year:  2011-10-11 23:00:00\n",
      "end_test_set_year:  2011-10-11 23:00:00\n",
      "## Existing Model is updated..\n",
      "generate data..\n",
      "start_train_year:  2009-09-13 22:00:00\n",
      "last_train_set_year:  2011-10-11 23:00:00\n",
      "start_validation_set_year:  2011-10-11 23:00:00\n",
      "start_test_set_year:  2011-10-11 23:00:00\n",
      "end_validation_set_year:  2011-10-11 23:00:00\n",
      "end_test_set_year:  2011-10-11 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2009-09-13 22:00:00\n",
      "last_train_set_year:  2011-10-11 23:00:00\n",
      "start_validation_set_year:  2011-10-11 23:00:00\n",
      "start_test_set_year:  2011-10-11 23:00:00\n",
      "end_validation_set_year:  2011-10-11 23:00:00\n",
      "end_test_set_year:  2011-10-11 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (17521, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (17521,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (350420, 203)\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 17us/step - loss: 0.2082 - mean_absolute_error: 0.3283\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2063 - mean_absolute_error: 0.3270\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2070 - mean_absolute_error: 0.3271\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2049 - mean_absolute_error: 0.3257\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2048 - mean_absolute_error: 0.3260\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2042 - mean_absolute_error: 0.3255\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2047 - mean_absolute_error: 0.3256\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2033 - mean_absolute_error: 0.3249\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2039 - mean_absolute_error: 0.3254\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2034 - mean_absolute_error: 0.3251\n",
      "## Only training history & model are returned\n",
      "Training data for weight updating is used to make predictions with updated model \n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  3\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> end of dataset is reached with preds of valid_set --> get last predictions with model\n",
      "current valid date:  2011-10-12 00:00:00\n",
      "current test date:  None\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-09-13 22:00:00'), Timestamp('2011-10-11 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-13 22:00:00'), Timestamp('2011-10-11 23:00:00'), Timestamp('2011-10-12 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin ####\n",
      "38880/38880 [==============================] - 2s 39us/step\n",
      "Shape of org. dataset after shift:  (1944, 20)\n",
      "20/20 [==============================] - 0s 55us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[64.00770297623117]\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1643835616438356\n",
      "## area  237\n",
      "Share of wrongly classified observations:  0.1643835616438356\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.2445205479452055\n",
      "## area  161\n",
      "Share of wrongly classified observations:  0.24452054794520547\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.24611872146118718\n",
      "## area  230\n",
      "Share of wrongly classified observations:  0.2461187214611872\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23550228310502286\n",
      "## area  79\n",
      "Share of wrongly classified observations:  0.23550228310502283\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.15194063926940637\n",
      "## area  236\n",
      "Share of wrongly classified observations:  0.1519406392694064\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.22739726027397256\n",
      "## area  162\n",
      "Share of wrongly classified observations:  0.2273972602739726\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16404109589041094\n",
      "## area  170\n",
      "Share of wrongly classified observations:  0.16404109589041097\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18162100456621\n",
      "## area  234\n",
      "Share of wrongly classified observations:  0.18162100456621005\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20182648401826486\n",
      "## area  48\n",
      "Share of wrongly classified observations:  0.20182648401826483\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2737442922374429\n",
      "## area  186\n",
      "Share of wrongly classified observations:  0.2737442922374429\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.2114155251141553\n",
      "## area  142\n",
      "Share of wrongly classified observations:  0.21141552511415526\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.16689497716894974\n",
      "## area  107\n",
      "Share of wrongly classified observations:  0.16689497716894977\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.1944063926940639\n",
      "## area  163\n",
      "Share of wrongly classified observations:  0.19440639269406393\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.1998858447488584\n",
      "## area  68\n",
      "Share of wrongly classified observations:  0.19988584474885845\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20125570776255708\n",
      "## area  239\n",
      "Share of wrongly classified observations:  0.20125570776255708\n",
      "## area  164\n",
      "Share of wrongly classified observations:  0.18824200913242006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## area  164\n",
      "Share of wrongly classified observations:  0.1882420091324201\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.14737442922374433\n",
      "## area  141\n",
      "Share of wrongly classified observations:  0.1473744292237443\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.17500000000000004\n",
      "## area  249\n",
      "Share of wrongly classified observations:  0.175\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.37385844748858443\n",
      "## area  138\n",
      "Share of wrongly classified observations:  0.3738584474885845\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.1670091324200913\n",
      "## area  90\n",
      "Share of wrongly classified observations:  0.16700913242009133\n",
      "## converted_stream_flag used:  True\n",
      "## Shape of streaming_df:  (1944, 20)\n",
      "## Head of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-10-12 00:00:00          0          1          0         1          1   \n",
      "2011-10-12 01:00:00          1          1          0         1          1   \n",
      "2011-10-12 02:00:00          1          1          1         1          1   \n",
      "2011-10-12 03:00:00          1          1          1         0          1   \n",
      "2011-10-12 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-10-12 00:00:00          0          1          0         1          1   \n",
      "2011-10-12 01:00:00          1          1          1         1          1   \n",
      "2011-10-12 02:00:00          1          1          1         1          1   \n",
      "2011-10-12 03:00:00          1          1          1         1          1   \n",
      "2011-10-12 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-10-12 00:00:00          0          1          1         1          1   \n",
      "2011-10-12 01:00:00          1          1          1         1          1   \n",
      "2011-10-12 02:00:00          1          1          1         1          1   \n",
      "2011-10-12 03:00:00          1          1          1         1          1   \n",
      "2011-10-12 04:00:00          1          1          1         1          1   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-10-12 00:00:00          1          1          1          1         1  \n",
      "2011-10-12 01:00:00          1          1          1          1         1  \n",
      "2011-10-12 02:00:00          1          1          1          1         1  \n",
      "2011-10-12 03:00:00          1          1          1          1         1  \n",
      "2011-10-12 04:00:00          1          1          1          1         1  \n",
      "## Tail of streaming_df:                       binary237  binary161  binary230  binary79  binary236  \\\n",
      "date                                                                        \n",
      "2011-12-31 19:00:00          0          1          1         0          0   \n",
      "2011-12-31 20:00:00          0          0          1         0          1   \n",
      "2011-12-31 21:00:00          1          1          0         0          1   \n",
      "2011-12-31 22:00:00          0          0          0         0          1   \n",
      "2011-12-31 23:00:00          0          0          0         0          1   \n",
      "\n",
      "                     binary162  binary170  binary234  binary48  binary186  \\\n",
      "date                                                                        \n",
      "2011-12-31 19:00:00          1          0          1         1          1   \n",
      "2011-12-31 20:00:00          0          0          0         0          1   \n",
      "2011-12-31 21:00:00          0          0          0         1          1   \n",
      "2011-12-31 22:00:00          0          0          0         0          0   \n",
      "2011-12-31 23:00:00          1          0          0         0          0   \n",
      "\n",
      "                     binary142  binary107  binary163  binary68  binary239  \\\n",
      "date                                                                        \n",
      "2011-12-31 19:00:00          1          0          0         0          0   \n",
      "2011-12-31 20:00:00          1          0          1         0          0   \n",
      "2011-12-31 21:00:00          1          0          1         0          1   \n",
      "2011-12-31 22:00:00          0          1          0         1          1   \n",
      "2011-12-31 23:00:00          0          0          1         0          0   \n",
      "\n",
      "                     binary164  binary141  binary249  binary138  binary90  \n",
      "date                                                                       \n",
      "2011-12-31 19:00:00          1          0          0          0         1  \n",
      "2011-12-31 20:00:00          0          0          0          0         0  \n",
      "2011-12-31 21:00:00          1          0          1          0         0  \n",
      "2011-12-31 22:00:00          1          0          0          1         1  \n",
      "2011-12-31 23:00:00          0          0          0          1         0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "# >>> End of dataset reached! Stop Predictions\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  3\n",
      ">> Total Number of retrainings:  0\n"
     ]
    }
   ],
   "source": [
    "#Apply switching of retraining scheme: update weights & retrain model\n",
    "\n",
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_adwin'\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "retraining_results_tuple_adwin_switch_backshfit2_weight_range = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'ADWIN', \n",
    "                                                    update_retrain_switch=True, first_forecast_range_days=168,\n",
    "                                                    n_epochs_retrain = 10, n_epochs_weight = 10, \n",
    "                                                    overwrite_params = True,\n",
    "                                                    weight_update_backshift=2,\n",
    "                                                    end_of_dataset_date = '2011-12-31 23:00:00',                \n",
    "                                                    make_preds_with_weight_range = True,\n",
    "                                                    verbosity = 2)\n",
    "                               \n",
    "\n",
    "                                                                                         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: Switching Scheme \n",
    "- Params: backshift = 2, weight_range == True , weight_range_update = 4\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T11:54:45.564454Z",
     "start_time": "2020-01-23T11:46:23.888368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "80660/80660 [==============================] - 3s 37us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary170, index: 1631\n",
      "date: 2011-03-09 23:00:00\n",
      "Drift detected at:  2011-03-09 23:00:00\n",
      ">> Current Time:  23/01/2020 12:46:51\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin_count1__trainsize758_s2_2009_e3_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 18us/step - loss: 0.2072 - mean_absolute_error: 0.3274\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2061 - mean_absolute_error: 0.3265\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2063 - mean_absolute_error: 0.3264\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2060 - mean_absolute_error: 0.3262\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2049 - mean_absolute_error: 0.3258\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2057 - mean_absolute_error: 0.3257\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2061 - mean_absolute_error: 0.3261\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2054 - mean_absolute_error: 0.3261\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2053 - mean_absolute_error: 0.3260\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2052 - mean_absolute_error: 0.3257\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  1\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-02-09 22:00:00'), Timestamp('2011-03-09 23:00:00'), Timestamp('2011-03-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "80660/80660 [==============================] - 3s 36us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "## Change detected in area binary234, index: 1375\n",
      "date: 2011-05-06 07:00:00\n",
      "Drift detected at:  2011-05-06 07:00:00\n",
      ">> Current Time:  23/01/2020 12:48:22\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin_count3__trainsize758_s4_2009_e5_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 18us/step - loss: 0.2064 - mean_absolute_error: 0.3267\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2053 - mean_absolute_error: 0.3258\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2055 - mean_absolute_error: 0.3261\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2057 - mean_absolute_error: 0.3257\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2043 - mean_absolute_error: 0.3248\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2040 - mean_absolute_error: 0.3249\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2041 - mean_absolute_error: 0.3251\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2037 - mean_absolute_error: 0.3247\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2038 - mean_absolute_error: 0.3250\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 15us/step - loss: 0.2043 - mean_absolute_error: 0.3253\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  2\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-05-06 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "80660/80660 [==============================] - 3s 34us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 56us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-10-21 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 27us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 51us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-10-28 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 29us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 59us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-11-04 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 61us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-11-11 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 29us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-11-18 13:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 28us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 49us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-11-25 14:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 27us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 57us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-12-02 15:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 27us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 57us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-12-09 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 29us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 54us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-12-16 17:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 29us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 61us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-12-23 18:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "3380/3380 [==============================] - 0s 30us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 77us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00')]\n",
      "year_list given:  [Timestamp('2009-04-08 06:00:00'), Timestamp('2011-05-06 07:00:00'), Timestamp('2011-12-30 19:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin ####\n",
      "580/580 [==============================] - 0s 53us/step\n",
      "Shape of org. dataset after shift:  (29, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  2\n",
      ">> Total Number of retrainings:  0\n"
     ]
    }
   ],
   "source": [
    "#Apply switching of retraining scheme: update weights & retrain model\n",
    "\n",
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_switch_backshift2_weight_range_w_update_r4_adwin'\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4 = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'ADWIN', \n",
    "                                                    update_retrain_switch=True, first_forecast_range_days=168,\n",
    "                                                    n_epochs_retrain = 10, n_epochs_weight = 10,\n",
    "                                                    overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2011-12-31 23:00:00',  \n",
    "                                                    weight_update_backshift=2,\n",
    "                                                    weight_update_range = [4],\n",
    "                                                    adjust_lags_flag = True,\n",
    "                                                    make_preds_with_weight_range = True,\n",
    "                                                    verbosity = 0)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T08:31:01.751888Z",
     "start_time": "2019-09-29T08:30:57.117296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions stored on disk!\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n"
     ]
    }
   ],
   "source": [
    "#call function to store results:\n",
    "df_save_PATH = 'media/...'\n",
    "model_save_PATH = 'media/...'\n",
    "\n",
    "#call function to store results:\n",
    "_ = sv_files.store_retrained_drift_detection_results(retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[1], \n",
    "                                            retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[0], \n",
    "                                            df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#call function to store models & history:\n",
    "#call function to store model:\n",
    "_ = sv_files.store_model_and_history_on_disk(retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[0], \n",
    "                                model_save_PATH, df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "## store dates:\n",
    "#call function to store dates at which change was detected:\n",
    "dates_df = sv_files.store_detected_change_dates(retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[2],\n",
    "                                           df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "## store dates with dates as index:\n",
    "_ = sv_files.store_detected_change_dates_with_index(retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[2],\n",
    "                                           retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[4],\n",
    "                                           switch_updating_dates_list=retraining_results_tuple_adwin_switch_backshfit2_weight_range_w_update_r4[5],\n",
    "                                           df_store_PATH=df_save_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDDDM Diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: training of new model if drift is detected\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T12:18:29.726215Z",
     "start_time": "2020-01-23T12:15:11.698484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## >> Streaming data is differenced...\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "80660/80660 [==============================] - 3s 36us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 91us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-18 01:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-25 02:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-02 03:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-09 04:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 62us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-16 05:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 60us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-23 06:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 61us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-30 07:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-06 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 59us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-13 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-20 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 68us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-27 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-09-03 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area 48, index: 155\n",
      "date: 2011-09-09 23:00:00\n",
      "Drift detected at:  2011-09-09 23:00:00\n",
      ">> Current Time:  23/01/2020 13:17:19\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff_count13__trainsize730_s9_2009_e9_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 336960 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "336960/336960 [==============================] - 6s 19us/step - loss: 0.3620 - mean_absolute_error: 0.4329 - val_loss: 0.6251 - val_mean_absolute_error: 0.6029\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2830 - mean_absolute_error: 0.3826 - val_loss: 0.6043 - val_mean_absolute_error: 0.5744\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2679 - mean_absolute_error: 0.3710 - val_loss: 0.5236 - val_mean_absolute_error: 0.5288\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2586 - mean_absolute_error: 0.3641 - val_loss: 0.6315 - val_mean_absolute_error: 0.5662\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2519 - mean_absolute_error: 0.3589 - val_loss: 0.5902 - val_mean_absolute_error: 0.5408\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2456 - mean_absolute_error: 0.3545 - val_loss: 0.6211 - val_mean_absolute_error: 0.5689\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2407 - mean_absolute_error: 0.3508 - val_loss: 0.6537 - val_mean_absolute_error: 0.5694\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 8/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2379 - mean_absolute_error: 0.3488 - val_loss: 0.6795 - val_mean_absolute_error: 0.5644\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2349 - mean_absolute_error: 0.3469 - val_loss: 0.5880 - val_mean_absolute_error: 0.5425\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2324 - mean_absolute_error: 0.3449 - val_loss: 0.5874 - val_mean_absolute_error: 0.5362\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  1\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2011-09-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff ####\n",
      "54240/54240 [==============================] - 2s 39us/step\n",
      "Shape of org. dataset after shift:  (2712, 20)\n",
      "20/20 [==============================] - 0s 60us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  0\n",
      ">> Total Number of retrainings:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_retraining_hdddm_diff'\n",
    "\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "retraining_results_tuple_hdddm_diff = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'HDDDM_diff', \n",
    "                                                    use_differenced_ts=True,\n",
    "                                                    update_retrain_switch=False, first_forecast_range_days=168,\n",
    "                                                    n_epochs_retrain = 10, \n",
    "                                                    overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2011-12-31 23:00:00', \n",
    "                                                    verbosity = 0)\n",
    "                               \n",
    "\n",
    "#assign results:\n",
    "all_MODELS_dict_MLP_hdddm_diff = retraining_results_tuple_hdddm_diff[0]\n",
    "all_model_RESULTS_dict_MLP_hdddm_diff = retraining_results_tuple_hdddm_diff[1] \n",
    "all_detected_DATES_dict_MLP_hdddm_diff = retraining_results_tuple_hdddm_diff[2]\n",
    "avg_rmse_resulst_all_hdddm_diff = retraining_results_tuple_hdddm_diff[3]\n",
    "all_retraining_dates_hdddm_diff = retraining_results_tuple_hdddm_diff[4]\n",
    "all_weight_update_dates_hdddm_diff = retraining_results_tuple_hdddm_diff[5]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: incremental training/updating of model if drift is detected\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T12:21:48.793718Z",
     "start_time": "2020-01-23T12:18:31.301689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## >> Streaming data is differenced...\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "80660/80660 [==============================] - 3s 37us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 58us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-18 01:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 58us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-25 02:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 72us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-02 03:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-09 04:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-16 05:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 70us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-23 06:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 73us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-30 07:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 64us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-06 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 64us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-13 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-20 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 54us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-27 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 74us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-09-03 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 61us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area 48, index: 155\n",
      "date: 2011-09-09 23:00:00\n",
      "Drift detected at:  2011-09-09 23:00:00\n",
      ">> Current Time:  23/01/2020 13:20:38\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff_count13__trainsize730_s9_2009_e9_2011__stepsize1__p12_2011 ####\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      "## Existing Model is updated..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "336960/336960 [==============================] - 6s 18us/step - loss: 0.2113 - mean_absolute_error: 0.3299\n",
      "Epoch 2/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2100 - mean_absolute_error: 0.3291\n",
      "Epoch 3/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2087 - mean_absolute_error: 0.3284\n",
      "Epoch 4/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2085 - mean_absolute_error: 0.3279\n",
      "Epoch 5/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2076 - mean_absolute_error: 0.3276\n",
      "Epoch 6/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2069 - mean_absolute_error: 0.3274\n",
      "Epoch 7/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2065 - mean_absolute_error: 0.3268\n",
      "Epoch 8/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2061 - mean_absolute_error: 0.3267\n",
      "Epoch 9/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2060 - mean_absolute_error: 0.3267\n",
      "Epoch 10/10\n",
      "336960/336960 [==============================] - 5s 14us/step - loss: 0.2056 - mean_absolute_error: 0.3259\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  0\n",
      ">> Current Number of retrainings:  1\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "selected years for training:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-09-09 23:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2011-09-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff ####\n",
      "54240/54240 [==============================] - 2s 40us/step\n",
      "Shape of org. dataset after shift:  (2712, 20)\n",
      "20/20 [==============================] - 0s 64us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Start Drift Detection\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  0\n",
      ">> Total Number of retrainings:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_weight_update_hdddm_diff'\n",
    "\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "weight_update_results_tuple_hdddm_diff = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'HDDDM_diff', \n",
    "                                                    use_differenced_ts=True,\n",
    "                                                    update_weights_flag = True,\n",
    "                                                    first_forecast_range_days=168,\n",
    "                                                    n_epochs_weight = 10,\n",
    "                                                    overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2011-12-31 23:00:00', \n",
    "                                                    verbosity = 0)\n",
    "                               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy: Switching Scheme \n",
    "- Params: backshift = 2, weight range == True, weight_update_range = [1,4.5]\n",
    "- Params for test purpose only: number of epochs = 10 instead of 150, end of dataset: 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T12:50:17.777154Z",
     "start_time": "2020-01-23T12:41:21.122149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## >> Streaming data is differenced...\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', '2011-01-01 00:00:00', None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "80660/80660 [==============================] - 3s 38us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.25613309572718]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170   234  \\\n",
      "date                                                                         \n",
      "2011-01-01 00:00:00 -106.0 -388.0 -467.0 -438.0  -99.0 -262.0  -81.0  51.0   \n",
      "2011-01-01 01:00:00  195.0 -207.0 -575.0 -582.0   82.0   56.0  190.0  27.0   \n",
      "2011-01-01 02:00:00  163.0   71.0 -204.0 -541.0  235.0  164.0  218.0 -56.0   \n",
      "2011-01-01 03:00:00  104.0   51.0 -115.0 -247.0  116.0  186.0  244.0  -9.0   \n",
      "2011-01-01 04:00:00   66.0   29.0   64.0  -50.0   62.0   77.0  128.0  39.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-01-01 00:00:00 -213.0 -296.0  -36.0  -82.0 -281.0 -156.0  -59.0 -361.0   \n",
      "2011-01-01 01:00:00 -325.0 -303.0   -9.0  141.0 -229.0 -303.0  208.0  -80.0   \n",
      "2011-01-01 02:00:00 -130.0  -13.0   60.0  104.0 -120.0 -224.0  276.0   13.0   \n",
      "2011-01-01 03:00:00  104.0  102.0  127.0  202.0   -4.0 -144.0  201.0   70.0   \n",
      "2011-01-01 04:00:00   54.0   37.0   96.0  166.0   24.0  -29.0  109.0  114.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-01-01 00:00:00  -13.0 -198.0 -471.0  -29.0  \n",
      "2011-01-01 01:00:00  197.0 -332.0 -107.0  -45.0  \n",
      "2011-01-01 02:00:00  260.0 -249.0    9.0  -22.0  \n",
      "2011-01-01 03:00:00  221.0 -136.0   23.0  126.0  \n",
      "2011-01-01 04:00:00  150.0  -16.0   -7.0   41.0  \n",
      "## Tail of streaming_df:                        237    161    230     79    236    162    170    234  \\\n",
      "date                                                                         \n",
      "2011-06-17 20:00:00  -4.0  -93.0 -197.0  128.0  -38.0  -52.0 -198.0 -338.0   \n",
      "2011-06-17 21:00:00  21.0   80.0  170.0 -225.0 -119.0  222.0  -10.0   55.0   \n",
      "2011-06-17 22:00:00   9.0 -231.0  319.0 -250.0  -29.0 -128.0 -125.0  110.0   \n",
      "2011-06-17 23:00:00  44.0  -73.0  -63.0  -51.0   42.0   46.0   98.0   13.0   \n",
      "2011-06-18 00:00:00  42.0  -85.0  -38.0 -326.0  -25.0    7.0   15.0  -45.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-06-17 20:00:00 -145.0  192.0 -294.0    2.0 -134.0  -37.0   83.0 -119.0   \n",
      "2011-06-17 21:00:00    0.0  157.0 -245.0  -50.0  -26.0   -1.0 -108.0   15.0   \n",
      "2011-06-17 22:00:00  -91.0   26.0 -232.0 -183.0 -141.0  -60.0 -116.0  -36.0   \n",
      "2011-06-17 23:00:00  -39.0   49.0 -188.0 -105.0 -108.0 -258.0  -27.0    7.0   \n",
      "2011-06-18 00:00:00 -151.0  -33.0  -62.0 -130.0 -132.0 -180.0  -93.0  -67.0   \n",
      "\n",
      "                      141    249    138     90  \n",
      "date                                            \n",
      "2011-06-17 20:00:00 -32.0 -183.0  156.0    3.0  \n",
      "2011-06-17 21:00:00 -32.0  -94.0    9.0 -123.0  \n",
      "2011-06-17 22:00:00 -54.0  201.0  466.0  130.0  \n",
      "2011-06-17 23:00:00  42.0  -14.0  875.0  -63.0  \n",
      "2011-06-18 00:00:00 -99.0 -120.0  894.0  -13.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-18 01:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[55.150346178085684]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161    230     79   236   162    170    234  \\\n",
      "date                                                                      \n",
      "2011-06-18 01:00:00 -21.0 -56.0 -269.0 -139.0  17.0 -48.0 -139.0  121.0   \n",
      "2011-06-18 02:00:00  -4.0  -1.0  -40.0  -81.0  -3.0 -43.0  -32.0  -18.0   \n",
      "2011-06-18 03:00:00 -17.0  53.0   39.0  -89.0   3.0   1.0  -34.0   33.0   \n",
      "2011-06-18 04:00:00 -25.0  -5.0    5.0  -15.0  25.0  17.0    6.0   -1.0   \n",
      "2011-06-18 05:00:00  11.0  20.0   -8.0   48.0  13.0  53.0  -10.0   -9.0   \n",
      "\n",
      "                        48    186    142   107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2011-06-18 01:00:00 -435.0 -143.0 -192.0  36.0 -99.0  -94.0 -135.0 -237.0   \n",
      "2011-06-18 02:00:00 -162.0   56.0    2.0   0.0  -4.0 -260.0   -9.0  -26.0   \n",
      "2011-06-18 03:00:00   26.0   28.0   -5.0 -23.0  20.0  -28.0  -11.0   61.0   \n",
      "2011-06-18 04:00:00   22.0    6.0   12.0 -17.0 -13.0   23.0  -11.0   13.0   \n",
      "2011-06-18 05:00:00  -72.0    9.0   -1.0  11.0 -51.0    4.0   47.0  -59.0   \n",
      "\n",
      "                      141    249    138    90  \n",
      "date                                           \n",
      "2011-06-18 01:00:00 -83.0  -74.0  337.0 -61.0  \n",
      "2011-06-18 02:00:00 -31.0 -266.0   40.0 -37.0  \n",
      "2011-06-18 03:00:00   4.0  -20.0   -1.0  -3.0  \n",
      "2011-06-18 04:00:00 -34.0   17.0   29.0  51.0  \n",
      "2011-06-18 05:00:00  12.0  -37.0   57.0  27.0  \n",
      "## Tail of streaming_df:                         237   161    230     79    236   162    170    234  \\\n",
      "date                                                                        \n",
      "2011-06-24 21:00:00  279.0 -50.0   -8.0  103.0  128.0  61.0  166.0  251.0   \n",
      "2011-06-24 22:00:00   39.0  54.0 -152.0  105.0  -72.0 -55.0   38.0  152.0   \n",
      "2011-06-24 23:00:00  -61.0  67.0 -140.0 -262.0   45.0 -40.0  -46.0 -141.0   \n",
      "2011-06-25 00:00:00   -1.0  10.0  -32.0 -135.0   42.0  -3.0  -30.0 -140.0   \n",
      "2011-06-25 01:00:00   33.0  -5.0   74.0   55.0  -11.0 -32.0 -127.0 -107.0   \n",
      "\n",
      "                        48    186   142    107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2011-06-24 21:00:00  132.0   23.0  75.0   44.0  62.0  170.0  187.0   48.0   \n",
      "2011-06-24 22:00:00   91.0   65.0  62.0  132.0  14.0   83.0  203.0   12.0   \n",
      "2011-06-24 23:00:00  162.0    0.0  75.0  -19.0  12.0  180.0  149.0 -104.0   \n",
      "2011-06-25 00:00:00  233.0   -1.0   6.0   88.0  27.0   34.0  128.0  -23.0   \n",
      "2011-06-25 01:00:00  288.0  135.0  86.0  -49.0  91.0   56.0   49.0  159.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-06-24 21:00:00  215.0  107.0 -101.0  191.0  \n",
      "2011-06-24 22:00:00  124.0  127.0 -309.0   67.0  \n",
      "2011-06-24 23:00:00   86.0  246.0 -508.0   65.0  \n",
      "2011-06-25 00:00:00  139.0   82.0   41.0   57.0  \n",
      "2011-06-25 01:00:00   52.0  201.0   17.0   77.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-06-25 02:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[52.188010907029664]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161   230     79   236   162   170   234     48  \\\n",
      "date                                                                          \n",
      "2011-06-25 02:00:00   7.0 -24.0  77.0  179.0   9.0  37.0  12.0  51.0  216.0   \n",
      "2011-06-25 03:00:00  30.0  -3.0  -6.0  225.0   6.0   4.0  39.0 -17.0  -11.0   \n",
      "2011-06-25 04:00:00  22.0   0.0  39.0   23.0  16.0  46.0 -16.0  19.0   58.0   \n",
      "2011-06-25 05:00:00   9.0  21.0  45.0  -32.0  -5.0 -45.0   8.0  -1.0   13.0   \n",
      "2011-06-25 06:00:00  70.0 -72.0 -49.0   39.0  25.0 -88.0 -73.0  32.0  -24.0   \n",
      "\n",
      "                      186   142   107   163     68   239   164   141    249  \\\n",
      "date                                                                          \n",
      "2011-06-25 02:00:00  24.0   3.0 -27.0  34.0  182.0  -6.0  36.0 -33.0  400.0   \n",
      "2011-06-25 03:00:00 -13.0   8.0  86.0  42.0   75.0  29.0  51.0   9.0   60.0   \n",
      "2011-06-25 04:00:00  -1.0  13.0  30.0  31.0  -26.0  -3.0  26.0  23.0   22.0   \n",
      "2011-06-25 05:00:00  31.0   7.0  -9.0  37.0   19.0 -23.0  33.0  21.0   28.0   \n",
      "2011-06-25 06:00:00 -31.0  10.0 -49.0 -14.0  -17.0 -43.0 -22.0  33.0  -54.0   \n",
      "\n",
      "                       138     90  \n",
      "date                               \n",
      "2011-06-25 02:00:00   16.0  189.0  \n",
      "2011-06-25 03:00:00    3.0   53.0  \n",
      "2011-06-25 04:00:00    1.0   36.0  \n",
      "2011-06-25 05:00:00  -16.0  -24.0  \n",
      "2011-06-25 06:00:00 -112.0   10.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-07-01 22:00:00 -149.0  -79.0  -42.0 -512.0 -139.0 -141.0 -353.0 -427.0   \n",
      "2011-07-01 23:00:00  -62.0 -185.0  131.0 -395.0 -114.0 -156.0 -376.0 -184.0   \n",
      "2011-07-02 00:00:00  -92.0 -148.0 -135.0 -217.0  -97.0 -146.0 -322.0 -312.0   \n",
      "2011-07-02 01:00:00 -102.0 -114.0 -127.0 -624.0  -45.0 -124.0 -155.0 -229.0   \n",
      "2011-07-02 02:00:00  -17.0  -54.0 -132.0 -745.0  -30.0 -103.0 -175.0 -157.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-07-01 22:00:00 -154.0 -206.0 -187.0 -301.0  -22.0 -233.0 -505.0 -166.0   \n",
      "2011-07-01 23:00:00 -356.0 -107.0 -150.0 -216.0  -81.0 -277.0 -405.0  -96.0   \n",
      "2011-07-02 00:00:00 -400.0 -118.0 -141.0 -330.0  -37.0 -216.0 -278.0 -153.0   \n",
      "2011-07-02 01:00:00 -298.0 -246.0  -35.0 -267.0 -109.0 -319.0  -97.0 -247.0   \n",
      "2011-07-02 02:00:00 -245.0 -123.0  -87.0 -215.0  -81.0 -478.0  -59.0 -151.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-07-01 22:00:00 -263.0  -57.0   16.0 -317.0  \n",
      "2011-07-01 23:00:00 -309.0 -306.0  124.0 -264.0  \n",
      "2011-07-02 00:00:00 -244.0 -227.0 -393.0 -246.0  \n",
      "2011-07-02 01:00:00 -145.0 -546.0 -103.0 -186.0  \n",
      "2011-07-02 02:00:00  -67.0 -568.0  -23.0 -314.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-02 03:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 62us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.881492602237145]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-07-02 03:00:00  -62.0  -48.0 -104.0 -473.0  -27.0  -44.0  -98.0  -17.0   \n",
      "2011-07-02 04:00:00   -9.0    2.0  -23.0  -83.0   -7.0  -46.0  -20.0  -32.0   \n",
      "2011-07-02 05:00:00    2.0    5.0   20.0  -27.0   22.0   12.0   82.0   15.0   \n",
      "2011-07-02 06:00:00  105.0  200.0  164.0  -13.0  176.0  207.0  164.0  122.0   \n",
      "2011-07-02 07:00:00  262.0  177.0   93.0  -88.0  259.0  121.0  224.0  142.0   \n",
      "\n",
      "                        48    186    142    107   163     68    239    164  \\\n",
      "date                                                                         \n",
      "2011-07-02 03:00:00  -78.0  -17.0  -10.0  -87.0 -98.0 -200.0  -33.0 -155.0   \n",
      "2011-07-02 04:00:00 -103.0  -45.0  -50.0  -96.0 -59.0  -63.0  -36.0  -72.0   \n",
      "2011-07-02 05:00:00   10.0    4.0   21.0   43.0   2.0   21.0   54.0   29.0   \n",
      "2011-07-02 06:00:00  164.0  247.0  132.0  125.0  62.0   74.0   48.0   84.0   \n",
      "2011-07-02 07:00:00  172.0   93.0   95.0  -67.0  79.0  -52.0 -145.0   65.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-07-02 03:00:00  -62.0 -233.0  -15.0 -185.0  \n",
      "2011-07-02 04:00:00   13.0  -77.0  -17.0  -75.0  \n",
      "2011-07-02 05:00:00   16.0   17.0    3.0   80.0  \n",
      "2011-07-02 06:00:00   90.0  104.0   83.0   88.0  \n",
      "2011-07-02 07:00:00  122.0   59.0  113.0   -2.0  \n",
      "## Tail of streaming_df:                        237    161    230     79   236    162    170    234  \\\n",
      "date                                                                        \n",
      "2011-07-08 23:00:00  52.0   49.0   50.0  822.0  98.0  262.0  206.0  363.0   \n",
      "2011-07-09 00:00:00  58.0  148.0  143.0  682.0  64.0  169.0  253.0  311.0   \n",
      "2011-07-09 01:00:00  65.0   75.0  149.0  693.0  12.0  138.0  186.0  117.0   \n",
      "2011-07-09 02:00:00   9.0   31.0  119.0  538.0   5.0   66.0  159.0   32.0   \n",
      "2011-07-09 03:00:00  50.0   17.0   25.0  271.0  28.0   55.0   51.0  -43.0   \n",
      "\n",
      "                        48    186   142    107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2011-07-08 23:00:00  294.0  430.0  84.0  288.0  -3.0  268.0  333.0   74.0   \n",
      "2011-07-09 00:00:00  217.0  117.0  82.0  234.0 -55.0  157.0  176.0   40.0   \n",
      "2011-07-09 01:00:00   64.0  141.0 -28.0  178.0 -82.0   37.0   59.0  147.0   \n",
      "2011-07-09 02:00:00   71.0   25.0  34.0  150.0  17.0  220.0   64.0  100.0   \n",
      "2011-07-09 03:00:00   43.0  -19.0 -18.0   12.0  -1.0   97.0   12.0   58.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-07-08 23:00:00  183.0  141.0  241.0  283.0  \n",
      "2011-07-09 00:00:00  126.0  254.0  340.0  235.0  \n",
      "2011-07-09 01:00:00   74.0  283.0  145.0   92.0  \n",
      "2011-07-09 02:00:00  111.0  135.0   29.0  114.0  \n",
      "2011-07-09 03:00:00   32.0  129.0    9.0   71.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-09 04:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[55.3066096959335]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161   230    79    236   162    170    234    48  \\\n",
      "date                                                                           \n",
      "2011-07-09 04:00:00  22.0  28.0  40.0  79.0   -7.0  44.0   46.0    0.0  50.0   \n",
      "2011-07-09 05:00:00  24.0  32.0 -20.0  32.0  -83.0  20.0   10.0    6.0  -3.0   \n",
      "2011-07-09 06:00:00 -17.0 -86.0  -3.0  18.0 -106.0 -51.0  -42.0  -73.0 -36.0   \n",
      "2011-07-09 07:00:00 -78.0  -7.0 -14.0  58.0  -99.0 -33.0 -175.0 -105.0 -86.0   \n",
      "2011-07-09 08:00:00  88.0 -87.0 -51.0  38.0  -90.0  95.0  -73.0  -71.0  -9.0   \n",
      "\n",
      "                       186   142    107   163    68    239   164   141   249  \\\n",
      "date                                                                           \n",
      "2011-07-09 04:00:00   56.0  50.0   66.0  44.0  56.0   45.0  39.0 -18.0  47.0   \n",
      "2011-07-09 05:00:00   15.0  16.0  -33.0  33.0  -2.0  -24.0  -4.0   7.0 -22.0   \n",
      "2011-07-09 06:00:00 -138.0 -43.0 -106.0  20.0 -73.0  -36.0 -52.0 -43.0  -3.0   \n",
      "2011-07-09 07:00:00  -57.0 -47.0  -76.0 -39.0 -12.0   25.0 -52.0 -46.0 -87.0   \n",
      "2011-07-09 08:00:00 -243.0   5.0   37.0  21.0 -91.0  106.0 -46.0  66.0  50.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2011-07-09 04:00:00  17.0  35.0  \n",
      "2011-07-09 05:00:00  20.0 -19.0  \n",
      "2011-07-09 06:00:00  44.0 -22.0  \n",
      "2011-07-09 07:00:00 -19.0 -68.0  \n",
      "2011-07-09 08:00:00 -36.0 -61.0  \n",
      "## Tail of streaming_df:                        237   161    230     79   236   162   170    234     48  \\\n",
      "date                                                                            \n",
      "2011-07-16 00:00:00  47.0  -7.0  118.0 -220.0  24.0  24.0 -44.0  108.0   92.0   \n",
      "2011-07-16 01:00:00 -35.0 -23.0 -107.0   28.0 -25.0 -30.0 -27.0  161.0   32.0   \n",
      "2011-07-16 02:00:00   3.0  18.0   -8.0  161.0 -12.0 -28.0 -13.0   41.0  106.0   \n",
      "2011-07-16 03:00:00  -6.0   5.0   84.0   51.0 -17.0   6.0  23.0   32.0   40.0   \n",
      "2011-07-16 04:00:00 -16.0 -13.0   -7.0   37.0  14.0 -44.0 -12.0   21.0   24.0   \n",
      "\n",
      "                      186    142    107   163     68    239    164    141  \\\n",
      "date                                                                        \n",
      "2011-07-16 00:00:00  46.0  126.0  106.0  64.0  154.0   43.0  104.0   20.0   \n",
      "2011-07-16 01:00:00 -69.0 -207.0   78.0 -49.0  206.0 -223.0    9.0 -159.0   \n",
      "2011-07-16 02:00:00  40.0  140.0  110.0  -9.0  137.0    0.0   43.0  -42.0   \n",
      "2011-07-16 03:00:00  29.0   65.0   45.0  78.0   23.0   -8.0   24.0   21.0   \n",
      "2011-07-16 04:00:00 -44.0  -73.0   -2.0  10.0   -5.0   -6.0    8.0   34.0   \n",
      "\n",
      "                       249    138    90  \n",
      "date                                     \n",
      "2011-07-16 00:00:00  -26.0 -287.0 -27.0  \n",
      "2011-07-16 01:00:00  204.0 -145.0 -15.0  \n",
      "2011-07-16 02:00:00  182.0  -26.0  -9.0  \n",
      "2011-07-16 03:00:00   15.0   -2.0  42.0  \n",
      "2011-07-16 04:00:00    4.0  -12.0   2.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-16 05:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 68us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.079071978204944]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237    161   230    79   236    162    170   234    48  \\\n",
      "date                                                                           \n",
      "2011-07-16 05:00:00 -15.0  -15.0 -20.0 -41.0  38.0  -40.0  -24.0 -17.0  59.0   \n",
      "2011-07-16 06:00:00 -66.0   53.0 -45.0  54.0  24.0  -44.0  -12.0   2.0 -79.0   \n",
      "2011-07-16 07:00:00 -65.0  -67.0 -37.0  37.0 -66.0 -110.0   46.0  15.0  89.0   \n",
      "2011-07-16 08:00:00 -43.0  -68.0 -31.0  31.0  57.0 -119.0  132.0 -13.0 -50.0   \n",
      "2011-07-16 09:00:00 -37.0 -122.0  41.0 -43.0  75.0  -15.0   11.0  34.0 -71.0   \n",
      "\n",
      "                       186   142    107   163    68    239   164   141   249  \\\n",
      "date                                                                           \n",
      "2011-07-16 05:00:00  -36.0 -41.0  -55.0 -23.0 -20.0   -5.0 -21.0  -3.0 -21.0   \n",
      "2011-07-16 06:00:00   35.0 -72.0   18.0 -65.0   5.0   -6.0  -3.0  -7.0 -14.0   \n",
      "2011-07-16 07:00:00   22.0   6.0  148.0  25.0 -75.0    6.0   2.0  -6.0  31.0   \n",
      "2011-07-16 08:00:00   75.0 -30.0    0.0   2.0 -32.0  132.0 -68.0  12.0 -21.0   \n",
      "2011-07-16 09:00:00  144.0 -32.0   31.0 -30.0 -64.0   27.0 -94.0 -52.0  20.0   \n",
      "\n",
      "                       138    90  \n",
      "date                              \n",
      "2011-07-16 05:00:00    2.0 -35.0  \n",
      "2011-07-16 06:00:00  -55.0 -23.0  \n",
      "2011-07-16 07:00:00  -22.0   5.0  \n",
      "2011-07-16 08:00:00  -11.0  74.0  \n",
      "2011-07-16 09:00:00 -209.0 -11.0  \n",
      "## Tail of streaming_df:                        237   161    230     79   236   162   170   234     48  \\\n",
      "date                                                                           \n",
      "2011-07-23 01:00:00  40.0  -7.0  125.0   -8.0  28.0 -20.0  -4.0 -24.0   49.0   \n",
      "2011-07-23 02:00:00  -4.0 -28.0   -5.0 -208.0  35.0  25.0 -12.0  32.0  -59.0   \n",
      "2011-07-23 03:00:00  -9.0 -33.0  -35.0  -49.0  14.0 -32.0   7.0 -28.0    3.0   \n",
      "2011-07-23 04:00:00  16.0  -9.0   39.0  -96.0  -7.0  38.0 -10.0 -46.0  -60.0   \n",
      "2011-07-23 05:00:00  13.0  -8.0    4.0  -35.0  12.0  73.0 -82.0   4.0 -146.0   \n",
      "\n",
      "                       186    142    107    163     68    239   164    141  \\\n",
      "date                                                                         \n",
      "2011-07-23 01:00:00  111.0  192.0 -105.0  117.0   -7.0  178.0  24.0  122.0   \n",
      "2011-07-23 02:00:00  -15.0 -108.0 -108.0   58.0 -186.0    9.0 -15.0   36.0   \n",
      "2011-07-23 03:00:00    0.0  -86.0  -30.0   24.0  -46.0   19.0  13.0   20.0   \n",
      "2011-07-23 04:00:00   40.0   52.0   -1.0    1.0    6.0  -15.0 -12.0    4.0   \n",
      "2011-07-23 05:00:00 -121.0  -18.0   -3.0   -6.0  -20.0  -20.0  23.0  -38.0   \n",
      "\n",
      "                       249   138    90  \n",
      "date                                    \n",
      "2011-07-23 01:00:00 -133.0   0.0  -7.0  \n",
      "2011-07-23 02:00:00 -111.0   1.0  14.0  \n",
      "2011-07-23 03:00:00  -27.0   1.0 -30.0  \n",
      "2011-07-23 04:00:00  -24.0  13.0 -42.0  \n",
      "2011-07-23 05:00:00   49.0  -4.0 -22.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-23 06:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 23us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 80us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[57.588020147887164]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-07-23 06:00:00  -75.0 -144.0  -47.0  -49.0 -177.0  -67.0 -197.0  -33.0   \n",
      "2011-07-23 07:00:00 -154.0 -144.0 -185.0  165.0    9.0  -80.0 -156.0 -140.0   \n",
      "2011-07-23 08:00:00 -219.0 -211.0 -122.0  210.0   57.0 -277.0 -294.0 -185.0   \n",
      "2011-07-23 09:00:00 -243.0 -109.0 -129.0  -42.0 -186.0 -272.0 -246.0 -168.0   \n",
      "2011-07-23 10:00:00  -27.0 -130.0 -189.0  -62.0 -141.0  -34.0  -77.0 -137.0   \n",
      "\n",
      "                        48    186   142    107   163    68    239    164  \\\n",
      "date                                                                       \n",
      "2011-07-23 06:00:00 -127.0 -152.0 -17.0  -47.0  -7.0 -51.0  -35.0  -61.0   \n",
      "2011-07-23 07:00:00 -277.0 -125.0 -58.0    7.0 -97.0   4.0  222.0 -166.0   \n",
      "2011-07-23 08:00:00  -59.0  -24.0   5.0  166.0 -26.0 -81.0   57.0  -94.0   \n",
      "2011-07-23 09:00:00   53.0  -88.0 -60.0  -82.0  -5.0 -40.0    7.0  -25.0   \n",
      "2011-07-23 10:00:00 -112.0 -156.0 -63.0  -13.0 -69.0 -24.0  -56.0  -21.0   \n",
      "\n",
      "                       141   249   138     90  \n",
      "date                                           \n",
      "2011-07-23 06:00:00 -110.0 -49.0  22.0  -85.0  \n",
      "2011-07-23 07:00:00  -89.0  79.0  23.0    7.0  \n",
      "2011-07-23 08:00:00  -63.0  -5.0 -28.0  -49.0  \n",
      "2011-07-23 09:00:00   56.0 -31.0  27.0  -98.0  \n",
      "2011-07-23 10:00:00  -59.0 -47.0  59.0 -144.0  \n",
      "## Tail of streaming_df:                        237   161   230    79    236   162    170   234     48  \\\n",
      "date                                                                           \n",
      "2011-07-30 02:00:00  -7.0  28.0 -27.0  92.0  -14.0  -5.0    3.0 -69.0  -64.0   \n",
      "2011-07-30 03:00:00 -26.0  20.0 -16.0 -42.0  -12.0   1.0    6.0  -7.0  -67.0   \n",
      "2011-07-30 04:00:00 -10.0 -13.0 -25.0  58.0  -24.0 -59.0   31.0  11.0  -18.0   \n",
      "2011-07-30 05:00:00 -18.0 -40.0  12.0  72.0  -21.0 -52.0   -6.0 -26.0  -28.0   \n",
      "2011-07-30 06:00:00  90.0  92.0  36.0 -25.0  114.0  49.0  206.0 -20.0  144.0   \n",
      "\n",
      "                       186   142   107    163    68   239   164   141   249  \\\n",
      "date                                                                          \n",
      "2011-07-30 02:00:00   -7.0   8.0 -93.0 -127.0  68.0   4.0  18.0 -11.0 -15.0   \n",
      "2011-07-30 03:00:00   -9.0   9.0 -41.0  -71.0   5.0  -5.0 -40.0 -40.0 -78.0   \n",
      "2011-07-30 04:00:00  -69.0  20.0   9.0  -20.0 -21.0  39.0  26.0  -9.0  36.0   \n",
      "2011-07-30 05:00:00   20.0  19.0  38.0   10.0  10.0  47.0 -33.0  17.0 -23.0   \n",
      "2011-07-30 06:00:00  140.0  23.0   5.0   40.0   7.0  75.0  71.0  64.0 -38.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2011-07-30 02:00:00   5.0 -16.0  \n",
      "2011-07-30 03:00:00   6.0 -39.0  \n",
      "2011-07-30 04:00:00  -1.0   7.0  \n",
      "2011-07-30 05:00:00 -14.0 -17.0  \n",
      "2011-07-30 06:00:00  17.0   0.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-07-30 07:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 58us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[55.43676491766102]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-07-30 07:00:00   99.0   43.0  173.0 -125.0   46.0   87.0  130.0  128.0   \n",
      "2011-07-30 08:00:00  231.0  179.0  114.0  -49.0 -115.0  216.0  139.0  211.0   \n",
      "2011-07-30 09:00:00  267.0  102.0   48.0   59.0  155.0  236.0  279.0  255.0   \n",
      "2011-07-30 10:00:00  139.0  135.0   19.0   -8.0  159.0  175.0  148.0  154.0   \n",
      "2011-07-30 11:00:00   52.0   50.0  -36.0  -16.0   47.0  150.0  -31.0  102.0   \n",
      "\n",
      "                        48    186   142    107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2011-07-30 07:00:00  120.0   92.0  70.0   35.0  58.0   41.0 -114.0  104.0   \n",
      "2011-07-30 08:00:00   52.0   51.0 -82.0  -66.0  78.0   88.0  -94.0  140.0   \n",
      "2011-07-30 09:00:00   90.0  157.0  36.0   75.0  74.0   42.0   -2.0  127.0   \n",
      "2011-07-30 10:00:00   90.0  213.0  98.0  169.0  16.0   78.0   91.0   45.0   \n",
      "2011-07-30 11:00:00  201.0  126.0 -20.0  -50.0 -25.0  102.0   46.0   48.0   \n",
      "\n",
      "                      141   249    138    90  \n",
      "date                                          \n",
      "2011-07-30 07:00:00  77.0 -89.0   30.0  49.0  \n",
      "2011-07-30 08:00:00 -15.0 -39.0    2.0  29.0  \n",
      "2011-07-30 09:00:00   0.0   8.0  220.0  84.0  \n",
      "2011-07-30 10:00:00   8.0 -13.0   62.0  89.0  \n",
      "2011-07-30 11:00:00  24.0  72.0   23.0  81.0  \n",
      "## Tail of streaming_df:                         237   161   230    79   236    162   170   234    48  \\\n",
      "date                                                                          \n",
      "2011-08-06 03:00:00   32.0   7.0  30.0 -31.0   3.0   26.0   3.0 -42.0  49.0   \n",
      "2011-08-06 04:00:00    4.0   8.0   0.0  -6.0  26.0   44.0 -14.0  37.0  35.0   \n",
      "2011-08-06 05:00:00    9.0  42.0   4.0 -64.0  -7.0    6.0  10.0  17.0  61.0   \n",
      "2011-08-06 06:00:00   39.0  50.0 -13.0 -31.0  63.0   10.0 -43.0  18.0 -47.0   \n",
      "2011-08-06 07:00:00  166.0  53.0  31.0 -27.0  16.0  129.0 -15.0  36.0  99.0   \n",
      "\n",
      "                      186   142   107   163    68   239   164   141   249  \\\n",
      "date                                                                        \n",
      "2011-08-06 03:00:00   5.0  13.0  39.0   8.0   5.0  25.0  25.0   6.0  60.0   \n",
      "2011-08-06 04:00:00  23.0  -4.0 -50.0  16.0  16.0 -25.0   6.0 -23.0 -31.0   \n",
      "2011-08-06 05:00:00  55.0  -6.0  11.0  -1.0 -61.0 -42.0 -38.0  -6.0 -30.0   \n",
      "2011-08-06 06:00:00  50.0   7.0  48.0  41.0 -17.0 -12.0 -15.0  12.0  44.0   \n",
      "2011-08-06 07:00:00 -14.0  34.0 -78.0  -9.0 -22.0 -71.0  19.0  78.0  50.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2011-08-06 03:00:00 -15.0  56.0  \n",
      "2011-08-06 04:00:00 -31.0   4.0  \n",
      "2011-08-06 05:00:00  -1.0 -16.0  \n",
      "2011-08-06 06:00:00 -65.0  49.0  \n",
      "2011-08-06 07:00:00   3.0   3.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-06 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 60us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[57.91440417889139]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230    79   236    162    170    234  \\\n",
      "date                                                                       \n",
      "2011-08-06 08:00:00  -17.0  90.0   56.0 -58.0  76.0   13.0  -49.0  -44.0   \n",
      "2011-08-06 09:00:00  -20.0  23.0  100.0   8.0 -38.0  -21.0 -141.0  -34.0   \n",
      "2011-08-06 10:00:00 -149.0 -72.0  113.0 -39.0 -26.0 -106.0 -154.0  -83.0   \n",
      "2011-08-06 11:00:00  -33.0  54.0   69.0 -26.0  21.0  -67.0  -65.0   -2.0   \n",
      "2011-08-06 12:00:00 -180.0 -38.0   41.0  -8.0 -64.0 -110.0  -26.0 -101.0   \n",
      "\n",
      "                        48    186    142    107   163    68    239    164  \\\n",
      "date                                                                        \n",
      "2011-08-06 08:00:00  129.0  -69.0  107.0 -129.0 -50.0  50.0 -109.0  114.0   \n",
      "2011-08-06 09:00:00 -143.0 -111.0  -38.0 -129.0 -39.0  31.0   17.0  -41.0   \n",
      "2011-08-06 10:00:00  -73.0 -167.0    9.0 -198.0  51.0 -49.0  -41.0  -50.0   \n",
      "2011-08-06 11:00:00 -130.0 -178.0   16.0  -48.0  73.0 -92.0  -13.0  -41.0   \n",
      "2011-08-06 12:00:00  -19.0   93.0  -35.0  -32.0 -22.0 -19.0  -41.0  -21.0   \n",
      "\n",
      "                      141   249    138    90  \n",
      "date                                          \n",
      "2011-08-06 08:00:00  32.0  91.0  -50.0  33.0  \n",
      "2011-08-06 09:00:00 -72.0  -6.0 -105.0 -35.0  \n",
      "2011-08-06 10:00:00  48.0   9.0 -180.0  -8.0  \n",
      "2011-08-06 11:00:00  26.0  -3.0   33.0 -56.0  \n",
      "2011-08-06 12:00:00   1.0  51.0   52.0 -67.0  \n",
      "## Tail of streaming_df:                        237    161   230     79   236   162    170    234    48  \\\n",
      "date                                                                            \n",
      "2011-08-13 04:00:00  28.0   21.0  29.0    7.0   9.0  22.0  -28.0   -8.0  33.0   \n",
      "2011-08-13 05:00:00  23.0  -11.0  33.0   95.0  54.0  23.0  109.0   26.0  32.0   \n",
      "2011-08-13 06:00:00  43.0  -23.0  53.0  -14.0  29.0  77.0   84.0   61.0  78.0   \n",
      "2011-08-13 07:00:00  70.0  163.0  15.0  -53.0  98.0  69.0   62.0  -67.0  29.0   \n",
      "2011-08-13 08:00:00  85.0   12.0   5.0 -168.0  92.0  45.0   40.0  154.0 -18.0   \n",
      "\n",
      "                      186   142   107   163    68   239    164   141   249  \\\n",
      "date                                                                         \n",
      "2011-08-13 04:00:00  66.0  -1.0  60.0   4.0  38.0  19.0   -7.0  10.0  52.0   \n",
      "2011-08-13 05:00:00  71.0  -1.0  23.0  -1.0  25.0   9.0   86.0  48.0   3.0   \n",
      "2011-08-13 06:00:00  42.0  37.0  56.0  -5.0  68.0 -24.0   38.0  43.0  -6.0   \n",
      "2011-08-13 07:00:00  76.0 -21.0 -54.0  84.0  77.0 -56.0  124.0   2.0 -41.0   \n",
      "2011-08-13 08:00:00 -25.0 -11.0 -55.0 -20.0  66.0 -39.0   25.0  57.0 -22.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2011-08-13 04:00:00  19.0  -4.0  \n",
      "2011-08-13 05:00:00  18.0  68.0  \n",
      "2011-08-13 06:00:00  -7.0  47.0  \n",
      "2011-08-13 07:00:00 -70.0  61.0  \n",
      "2011-08-13 08:00:00  65.0  -7.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-13 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.62939731528265]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230    79   236    162    170   234  \\\n",
      "date                                                                      \n",
      "2011-08-13 09:00:00   54.0  82.0  -32.0 -24.0  75.0  109.0   61.0   2.0   \n",
      "2011-08-13 10:00:00   81.0  53.0   22.0  64.0  60.0    5.0  -37.0   1.0   \n",
      "2011-08-13 11:00:00    5.0 -34.0  105.0  58.0  37.0  -60.0   33.0 -46.0   \n",
      "2011-08-13 12:00:00  151.0  39.0   32.0 -77.0  55.0  117.0   78.0 -71.0   \n",
      "2011-08-13 13:00:00  -96.0 -20.0 -125.0  -9.0  59.0  108.0  113.0  34.0   \n",
      "\n",
      "                        48    186   142    107   163    68   239   164   141  \\\n",
      "date                                                                           \n",
      "2011-08-13 09:00:00  108.0   12.0  38.0  108.0  31.0  -4.0 -51.0  28.0  84.0   \n",
      "2011-08-13 10:00:00   53.0   97.0 -44.0    7.0 -17.0   9.0 -23.0  56.0  29.0   \n",
      "2011-08-13 11:00:00   54.0  152.0  -5.0    1.0  20.0  83.0  15.0  33.0  10.0   \n",
      "2011-08-13 12:00:00  -28.0    7.0   7.0  -21.0  53.0  47.0   6.0  24.0   4.0   \n",
      "2011-08-13 13:00:00   70.0  -52.0 -18.0  136.0  42.0 -66.0 -95.0  10.0  37.0   \n",
      "\n",
      "                      249    138     90  \n",
      "date                                     \n",
      "2011-08-13 09:00:00 -33.0   21.0  104.0  \n",
      "2011-08-13 10:00:00  34.0   80.0   23.0  \n",
      "2011-08-13 11:00:00 -38.0  -29.0   27.0  \n",
      "2011-08-13 12:00:00 -56.0 -127.0   20.0  \n",
      "2011-08-13 13:00:00 -15.0   14.0  -55.0  \n",
      "## Tail of streaming_df:                        237   161   230    79   236    162   170   234    48  \\\n",
      "date                                                                         \n",
      "2011-08-20 05:00:00  43.0  -9.0 -37.0 -27.0  32.0  -32.0 -43.0  30.0 -24.0   \n",
      "2011-08-20 06:00:00  14.0  25.0 -33.0  64.0  89.0  -24.0  -7.0 -15.0 -11.0   \n",
      "2011-08-20 07:00:00  91.0 -87.0  50.0 -29.0  96.0 -107.0 -65.0  14.0  26.0   \n",
      "2011-08-20 08:00:00 -17.0 -21.0 -56.0 -14.0  31.0  -39.0 -26.0 -76.0 -76.0   \n",
      "2011-08-20 09:00:00 -69.0 -61.0 -78.0 -58.0 -27.0 -126.0  29.0 -80.0 -48.0   \n",
      "\n",
      "                      186   142   107   163    68   239   164   141   249  \\\n",
      "date                                                                        \n",
      "2011-08-20 05:00:00 -39.0   6.0  11.0 -21.0 -19.0   8.0 -37.0 -16.0  26.0   \n",
      "2011-08-20 06:00:00  12.0 -36.0 -51.0  48.0 -54.0  33.0 -31.0  -8.0  47.0   \n",
      "2011-08-20 07:00:00 -39.0  23.0 -57.0 -49.0  -1.0  62.0 -36.0 -76.0  -2.0   \n",
      "2011-08-20 08:00:00  24.0  15.0 -59.0  50.0 -12.0 -87.0 -93.0 -65.0 -41.0   \n",
      "2011-08-20 09:00:00 -11.0 -47.0 -39.0 -31.0 -27.0 -30.0 -26.0 -85.0  29.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2011-08-20 05:00:00  13.0 -22.0  \n",
      "2011-08-20 06:00:00  64.0 -19.0  \n",
      "2011-08-20 07:00:00  48.0 -91.0  \n",
      "2011-08-20 08:00:00 -72.0  40.0  \n",
      "2011-08-20 09:00:00 -81.0 -59.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-20 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 60us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[55.897113525255975]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230    79   236    162    170   234  \\\n",
      "date                                                                       \n",
      "2011-08-20 10:00:00   21.0 -142.0 -100.0 -30.0 -74.0  -10.0   46.0 -48.0   \n",
      "2011-08-20 11:00:00  -11.0  -48.0  -53.0  -9.0  11.0 -157.0 -127.0  20.0   \n",
      "2011-08-20 12:00:00 -106.0 -206.0  -79.0  39.0 -28.0 -129.0 -104.0 -11.0   \n",
      "2011-08-20 13:00:00   89.0   -5.0  -43.0  55.0 -50.0 -115.0 -112.0 -80.0   \n",
      "2011-08-20 14:00:00   21.0  -78.0  -28.0  20.0  40.0 -106.0   -6.0   9.0   \n",
      "\n",
      "                       48    186   142    107    163    68   239   164   141  \\\n",
      "date                                                                           \n",
      "2011-08-20 10:00:00   4.0   -3.0 -74.0    2.0   17.0 -34.0  32.0 -20.0  -2.0   \n",
      "2011-08-20 11:00:00 -34.0  -69.0  43.0 -124.0   12.0  35.0 -21.0 -36.0 -13.0   \n",
      "2011-08-20 12:00:00  -8.0 -108.0 -61.0  -23.0  -14.0 -72.0  60.0  -2.0  13.0   \n",
      "2011-08-20 13:00:00 -84.0  -34.0  27.0  -58.0   56.0  44.0  45.0  -5.0 -49.0   \n",
      "2011-08-20 14:00:00  81.0 -173.0  18.0   24.0 -107.0 -20.0  16.0 -74.0  48.0   \n",
      "\n",
      "                      249    138    90  \n",
      "date                                    \n",
      "2011-08-20 10:00:00  -3.0    5.0   3.0  \n",
      "2011-08-20 11:00:00  19.0   31.0  34.0  \n",
      "2011-08-20 12:00:00  -2.0   71.0  10.0  \n",
      "2011-08-20 13:00:00 -18.0  -62.0  64.0  \n",
      "2011-08-20 14:00:00  86.0  139.0  35.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-08-27 06:00:00   58.0   29.0   23.0  -14.0   23.0   71.0  -26.0   -3.0   \n",
      "2011-08-27 07:00:00  -29.0   29.0  -85.0  -55.0   16.0   -1.0  114.0   74.0   \n",
      "2011-08-27 08:00:00  -92.0  -57.0 -136.0 -131.0 -120.0  -68.0   60.0  -74.0   \n",
      "2011-08-27 09:00:00 -165.0 -171.0 -275.0  -56.0 -119.0 -209.0 -108.0  -66.0   \n",
      "2011-08-27 10:00:00 -144.0 -122.0 -228.0 -207.0  -14.0 -259.0 -146.0 -100.0   \n",
      "\n",
      "                        48    186   142    107    163     68    239    164  \\\n",
      "date                                                                         \n",
      "2011-08-27 06:00:00   58.0   12.0  52.0   53.0  -57.0   52.0   36.0   53.0   \n",
      "2011-08-27 07:00:00   36.0   22.0 -10.0   -3.0  -25.0   -8.0  -57.0  -38.0   \n",
      "2011-08-27 08:00:00 -141.0  -31.0 -26.0   41.0  -93.0 -152.0   46.0  -44.0   \n",
      "2011-08-27 09:00:00 -256.0 -216.0 -67.0 -166.0 -162.0 -114.0  -85.0 -100.0   \n",
      "2011-08-27 10:00:00 -315.0 -341.0 -30.0 -180.0 -184.0  -58.0 -141.0 -217.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-08-27 06:00:00  -18.0  -38.0  -39.0   58.0  \n",
      "2011-08-27 07:00:00   23.0  -21.0  -90.0   97.0  \n",
      "2011-08-27 08:00:00  -48.0  -57.0  -36.0  -81.0  \n",
      "2011-08-27 09:00:00  -83.0 -103.0  -50.0 -149.0  \n",
      "2011-08-27 10:00:00 -164.0  -66.0 -282.0 -135.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-08-27 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 64us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[54.175076665888454]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-08-27 11:00:00 -302.0 -412.0 -352.0 -331.0 -304.0 -204.0 -213.0 -356.0   \n",
      "2011-08-27 12:00:00 -490.0 -304.0 -419.0 -405.0 -352.0 -407.0 -345.0 -393.0   \n",
      "2011-08-27 13:00:00 -552.0 -426.0 -388.0 -560.0 -399.0 -417.0 -416.0 -464.0   \n",
      "2011-08-27 14:00:00 -591.0 -417.0 -356.0 -448.0 -321.0 -380.0 -441.0 -544.0   \n",
      "2011-08-27 15:00:00 -573.0 -635.0 -424.0 -505.0 -317.0 -392.0 -431.0 -435.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-08-27 11:00:00 -498.0 -558.0 -328.0 -258.0 -359.0 -258.0 -312.0 -364.0   \n",
      "2011-08-27 12:00:00 -386.0 -433.0 -336.0 -239.0 -352.0 -286.0 -330.0 -369.0   \n",
      "2011-08-27 13:00:00 -378.0 -514.0 -431.0 -351.0 -481.0 -318.0 -333.0 -350.0   \n",
      "2011-08-27 14:00:00 -523.0 -516.0 -528.0 -459.0 -333.0 -408.0 -406.0 -370.0   \n",
      "2011-08-27 15:00:00 -539.0 -533.0 -446.0 -417.0 -405.0 -397.0 -352.0 -308.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-08-27 11:00:00 -289.0 -227.0 -356.0 -347.0  \n",
      "2011-08-27 12:00:00 -305.0 -245.0 -148.0 -291.0  \n",
      "2011-08-27 13:00:00 -265.0 -259.0 -215.0 -396.0  \n",
      "2011-08-27 14:00:00 -309.0 -407.0 -395.0 -418.0  \n",
      "2011-08-27 15:00:00 -332.0 -326.0 -673.0 -292.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-09-03 07:00:00   36.0   99.0   48.0   16.0   88.0  156.0  118.0  142.0   \n",
      "2011-09-03 08:00:00  138.0  211.0  250.0  106.0  152.0  275.0  104.0  155.0   \n",
      "2011-09-03 09:00:00  305.0  317.0  549.0   56.0   83.0  323.0  168.0  216.0   \n",
      "2011-09-03 10:00:00  237.0  442.0  454.0  175.0    0.0  333.0  172.0  294.0   \n",
      "2011-09-03 11:00:00  348.0  676.0  501.0  298.0  272.0  383.0  413.0  464.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-09-03 07:00:00   48.0   65.0  107.0  108.0  110.0   42.0   39.0   39.0   \n",
      "2011-09-03 08:00:00  226.0  173.0   62.0  141.0  198.0  141.0   18.0  105.0   \n",
      "2011-09-03 09:00:00  289.0  327.0  165.0  197.0  203.0  199.0   92.0  121.0   \n",
      "2011-09-03 10:00:00  369.0  537.0   97.0  208.0  203.0  117.0  141.0  232.0   \n",
      "2011-09-03 11:00:00  531.0  609.0  315.0  363.0  378.0  248.0  277.0  349.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-09-03 07:00:00  103.0   65.0   40.0  -30.0  \n",
      "2011-09-03 08:00:00   58.0  140.0  101.0  111.0  \n",
      "2011-09-03 09:00:00  122.0   68.0  -38.0  193.0  \n",
      "2011-09-03 10:00:00   66.0   59.0  257.0  107.0  \n",
      "2011-09-03 11:00:00  251.0  249.0  224.0  266.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  729\n",
      "selected years for training:  ['2009', '2010']\n",
      "year_list given:  ['2009', '2010', Timestamp('2011-09-03 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 63us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[74.52488693938635]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-09-03 12:00:00  508.0  605.0  525.0  434.0  339.0  448.0  357.0  514.0   \n",
      "2011-09-03 13:00:00  594.0  655.0  597.0  458.0  365.0  400.0  376.0  568.0   \n",
      "2011-09-03 14:00:00  686.0  699.0  507.0  391.0  394.0  356.0  401.0  672.0   \n",
      "2011-09-03 15:00:00  538.0  689.0  541.0  414.0  271.0  362.0  390.0  515.0   \n",
      "2011-09-03 16:00:00  674.0  550.0  607.0  464.0  409.0  592.0  451.0  452.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-09-03 12:00:00  407.0  552.0  429.0  288.0  341.0  414.0  300.0  417.0   \n",
      "2011-09-03 13:00:00  430.0  611.0  427.0  383.0  438.0  363.0  288.0  392.0   \n",
      "2011-09-03 14:00:00  414.0  564.0  543.0  376.0  485.0  402.0  359.0  432.0   \n",
      "2011-09-03 15:00:00  508.0  481.0  445.0  381.0  461.0  384.0  375.0  400.0   \n",
      "2011-09-03 16:00:00  524.0  517.0  499.0  459.0  525.0  435.0  413.0  419.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-09-03 12:00:00  299.0  223.0   78.0  290.0  \n",
      "2011-09-03 13:00:00  230.0  258.0   43.0  369.0  \n",
      "2011-09-03 14:00:00  253.0  347.0  310.0  353.0  \n",
      "2011-09-03 15:00:00  218.0  286.0  342.0  399.0  \n",
      "2011-09-03 16:00:00  377.0  276.0  184.0  381.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2011-09-10 08:00:00  -27.0 -357.0 -128.0  617.0  -76.0 -318.0  111.0 -179.0   \n",
      "2011-09-10 09:00:00   10.0 -294.0 -183.0  393.0   -6.0  -64.0   51.0   -5.0   \n",
      "2011-09-10 10:00:00  249.0 -134.0   86.0  160.0  156.0  192.0  231.0  133.0   \n",
      "2011-09-10 11:00:00  265.0  109.0  217.0  237.0  143.0  365.0  329.0  237.0   \n",
      "2011-09-10 12:00:00  364.0   67.0  172.0  251.0   60.0  326.0  349.0  291.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2011-09-10 08:00:00  222.0  242.0  144.0  405.0 -158.0  -22.0  222.0 -128.0   \n",
      "2011-09-10 09:00:00  -27.0   49.0  197.0  232.0  -79.0  -99.0  182.0   -6.0   \n",
      "2011-09-10 10:00:00   23.0  -62.0  353.0  185.0   84.0   12.0   41.0  137.0   \n",
      "2011-09-10 11:00:00  143.0  195.0  309.0  240.0  224.0  103.0  118.0  263.0   \n",
      "2011-09-10 12:00:00  153.0  227.0   99.0  285.0  316.0   44.0   38.0  126.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2011-09-10 08:00:00  255.0  284.0 -179.0  189.0  \n",
      "2011-09-10 09:00:00  188.0   36.0   65.0    6.0  \n",
      "2011-09-10 10:00:00  215.0   93.0  -46.0  197.0  \n",
      "2011-09-10 11:00:00  251.0  200.0   88.0  190.0  \n",
      "2011-09-10 12:00:00  236.0  149.0  -24.0  261.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area 48, index: 155\n",
      "date: 2011-09-09 23:00:00\n",
      "Drift detected at:  2011-09-09 23:00:00\n",
      ">> Current Time:  23/01/2020 13:43:46\n",
      " ->> update_weights_flag set to \"True\" , delta of drift dates: -252\n",
      " >> delta of last start trainset & current drift:  -982\n",
      "## ++ previous detected dates:  [Timestamp('2011-01-01 00:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  759\n",
      "#### Current dates: \n",
      "#### training_start_date:  2009-08-12 22:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### Model weights are updated based on Switching Scheme\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff_count13__trainsize758_s8_2009_e9_2011__stepsize1__p12_2012 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2009-08-12 22:00:00\n",
      ">last_train_set_year:  2011-09-09 23:00:00\n",
      "start_validation_set_year:  2011-09-09 23:00:00\n",
      "end_validation_set_year:  2011-09-09 23:00:00\n",
      "start_test_set_year:  2011-09-09 23:00:00\n",
      "end_test_set_year:  2011-09-09 23:00:00\n",
      "## Existing Model is updated..\n",
      "generate data..\n",
      "start_train_year:  2009-08-12 22:00:00\n",
      "last_train_set_year:  2011-09-09 23:00:00\n",
      "start_validation_set_year:  2011-09-09 23:00:00\n",
      "start_test_set_year:  2011-09-09 23:00:00\n",
      "end_validation_set_year:  2011-09-09 23:00:00\n",
      "end_test_set_year:  2011-09-09 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2009-08-12 22:00:00\n",
      "last_train_set_year:  2011-09-09 23:00:00\n",
      "start_validation_set_year:  2011-09-09 23:00:00\n",
      "start_test_set_year:  2011-09-09 23:00:00\n",
      "end_validation_set_year:  2011-09-09 23:00:00\n",
      "end_test_set_year:  2011-09-09 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (17521, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (17521,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (350420, 203)\n",
      "#Clipping Norm applied\n",
      "Epoch 1/10\n",
      "350420/350420 [==============================] - 6s 18us/step - loss: 0.2108 - mean_absolute_error: 0.3299\n",
      "Epoch 2/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2091 - mean_absolute_error: 0.3285\n",
      "Epoch 3/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2074 - mean_absolute_error: 0.3278\n",
      "Epoch 4/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2077 - mean_absolute_error: 0.3278\n",
      "Epoch 5/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2067 - mean_absolute_error: 0.3269\n",
      "Epoch 6/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2066 - mean_absolute_error: 0.3267\n",
      "Epoch 7/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2060 - mean_absolute_error: 0.3269\n",
      "Epoch 8/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2053 - mean_absolute_error: 0.3260\n",
      "Epoch 9/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2051 - mean_absolute_error: 0.3262\n",
      "Epoch 10/10\n",
      "350420/350420 [==============================] - 5s 14us/step - loss: 0.2055 - mean_absolute_error: 0.3260\n",
      "## Only training history & model are returned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for weight updating is used to make predictions with updated model \n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  1\n",
      ">> Current Number of retrainings:  0\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2011-09-10 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "80660/80660 [==============================] - 3s 39us/step\n",
      "Shape of org. dataset after shift:  (4033, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.497815252256395]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                        237   161   230     79   236   162    170    234     48  \\\n",
      "date                                                                            \n",
      "2011-09-10 00:00:00 -89.0  42.0 -41.0 -556.0  20.0 -24.0  235.0   54.0   17.0   \n",
      "2011-09-10 01:00:00  -7.0   9.0 -40.0 -254.0   7.0  47.0  280.0  189.0   84.0   \n",
      "2011-09-10 02:00:00  32.0  10.0 -63.0  188.0  -9.0  20.0   58.0   58.0  114.0   \n",
      "2011-09-10 03:00:00 -10.0  37.0  -4.0   81.0 -22.0   6.0    9.0  148.0   54.0   \n",
      "2011-09-10 04:00:00 -40.0 -46.0  -1.0   96.0 -15.0 -15.0   27.0   18.0  -52.0   \n",
      "\n",
      "                      186   142    107   163     68   239   164    141    249  \\\n",
      "date                                                                            \n",
      "2011-09-10 00:00:00  96.0  64.0  363.0 -15.0 -291.0  48.0  53.0  100.0 -345.0   \n",
      "2011-09-10 01:00:00  44.0  23.0  245.0   6.0  -92.0   8.0 -18.0   65.0  110.0   \n",
      "2011-09-10 02:00:00  56.0  47.0  119.0  52.0   20.0  29.0  59.0   57.0  390.0   \n",
      "2011-09-10 03:00:00  36.0  17.0   16.0  10.0  150.0  -7.0  41.0   18.0  189.0   \n",
      "2011-09-10 04:00:00 -45.0 -12.0  -15.0  -6.0  -25.0  -4.0  33.0  -28.0    2.0   \n",
      "\n",
      "                      138     90  \n",
      "date                              \n",
      "2011-09-10 00:00:00 -13.0   47.0  \n",
      "2011-09-10 01:00:00  -9.0  137.0  \n",
      "2011-09-10 02:00:00  -7.0  160.0  \n",
      "2011-09-10 03:00:00   4.0   66.0  \n",
      "2011-09-10 04:00:00   3.0   11.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-02-24 20:00:00  270.0 -464.0 -292.0   64.0  152.0  -64.0  -55.0 -206.0   \n",
      "2012-02-24 21:00:00  238.0 -337.0 -327.0  -45.0   77.0 -215.0  -88.0 -139.0   \n",
      "2012-02-24 22:00:00 -158.0  117.0 -107.0  -82.0 -253.0 -213.0  -53.0  -68.0   \n",
      "2012-02-24 23:00:00  -85.0  -38.0   60.0  -49.0   54.0 -160.0 -141.0  228.0   \n",
      "2012-02-25 00:00:00   33.0  -40.0 -125.0 -400.0    3.0  -69.0   20.0 -175.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-02-24 20:00:00   60.0 -156.0  295.0  149.0  108.0  120.0  146.0  -76.0   \n",
      "2012-02-24 21:00:00 -181.0   23.0  -32.0   13.0  221.0  -27.0   81.0  -31.0   \n",
      "2012-02-24 22:00:00  -82.0   30.0  192.0 -155.0   53.0  -53.0  -25.0 -118.0   \n",
      "2012-02-24 23:00:00 -124.0 -133.0   75.0  171.0  111.0 -103.0  -35.0   65.0   \n",
      "2012-02-25 00:00:00 -131.0 -192.0   82.0  171.0   27.0  -18.0 -190.0  -23.0   \n",
      "\n",
      "                       141   249    138    90  \n",
      "date                                           \n",
      "2012-02-24 20:00:00   80.0  20.0  -78.0 -36.0  \n",
      "2012-02-24 21:00:00  -39.0 -95.0 -139.0 -85.0  \n",
      "2012-02-24 22:00:00 -119.0  15.0  286.0 -80.0  \n",
      "2012-02-24 23:00:00   57.0  94.0  -20.0  74.0  \n",
      "2012-02-25 00:00:00   11.0 -27.0  372.0 -85.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-02-25 01:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.34384444288703]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161    230     79   236   162   170   234     48  \\\n",
      "date                                                                           \n",
      "2012-02-25 01:00:00  89.0 -18.0 -232.0  -40.0 -10.0  63.0  97.0  -2.0 -209.0   \n",
      "2012-02-25 02:00:00  14.0 -14.0 -444.0 -138.0  -6.0  36.0 -36.0 -37.0 -287.0   \n",
      "2012-02-25 03:00:00   3.0 -20.0  -91.0 -338.0   4.0 -37.0  17.0 -66.0  -47.0   \n",
      "2012-02-25 04:00:00 -20.0   5.0  -35.0 -195.0   5.0   6.0  41.0  17.0  -38.0   \n",
      "2012-02-25 05:00:00  28.0 -11.0  -17.0 -101.0 -12.0   6.0 -56.0  16.0  -53.0   \n",
      "\n",
      "                       186   142   107   163     68    239    164   141  \\\n",
      "date                                                                      \n",
      "2012-02-25 01:00:00  102.0   4.0  69.0   9.0  -63.0 -164.0   -4.0  33.0   \n",
      "2012-02-25 02:00:00  101.0 -62.0 -27.0  35.0  118.0  -42.0   10.0 -28.0   \n",
      "2012-02-25 03:00:00  -27.0 -32.0  11.0  38.0    8.0  -27.0 -115.0  29.0   \n",
      "2012-02-25 04:00:00  -27.0  16.0  36.0  20.0  -78.0   -8.0  -54.0 -10.0   \n",
      "2012-02-25 05:00:00  -33.0   2.0  12.0 -10.0  -64.0   -9.0    3.0  17.0   \n",
      "\n",
      "                       249    138    90  \n",
      "date                                     \n",
      "2012-02-25 01:00:00 -124.0  142.0 -61.0  \n",
      "2012-02-25 02:00:00 -142.0   25.0 -57.0  \n",
      "2012-02-25 03:00:00 -130.0    1.0 -18.0  \n",
      "2012-02-25 04:00:00  -20.0    6.0 -40.0  \n",
      "2012-02-25 05:00:00  -18.0    8.0 -73.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-03-02 21:00:00   88.0  227.0  -60.0  557.0  244.0  392.0  266.0  349.0   \n",
      "2012-03-02 22:00:00  134.0    8.0 -385.0  270.0  177.0   10.0  105.0   15.0   \n",
      "2012-03-02 23:00:00   29.0 -178.0 -580.0 -161.0   85.0    7.0    9.0 -188.0   \n",
      "2012-03-03 00:00:00  -20.0  -78.0 -297.0 -295.0   64.0  -40.0   28.0 -171.0   \n",
      "2012-03-03 01:00:00  -45.0  -55.0 -152.0 -290.0  -29.0  -48.0  -61.0    0.0   \n",
      "\n",
      "                        48    186    142    107   163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-03-02 21:00:00  310.0 -157.0  173.0  308.0  -7.0  249.0  179.0  106.0   \n",
      "2012-03-02 22:00:00   15.0  -93.0 -130.0  237.0  66.0   18.0  275.0  101.0   \n",
      "2012-03-02 23:00:00  133.0  123.0 -171.0 -192.0 -77.0  -44.0  168.0 -149.0   \n",
      "2012-03-03 00:00:00   88.0  -71.0   49.0 -116.0  24.0  -32.0   97.0 -174.0   \n",
      "2012-03-03 01:00:00  130.0 -181.0  -26.0  -22.0 -11.0   95.0   23.0 -139.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-03-02 21:00:00  316.0  260.0  287.0  112.0  \n",
      "2012-03-02 22:00:00  337.0   11.0 -479.0   87.0  \n",
      "2012-03-02 23:00:00  111.0   -6.0  -27.0  -60.0  \n",
      "2012-03-03 00:00:00    8.0 -165.0 -554.0 -189.0  \n",
      "2012-03-03 01:00:00  -35.0  -78.0  -81.0  -26.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-03-03 02:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 68us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[55.58670919514431]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161   230     79    236   162   170    234     48  \\\n",
      "date                                                                            \n",
      "2012-03-03 02:00:00 -45.0 -98.0   1.0 -216.0    2.0 -29.0   4.0 -144.0  102.0   \n",
      "2012-03-03 03:00:00   8.0 -14.0 -14.0 -111.0   -6.0   6.0 -85.0  -18.0    2.0   \n",
      "2012-03-03 04:00:00  14.0   7.0   5.0    7.0   -3.0 -19.0 -73.0  -64.0  -44.0   \n",
      "2012-03-03 05:00:00 -19.0   9.0 -11.0   14.0   10.0  28.0  57.0   -2.0   54.0   \n",
      "2012-03-03 06:00:00 -77.0 -18.0  57.0   -5.0 -161.0 -80.0 -47.0  -49.0   14.0   \n",
      "\n",
      "                      186   142   107   163    68   239   164   141    249  \\\n",
      "date                                                                         \n",
      "2012-03-03 02:00:00 -79.0  10.0 -50.0 -25.0 -31.0  38.0 -97.0  44.0  156.0   \n",
      "2012-03-03 03:00:00  -4.0  13.0 -23.0 -23.0  94.0  42.0 -15.0 -30.0    8.0   \n",
      "2012-03-03 04:00:00 -22.0 -16.0   8.0   4.0 -45.0  -3.0  -1.0 -23.0  -27.0   \n",
      "2012-03-03 05:00:00  37.0   8.0 -43.0  15.0  62.0 -28.0 -15.0 -47.0   10.0   \n",
      "2012-03-03 06:00:00 -61.0 -20.0 -77.0  -8.0 -10.0  12.0  34.0 -45.0   -8.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2012-03-03 02:00:00  15.0 -41.0  \n",
      "2012-03-03 03:00:00   5.0 -52.0  \n",
      "2012-03-03 04:00:00   2.0 -24.0  \n",
      "2012-03-03 05:00:00   0.0  26.0  \n",
      "2012-03-03 06:00:00 -13.0 -36.0  \n",
      "## Tail of streaming_df:                         237    161    230     79   236    162   170    234  \\\n",
      "date                                                                        \n",
      "2012-03-09 22:00:00   62.0 -193.0  109.0  -69.0 -44.0  159.0 -62.0 -112.0   \n",
      "2012-03-09 23:00:00  182.0  131.0  423.0   42.0   6.0  177.0  52.0 -252.0   \n",
      "2012-03-10 00:00:00  -26.0  138.0   94.0  115.0  16.0  128.0  25.0    2.0   \n",
      "2012-03-10 01:00:00   24.0  -12.0   95.0 -153.0  38.0   45.0  84.0  -97.0   \n",
      "2012-03-10 02:00:00   19.0   86.0   32.0  163.0  12.0   35.0  61.0   61.0   \n",
      "\n",
      "                        48   186    142    107    163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-03-09 22:00:00  -49.0  72.0  -36.0  -13.0   52.0    3.0 -168.0 -133.0   \n",
      "2012-03-09 23:00:00  -10.0  16.0 -126.0  144.0 -120.0  130.0   13.0  112.0   \n",
      "2012-03-10 00:00:00 -188.0 -85.0   -8.0   38.0    5.0   57.0  -99.0   27.0   \n",
      "2012-03-10 01:00:00 -153.0  93.0   20.0  -49.0   -6.0  123.0   29.0   55.0   \n",
      "2012-03-10 02:00:00   55.0  -8.0  -16.0   99.0  -18.0    9.0   -2.0   86.0   \n",
      "\n",
      "                       141    249    138    90  \n",
      "date                                            \n",
      "2012-03-09 22:00:00 -184.0  106.0  235.0   1.0  \n",
      "2012-03-09 23:00:00 -130.0 -216.0   18.0  26.0  \n",
      "2012-03-10 00:00:00   -1.0    6.0  288.0  89.0  \n",
      "2012-03-10 01:00:00   11.0  -72.0   -7.0  88.0  \n",
      "2012-03-10 02:00:00   12.0   26.0   11.0  42.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-03-10 03:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 64us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[77.39820326609942]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161   230     79   236   162   170    234    48  \\\n",
      "date                                                                          \n",
      "2012-03-10 03:00:00  -7.0  14.0  91.0  308.0  10.0  24.0  57.0   86.0  -2.0   \n",
      "2012-03-10 04:00:00  12.0  -8.0  27.0   39.0  10.0  -2.0  52.0   47.0  99.0   \n",
      "2012-03-10 05:00:00  14.0  -1.0  83.0  -22.0 -11.0 -14.0 -17.0   -4.0  30.0   \n",
      "2012-03-10 06:00:00  50.0  -5.0  52.0  -30.0   0.0  87.0  62.0   -1.0  23.0   \n",
      "2012-03-10 07:00:00 -34.0  14.0 -48.0  127.0  97.0  -5.0 -48.0 -129.0  27.0   \n",
      "\n",
      "                       186   142   107   163    68   239   164   141   249  \\\n",
      "date                                                                         \n",
      "2012-03-10 03:00:00   13.0  29.0  38.0  26.0 -50.0 -17.0  74.0 -22.0  53.0   \n",
      "2012-03-10 04:00:00  -11.0  32.0  16.0  34.0  18.0  -9.0  64.0  43.0  -8.0   \n",
      "2012-03-10 05:00:00  -21.0  -9.0  -2.0  14.0 -48.0  37.0  47.0  60.0 -11.0   \n",
      "2012-03-10 06:00:00  120.0  39.0  46.0  40.0  27.0  47.0 -19.0 -18.0   8.0   \n",
      "2012-03-10 07:00:00   13.0  65.0  47.0  26.0  48.0  -6.0  27.0 -51.0  51.0   \n",
      "\n",
      "                      138    90  \n",
      "date                             \n",
      "2012-03-10 03:00:00   1.0  28.0  \n",
      "2012-03-10 04:00:00   1.0 -29.0  \n",
      "2012-03-10 05:00:00  -6.0 -37.0  \n",
      "2012-03-10 06:00:00 -17.0   5.0  \n",
      "2012-03-10 07:00:00   4.0 -28.0  \n",
      "## Tail of streaming_df:                         237    161    230      79   236   162    170    234  \\\n",
      "date                                                                         \n",
      "2012-03-16 23:00:00 -191.0  217.0  194.0   149.0 -96.0  69.0   91.0  672.0   \n",
      "2012-03-17 00:00:00   41.0   54.0  311.0   827.0 -80.0  34.0  -12.0  507.0   \n",
      "2012-03-17 01:00:00  -56.0  111.0   21.0   346.0 -71.0 -62.0 -136.0  149.0   \n",
      "2012-03-17 02:00:00  -16.0    3.0   -6.0  -545.0 -18.0 -33.0 -117.0  -10.0   \n",
      "2012-03-17 03:00:00  -18.0   23.0  -21.0 -1073.0 -33.0 -36.0 -186.0 -189.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-03-16 23:00:00  115.0   79.0  359.0  -82.0  308.0  -23.0  119.0   96.0   \n",
      "2012-03-17 00:00:00   45.0  291.0  -57.0  -28.0   70.0   86.0  128.0  185.0   \n",
      "2012-03-17 01:00:00   47.0  -44.0 -109.0  -47.0   85.0  -68.0   -6.0   72.0   \n",
      "2012-03-17 02:00:00  -33.0 -101.0  -54.0 -256.0   93.0 -143.0  -31.0  -84.0   \n",
      "2012-03-17 03:00:00 -178.0  -23.0  -60.0  -96.0   35.0  -74.0  -19.0 -101.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-03-16 23:00:00   -5.0  153.0  125.0  151.0  \n",
      "2012-03-17 00:00:00 -101.0  529.0    8.0   25.0  \n",
      "2012-03-17 01:00:00  -50.0  152.0  -10.0 -105.0  \n",
      "2012-03-17 02:00:00  -97.0 -324.0  -47.0  -13.0  \n",
      "2012-03-17 03:00:00  -58.0 -263.0   -8.0  -51.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-03-17 04:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[57.420050597004334]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-03-17 04:00:00  -63.0  -77.0 -156.0 -551.0  -81.0  -78.0 -151.0 -138.0   \n",
      "2012-03-17 05:00:00 -155.0 -131.0 -110.0 -238.0 -223.0 -121.0 -216.0 -203.0   \n",
      "2012-03-17 06:00:00 -623.0 -277.0 -136.0 -153.0 -631.0 -312.0 -429.0 -273.0   \n",
      "2012-03-17 07:00:00 -534.0 -297.0  -48.0  178.0 -299.0 -248.0 -351.0 -201.0   \n",
      "2012-03-17 08:00:00  283.0  -45.0  -66.0   23.0  362.0   51.0  166.0   10.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-03-17 04:00:00 -249.0 -102.0  -70.0 -197.0  -95.0 -144.0  -60.0 -112.0   \n",
      "2012-03-17 05:00:00 -289.0 -435.0 -166.0 -236.0  -74.0 -130.0 -104.0 -212.0   \n",
      "2012-03-17 06:00:00 -253.0 -263.0 -266.0 -242.0 -105.0 -134.0 -194.0 -152.0   \n",
      "2012-03-17 07:00:00   25.0  221.0 -128.0    2.0 -189.0 -117.0  140.0 -161.0   \n",
      "2012-03-17 08:00:00  248.0   95.0  113.0  227.0  -66.0  -48.0  247.0  -23.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-03-17 04:00:00 -140.0 -164.0   -9.0  -24.0  \n",
      "2012-03-17 05:00:00 -225.0 -169.0  -24.0 -152.0  \n",
      "2012-03-17 06:00:00 -270.0 -157.0  -45.0 -143.0  \n",
      "2012-03-17 07:00:00  -11.0   81.0   39.0  112.0  \n",
      "2012-03-17 08:00:00  391.0  -58.0 -123.0    9.0  \n",
      "## Tail of streaming_df:                        237   161   230     79   236   162   170    234     48  \\\n",
      "date                                                                           \n",
      "2012-03-24 00:00:00 -45.0 -14.0 -39.0 -373.0  10.0 -67.0  80.0 -139.0  125.0   \n",
      "2012-03-24 01:00:00 -23.0 -18.0 -53.0   83.0 -49.0  78.0  56.0 -100.0   26.0   \n",
      "2012-03-24 02:00:00 -15.0 -16.0  21.0  203.0   7.0 -21.0 -13.0 -150.0  -28.0   \n",
      "2012-03-24 03:00:00   9.0 -37.0 -63.0  167.0  -2.0  13.0  33.0  -12.0   36.0   \n",
      "2012-03-24 04:00:00  -6.0  30.0  23.0  -30.0  17.0  -5.0  -5.0    2.0   -8.0   \n",
      "\n",
      "                       186    142    107    163     68    239   164   141  \\\n",
      "date                                                                        \n",
      "2012-03-24 00:00:00 -127.0  139.0  112.0 -148.0  139.0  -90.0 -30.0   4.0   \n",
      "2012-03-24 01:00:00  -64.0 -120.0   22.0  -58.0   32.0 -109.0 -94.0 -40.0   \n",
      "2012-03-24 02:00:00   -9.0  -23.0  139.0  -50.0   98.0  -34.0 -10.0 -10.0   \n",
      "2012-03-24 03:00:00  -49.0   24.0    9.0   23.0   13.0  -23.0  21.0 -43.0   \n",
      "2012-03-24 04:00:00  -10.0   17.0  -21.0    2.0   22.0   -4.0   2.0  -8.0   \n",
      "\n",
      "                       249   138     90  \n",
      "date                                     \n",
      "2012-03-24 00:00:00 -175.0  22.0  103.0  \n",
      "2012-03-24 01:00:00  112.0  13.0   45.0  \n",
      "2012-03-24 02:00:00   36.0   0.0  -23.0  \n",
      "2012-03-24 03:00:00  -11.0   4.0  -14.0  \n",
      "2012-03-24 04:00:00    6.0   3.0  -37.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-03-24 05:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.922875889645184]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-03-24 05:00:00   53.0   12.0  -75.0   45.0   66.0    3.0   -4.0   49.0   \n",
      "2012-03-24 06:00:00  325.0   78.0 -128.0    3.0  402.0   42.0  112.0  105.0   \n",
      "2012-03-24 07:00:00  659.0  151.0    9.0 -107.0  526.0    3.0  191.0  181.0   \n",
      "2012-03-24 08:00:00  241.0   57.0  177.0  -44.0  306.0  126.0   47.0   63.0   \n",
      "2012-03-24 09:00:00  276.0  349.0  134.0  -60.0  322.0  147.0   87.0   84.0   \n",
      "\n",
      "                        48   186    142   107    163    68    239    164  \\\n",
      "date                                                                       \n",
      "2012-03-24 05:00:00  -55.0  62.0   50.0  54.0   -6.0   5.0   20.0  -23.0   \n",
      "2012-03-24 06:00:00   -8.0  72.0   25.0   9.0   -9.0 -16.0   -7.0  -13.0   \n",
      "2012-03-24 07:00:00  -43.0 -20.0  132.0  95.0   58.0 -74.0  -29.0   29.0   \n",
      "2012-03-24 08:00:00  118.0 -50.0  156.0   1.0  109.0  69.0  -79.0  148.0   \n",
      "2012-03-24 09:00:00  130.0 -88.0  180.0  80.0  180.0   6.0  100.0   79.0   \n",
      "\n",
      "                       141   249   138    90  \n",
      "date                                          \n",
      "2012-03-24 05:00:00   33.0 -47.0  13.0  43.0  \n",
      "2012-03-24 06:00:00   39.0  31.0 -66.0  13.0  \n",
      "2012-03-24 07:00:00   -9.0 -65.0 -79.0 -89.0  \n",
      "2012-03-24 08:00:00   97.0 -21.0 -15.0 -43.0  \n",
      "2012-03-24 09:00:00  115.0  25.0  76.0  67.0  \n",
      "## Tail of streaming_df:                        237   161    230     79   236   162   170    234    48  \\\n",
      "date                                                                           \n",
      "2012-03-31 01:00:00  46.0   6.0  131.0 -183.0  73.0 -53.0  19.0   48.0 -28.0   \n",
      "2012-03-31 02:00:00  -3.0  -5.0   30.0  -52.0 -15.0  30.0  77.0  166.0  28.0   \n",
      "2012-03-31 03:00:00 -13.0  21.0   52.0  -16.0  -6.0 -61.0   9.0   53.0  14.0   \n",
      "2012-03-31 04:00:00 -36.0 -30.0  -11.0   26.0 -21.0   3.0  -1.0  -15.0  16.0   \n",
      "2012-03-31 05:00:00 -66.0 -31.0   85.0  -49.0 -45.0  18.0  16.0  -56.0  18.0   \n",
      "\n",
      "                       186    142    107   163     68   239    164   141  \\\n",
      "date                                                                       \n",
      "2012-03-31 01:00:00  132.0  230.0  114.0 -79.0  151.0  80.0  118.0  14.0   \n",
      "2012-03-31 02:00:00  127.0   52.0  -36.0 -63.0  152.0  36.0  152.0  61.0   \n",
      "2012-03-31 03:00:00   41.0  -44.0  -30.0 -89.0  -43.0  11.0   22.0  71.0   \n",
      "2012-03-31 04:00:00  -34.0  -21.0   25.0 -69.0  -37.0 -10.0   14.0   8.0   \n",
      "2012-03-31 05:00:00  -43.0  -21.0   -2.0  -8.0  -17.0 -61.0   17.0 -33.0   \n",
      "\n",
      "                       249   138    90  \n",
      "date                                    \n",
      "2012-03-31 01:00:00 -153.0  34.0  87.0  \n",
      "2012-03-31 02:00:00   76.0   4.0  66.0  \n",
      "2012-03-31 03:00:00   16.0  -5.0  -7.0  \n",
      "2012-03-31 04:00:00  -18.0  -3.0   6.0  \n",
      "2012-03-31 05:00:00    1.0  30.0 -55.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-03-31 06:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 92us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.65332876508116]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230     79    236    162    170    234  \\\n",
      "date                                                                         \n",
      "2012-03-31 06:00:00  -76.0  -8.0   92.0   40.0 -121.0  -12.0  -45.0  -29.0   \n",
      "2012-03-31 07:00:00  -70.0  14.0   26.0   27.0  -65.0   57.0  -52.0  -75.0   \n",
      "2012-03-31 08:00:00  -44.0  56.0  -48.0   64.0 -117.0  146.0   92.0   37.0   \n",
      "2012-03-31 09:00:00   45.0  77.0  136.0  158.0  -77.0  150.0   57.0   80.0   \n",
      "2012-03-31 10:00:00  136.0   1.0  258.0  107.0   84.0  112.0  108.0  162.0   \n",
      "\n",
      "                        48   186    142   107    163     68    239    164  \\\n",
      "date                                                                        \n",
      "2012-03-31 06:00:00  -88.0 -54.0   -6.0  16.0  -70.0  -29.0   24.0   -1.0   \n",
      "2012-03-31 07:00:00   50.0 -97.0   -3.0 -94.0   13.0   39.0   36.0   10.0   \n",
      "2012-03-31 08:00:00    4.0  -5.0   -8.0  52.0  147.0   26.0  104.0   -3.0   \n",
      "2012-03-31 09:00:00  109.0  75.0   63.0  97.0  163.0  172.0  128.0   84.0   \n",
      "2012-03-31 10:00:00  143.0  64.0  185.0  39.0  -29.0   75.0   97.0  159.0   \n",
      "\n",
      "                       141   249   138     90  \n",
      "date                                           \n",
      "2012-03-31 06:00:00   29.0 -24.0  29.0  -19.0  \n",
      "2012-03-31 07:00:00   45.0  14.0   0.0  -48.0  \n",
      "2012-03-31 08:00:00  110.0  28.0  28.0   14.0  \n",
      "2012-03-31 09:00:00    3.0  47.0  33.0  100.0  \n",
      "2012-03-31 10:00:00   44.0  85.0  64.0   36.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-04-07 02:00:00    3.0   -9.0   99.0 -540.0  -15.0  -31.0 -201.0 -280.0   \n",
      "2012-04-07 03:00:00    6.0   -9.0 -107.0 -150.0  -15.0   21.0  -84.0 -101.0   \n",
      "2012-04-07 04:00:00   37.0   35.0  -16.0  -95.0    5.0   15.0    7.0  -14.0   \n",
      "2012-04-07 05:00:00   74.0  111.0   -1.0   73.0   79.0   68.0   79.0   72.0   \n",
      "2012-04-07 06:00:00  291.0  238.0  169.0  126.0  336.0  204.0  288.0  189.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-04-07 02:00:00 -122.0  -88.0  -42.0 -196.0   57.0 -368.0  -31.0 -251.0   \n",
      "2012-04-07 03:00:00  -86.0  -81.0  -22.0  -69.0   47.0  -16.0  -70.0  -70.0   \n",
      "2012-04-07 04:00:00   17.0   28.0  -15.0   14.0   26.0    5.0   13.0  -60.0   \n",
      "2012-04-07 05:00:00  105.0  189.0   80.0   65.0   -1.0   92.0   77.0   47.0   \n",
      "2012-04-07 06:00:00  392.0  384.0  225.0  189.0  147.0  152.0  154.0  101.0   \n",
      "\n",
      "                       141    249   138     90  \n",
      "date                                            \n",
      "2012-04-07 02:00:00 -113.0 -327.0 -11.0 -194.0  \n",
      "2012-04-07 03:00:00  -45.0 -119.0  -5.0  -52.0  \n",
      "2012-04-07 04:00:00   19.0  -78.0 -13.0  -16.0  \n",
      "2012-04-07 05:00:00   74.0  142.0 -18.0  111.0  \n",
      "2012-04-07 06:00:00  201.0  181.0  16.0  154.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-04-07 07:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 71us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.86811168202921]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-04-07 07:00:00  190.0  268.0  246.0 -221.0  110.0  325.0  253.0  212.0   \n",
      "2012-04-07 08:00:00  -30.0  147.0  181.0 -308.0  -70.0   -4.0   43.0  107.0   \n",
      "2012-04-07 09:00:00  -63.0  -62.0  -99.0 -116.0 -199.0  -66.0 -168.0  -65.0   \n",
      "2012-04-07 10:00:00  -26.0   99.0 -133.0 -236.0 -347.0  -74.0 -191.0 -228.0   \n",
      "2012-04-07 11:00:00 -322.0  -22.0 -145.0 -319.0 -241.0    6.0 -231.0 -295.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-04-07 07:00:00  119.0  164.0  105.0  -87.0  141.0   74.0 -108.0  121.0   \n",
      "2012-04-07 08:00:00  -79.0   55.0   58.0 -253.0  -34.0   10.0 -345.0    8.0   \n",
      "2012-04-07 09:00:00 -203.0  -21.0 -214.0 -242.0 -173.0  -83.0 -316.0  -93.0   \n",
      "2012-04-07 10:00:00 -196.0 -232.0 -268.0 -248.0  -73.0 -163.0 -208.0 -159.0   \n",
      "2012-04-07 11:00:00 -217.0 -420.0 -292.0 -196.0 -183.0 -111.0 -478.0  -52.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-04-07 07:00:00  141.0   22.0    9.0   85.0  \n",
      "2012-04-07 08:00:00 -170.0  -38.0  -39.0  -68.0  \n",
      "2012-04-07 09:00:00 -245.0 -173.0 -166.0 -223.0  \n",
      "2012-04-07 10:00:00 -255.0 -220.0 -167.0 -102.0  \n",
      "2012-04-07 11:00:00 -161.0 -280.0   -2.0 -139.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-04-14 03:00:00   35.0   34.0   69.0  140.0   11.0   14.0   96.0  108.0   \n",
      "2012-04-14 04:00:00   -5.0  -16.0   37.0   76.0  -16.0    0.0   28.0   28.0   \n",
      "2012-04-14 05:00:00  -31.0  -94.0  -34.0  -31.0  -90.0  -52.0 -110.0    1.0   \n",
      "2012-04-14 06:00:00 -344.0 -124.0 -104.0 -136.0 -589.0 -178.0 -206.0 -160.0   \n",
      "2012-04-14 07:00:00 -287.0 -176.0 -141.0  183.0 -349.0 -157.0 -138.0 -155.0   \n",
      "\n",
      "                        48    186    142    107   163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-04-14 03:00:00   92.0   36.0   50.0   83.0 -52.0  29.0   37.0  77.0   \n",
      "2012-04-14 04:00:00  -41.0   -2.0    4.0   -6.0  58.0  49.0  -20.0  39.0   \n",
      "2012-04-14 05:00:00  -70.0  -65.0  -63.0 -112.0  32.0 -88.0  -44.0 -46.0   \n",
      "2012-04-14 06:00:00 -290.0 -223.0 -157.0 -156.0 -21.0 -91.0 -163.0 -43.0   \n",
      "2012-04-14 07:00:00   11.0  -17.0  -89.0  118.0 -54.0  -6.0   64.0 -78.0   \n",
      "\n",
      "                       141    249   138     90  \n",
      "date                                            \n",
      "2012-04-14 03:00:00   31.0  142.0  12.0   67.0  \n",
      "2012-04-14 04:00:00  -40.0   62.0  16.0    6.0  \n",
      "2012-04-14 05:00:00  -93.0 -121.0   9.0 -103.0  \n",
      "2012-04-14 06:00:00 -256.0 -114.0 -25.0 -145.0  \n",
      "2012-04-14 07:00:00  -96.0  -56.0 -51.0   -4.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-04-14 08:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 62us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.217963332648786]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-04-14 08:00:00  114.0  -90.0 -139.0   94.0  139.0   20.0  -72.0  -34.0   \n",
      "2012-04-14 09:00:00  104.0    5.0   22.0  114.0  347.0   10.0   86.0   26.0   \n",
      "2012-04-14 10:00:00  147.0 -132.0    9.0  -27.0  301.0  111.0   64.0  153.0   \n",
      "2012-04-14 11:00:00  132.0  -32.0   29.0  188.0  193.0  -35.0  214.0   56.0   \n",
      "2012-04-14 12:00:00  153.0  -91.0   17.0  221.0  275.0   29.0  118.0   97.0   \n",
      "\n",
      "                        48    186    142    107   163     68    239   164  \\\n",
      "date                                                                        \n",
      "2012-04-14 08:00:00   -4.0    1.0  -60.0  165.0   9.0   37.0  317.0 -40.0   \n",
      "2012-04-14 09:00:00  142.0  106.0  157.0  138.0  56.0   61.0  225.0  29.0   \n",
      "2012-04-14 10:00:00   91.0  347.0  147.0  237.0 -11.0  192.0   84.0  62.0   \n",
      "2012-04-14 11:00:00  204.0  227.0  132.0  194.0  62.0  174.0  223.0 -16.0   \n",
      "2012-04-14 12:00:00  195.0  209.0  104.0  210.0  48.0  158.0  223.0 -21.0   \n",
      "\n",
      "                       141    249   138     90  \n",
      "date                                            \n",
      "2012-04-14 08:00:00  157.0   21.0  66.0  118.0  \n",
      "2012-04-14 09:00:00  253.0  197.0  80.0  177.0  \n",
      "2012-04-14 10:00:00  240.0  136.0  36.0  151.0  \n",
      "2012-04-14 11:00:00  168.0  197.0 -49.0  181.0  \n",
      "2012-04-14 12:00:00  128.0  189.0  52.0  179.0  \n",
      "## Tail of streaming_df:                         237    161    230     79   236   162    170   234  \\\n",
      "date                                                                       \n",
      "2012-04-21 04:00:00    5.0   10.0   34.0    2.0  17.0 -22.0  -42.0 -24.0   \n",
      "2012-04-21 05:00:00  -40.0   11.0  -38.0  -36.0 -30.0   7.0   23.0 -70.0   \n",
      "2012-04-21 06:00:00  -79.0  -62.0  -96.0   54.0 -35.0 -40.0 -107.0 -69.0   \n",
      "2012-04-21 07:00:00 -150.0 -112.0 -110.0  173.0 -24.0 -40.0   17.0 -33.0   \n",
      "2012-04-21 08:00:00  -36.0 -151.0 -115.0  209.0 -15.0  38.0  -66.0 -37.0   \n",
      "\n",
      "                        48    186    142    107   163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-04-21 04:00:00  -57.0   28.0   35.0  -18.0 -39.0  -6.0    4.0  -9.0   \n",
      "2012-04-21 05:00:00  -11.0  -70.0  -21.0   12.0  19.0 -23.0  -45.0 -23.0   \n",
      "2012-04-21 06:00:00  131.0 -212.0 -105.0  -39.0 -64.0 -13.0    7.0 -59.0   \n",
      "2012-04-21 07:00:00  -40.0  -63.0  -65.0  184.0 -97.0 -13.0  114.0   0.0   \n",
      "2012-04-21 08:00:00    8.0   33.0   54.0  185.0 -80.0 -41.0   94.0 -41.0   \n",
      "\n",
      "                       141    249   138    90  \n",
      "date                                           \n",
      "2012-04-21 04:00:00   17.0  -16.0  -5.0  37.0  \n",
      "2012-04-21 05:00:00    5.0    6.0   4.0   1.0  \n",
      "2012-04-21 06:00:00    4.0   10.0 -10.0  37.0  \n",
      "2012-04-21 07:00:00  -15.0  122.0 -38.0  82.0  \n",
      "2012-04-21 08:00:00 -102.0   92.0 -84.0 -33.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-04-21 09:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.24710601441259]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170   234  \\\n",
      "date                                                                         \n",
      "2012-04-21 09:00:00    5.0  -97.0 -120.0  -50.0   -6.0  -41.0   14.0 -16.0   \n",
      "2012-04-21 10:00:00 -217.0   -3.0  -29.0  127.0  -98.0 -122.0  -41.0  22.0   \n",
      "2012-04-21 11:00:00  -54.0   30.0   -4.0   53.0 -120.0  -12.0 -131.0 -36.0   \n",
      "2012-04-21 12:00:00  -61.0   14.0   33.0   -1.0 -146.0   20.0  -68.0 -36.0   \n",
      "2012-04-21 13:00:00 -139.0  150.0  166.0  -65.0 -168.0  107.0  -49.0 -16.0   \n",
      "\n",
      "                       48    186    142    107   163     68   239    164  \\\n",
      "date                                                                       \n",
      "2012-04-21 09:00:00 -22.0 -146.0    3.0  -48.0   7.0  -38.0  77.0  -27.0   \n",
      "2012-04-21 10:00:00  10.0 -135.0  -39.0  -81.0 -39.0  -36.0  71.0  -10.0   \n",
      "2012-04-21 11:00:00 -95.0  105.0   16.0    5.0  44.0  -57.0  69.0    9.0   \n",
      "2012-04-21 12:00:00 -90.0    0.0 -103.0 -119.0  20.0 -106.0 -62.0 -101.0   \n",
      "2012-04-21 13:00:00 -81.0   53.0   -8.0   -9.0  81.0   10.0 -79.0  -27.0   \n",
      "\n",
      "                       141   249   138     90  \n",
      "date                                           \n",
      "2012-04-21 09:00:00  -25.0 -42.0  -6.0  -11.0  \n",
      "2012-04-21 10:00:00  -23.0  53.0  30.0 -122.0  \n",
      "2012-04-21 11:00:00   11.0 -64.0  22.0  -29.0  \n",
      "2012-04-21 12:00:00    1.0  -2.0  40.0   73.0  \n",
      "2012-04-21 13:00:00 -128.0 -71.0 -89.0   52.0  \n",
      "## Tail of streaming_df:                         237    161    230     79   236    162    170    234  \\\n",
      "date                                                                         \n",
      "2012-04-28 05:00:00  -19.0   14.0   50.0  -35.0 -23.0 -106.0  -33.0   18.0   \n",
      "2012-04-28 06:00:00 -107.0 -110.0   65.0  -50.0 -80.0   18.0    1.0   11.0   \n",
      "2012-04-28 07:00:00  -82.0   32.0    0.0  -18.0 -43.0  -15.0 -105.0  -55.0   \n",
      "2012-04-28 08:00:00 -161.0   23.0   29.0  158.0  25.0 -113.0  104.0 -170.0   \n",
      "2012-04-28 09:00:00  -85.0   -3.0  113.0  -19.0 -28.0   37.0  -76.0  -10.0   \n",
      "\n",
      "                        48   186   142    107   163     68   239   164    141  \\\n",
      "date                                                                            \n",
      "2012-04-28 05:00:00  -74.0  35.0 -37.0  -16.0  -2.0  -12.0  11.0  14.0  -53.0   \n",
      "2012-04-28 06:00:00 -157.0 -24.0 -10.0  -24.0   7.0  -40.0  15.0  59.0  -46.0   \n",
      "2012-04-28 07:00:00  -78.0  -3.0   0.0 -129.0   1.0   44.0 -19.0 -23.0  107.0   \n",
      "2012-04-28 08:00:00    5.0 -47.0 -14.0   49.0  24.0  -35.0  30.0 -16.0   61.0   \n",
      "2012-04-28 09:00:00  -38.0  40.0 -35.0  125.0 -30.0 -102.0 -21.0   3.0   47.0   \n",
      "\n",
      "                      249   138    90  \n",
      "date                                   \n",
      "2012-04-28 05:00:00  34.0  -9.0 -31.0  \n",
      "2012-04-28 06:00:00 -28.0 -22.0 -25.0  \n",
      "2012-04-28 07:00:00  60.0 -54.0 -52.0  \n",
      "2012-04-28 08:00:00 -30.0   6.0 -23.0  \n",
      "2012-04-28 09:00:00  61.0  20.0 -66.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-04-28 10:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 76us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[54.892328250355014]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230    79   236   162    170    234  \\\n",
      "date                                                                       \n",
      "2012-04-28 10:00:00   60.0   25.0   81.0  -9.0  65.0  11.0  117.0  -50.0   \n",
      "2012-04-28 11:00:00   75.0   56.0   67.0 -85.0  53.0  54.0   86.0   36.0   \n",
      "2012-04-28 12:00:00   24.0  134.0  -27.0 -24.0  61.0  67.0   84.0   46.0   \n",
      "2012-04-28 13:00:00  346.0  347.0  100.0  92.0  45.0  33.0  149.0  126.0   \n",
      "2012-04-28 14:00:00    4.0   99.0   75.0  69.0  -4.0  67.0    3.0  229.0   \n",
      "\n",
      "                       48    186   142   107   163    68   239    164   141  \\\n",
      "date                                                                          \n",
      "2012-04-28 10:00:00 -54.0  -67.0  37.0  23.0   6.0 -70.0   7.0   42.0  47.0   \n",
      "2012-04-28 11:00:00  23.0  -84.0  33.0  -4.0 -79.0 -68.0  49.0   53.0 -29.0   \n",
      "2012-04-28 12:00:00  -6.0  116.0  95.0  25.0 -81.0  -2.0  41.0   57.0  35.0   \n",
      "2012-04-28 13:00:00 -21.0  -15.0  34.0  -6.0 -19.0 -13.0  29.0  141.0  79.0   \n",
      "2012-04-28 14:00:00  24.0  147.0  32.0  76.0 -83.0 -78.0  12.0   40.0  11.0   \n",
      "\n",
      "                       249    138    90  \n",
      "date                                     \n",
      "2012-04-28 10:00:00   -4.0  131.0  68.0  \n",
      "2012-04-28 11:00:00   21.0 -138.0 -89.0  \n",
      "2012-04-28 12:00:00    8.0  -33.0 -64.0  \n",
      "2012-04-28 13:00:00  102.0  -95.0 -69.0  \n",
      "2012-04-28 14:00:00  112.0   96.0 -26.0  \n",
      "## Tail of streaming_df:                         237   161    230    79    236    162   170   234    48  \\\n",
      "date                                                                            \n",
      "2012-05-05 06:00:00   13.0 -16.0 -106.0   8.0  132.0  -65.0   5.0  -4.0 -11.0   \n",
      "2012-05-05 07:00:00   12.0  28.0 -124.0  46.0  -80.0 -118.0 -25.0  59.0  45.0   \n",
      "2012-05-05 08:00:00  105.0  52.0 -250.0 -89.0  -50.0  -47.0  26.0  64.0  60.0   \n",
      "2012-05-05 09:00:00  -90.0   5.0 -364.0 -14.0 -109.0 -123.0  41.0 -26.0  33.0   \n",
      "2012-05-05 10:00:00  -60.0  28.0 -131.0  15.0   16.0  -68.0 -19.0 -25.0 -54.0   \n",
      "\n",
      "                      186   142   107    163    68    239   164    141   249  \\\n",
      "date                                                                           \n",
      "2012-05-05 06:00:00 -57.0  -1.0   2.0  -58.0 -16.0  142.0 -17.0   61.0  -8.0   \n",
      "2012-05-05 07:00:00  10.0 -17.0   4.0  -29.0 -40.0  117.0 -58.0 -146.0 -24.0   \n",
      "2012-05-05 08:00:00 -59.0 -33.0 -33.0 -177.0 -20.0   30.0  59.0  -66.0 -14.0   \n",
      "2012-05-05 09:00:00  -6.0  27.0 -34.0  -73.0  27.0   22.0  88.0  -86.0 -26.0   \n",
      "2012-05-05 10:00:00  41.0  71.0  21.0   26.0  68.0  -37.0 -10.0  -15.0 -81.0   \n",
      "\n",
      "                       138    90  \n",
      "date                              \n",
      "2012-05-05 06:00:00   62.0 -28.0  \n",
      "2012-05-05 07:00:00   44.0  28.0  \n",
      "2012-05-05 08:00:00  -14.0  62.0  \n",
      "2012-05-05 09:00:00   94.0  40.0  \n",
      "2012-05-05 10:00:00 -133.0 -31.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-05-05 11:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 59us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.18890664109949]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79   236    162    170    234  \\\n",
      "date                                                                         \n",
      "2012-05-05 11:00:00  -77.0    3.0  -99.0  107.0  -1.0  -14.0   27.0  118.0   \n",
      "2012-05-05 12:00:00 -128.0    1.0  131.0   83.0   1.0  -70.0   62.0   30.0   \n",
      "2012-05-05 13:00:00 -175.0 -226.0  -32.0   66.0  47.0  -49.0 -127.0   -1.0   \n",
      "2012-05-05 14:00:00  -13.0  -77.0   19.0   75.0  -8.0 -148.0  179.0  -26.0   \n",
      "2012-05-05 15:00:00 -102.0   -5.0   13.0  147.0  37.0  -24.0  159.0  -77.0   \n",
      "\n",
      "                       48   186    142    107   163    68   239   164    141  \\\n",
      "date                                                                           \n",
      "2012-05-05 11:00:00 -24.0  93.0   16.0  145.0   9.0  77.0 -82.0  17.0   45.0   \n",
      "2012-05-05 12:00:00  24.0 -64.0    3.0   93.0  94.0  -3.0  54.0  51.0   29.0   \n",
      "2012-05-05 13:00:00  37.0  55.0  -23.0  140.0 -35.0  43.0 -15.0  -1.0   94.0   \n",
      "2012-05-05 14:00:00  13.0 -47.0   31.0  -23.0  93.0 -14.0  74.0  21.0  193.0   \n",
      "2012-05-05 15:00:00  33.0 -33.0  107.0   55.0   9.0 -61.0 -10.0   3.0   26.0   \n",
      "\n",
      "                      249    138    90  \n",
      "date                                    \n",
      "2012-05-05 11:00:00  83.0  123.0  73.0  \n",
      "2012-05-05 12:00:00 -31.0  -19.0  54.0  \n",
      "2012-05-05 13:00:00  12.0  126.0  52.0  \n",
      "2012-05-05 14:00:00  25.0  -86.0  64.0  \n",
      "2012-05-05 15:00:00  66.0   56.0  55.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162   170    234  \\\n",
      "date                                                                         \n",
      "2012-05-12 07:00:00  119.0  -11.0   89.0  -64.0    0.0   40.0  41.0   22.0   \n",
      "2012-05-12 08:00:00  131.0   -5.0  242.0   16.0   57.0   73.0  69.0   85.0   \n",
      "2012-05-12 09:00:00   92.0   25.0  279.0   -7.0  148.0   31.0  94.0   85.0   \n",
      "2012-05-12 10:00:00   34.0  -59.0   41.0  -35.0   31.0  164.0  39.0   71.0   \n",
      "2012-05-12 11:00:00  -72.0 -170.0    6.0 -116.0  -23.0   71.0 -46.0 -111.0   \n",
      "\n",
      "                       48   186    142    107    163    68   239   164    141  \\\n",
      "date                                                                            \n",
      "2012-05-12 07:00:00  18.0  28.0   25.0  -26.0    9.0  29.0 -64.0   5.0   12.0   \n",
      "2012-05-12 08:00:00 -34.0  99.0   23.0  -92.0  218.0  81.0 -77.0 -47.0  144.0   \n",
      "2012-05-12 09:00:00  34.0 -32.0   21.0   47.0    9.0  88.0 -15.0 -61.0   97.0   \n",
      "2012-05-12 10:00:00  64.0 -35.0 -112.0  -53.0   72.0 -60.0 -71.0 -21.0   59.0   \n",
      "2012-05-12 11:00:00 -46.0 -48.0  -32.0 -179.0    1.0  26.0 -47.0 -11.0  -36.0   \n",
      "\n",
      "                      249    138    90  \n",
      "date                                    \n",
      "2012-05-12 07:00:00 -17.0   35.0  -3.0  \n",
      "2012-05-12 08:00:00   3.0   57.0  43.0  \n",
      "2012-05-12 09:00:00  28.0  -53.0  16.0  \n",
      "2012-05-12 10:00:00  52.0 -124.0  49.0  \n",
      "2012-05-12 11:00:00 -66.0  -56.0 -21.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-05-12 12:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 75us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.07621215317808]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230     79   236   162    170    234  \\\n",
      "date                                                                       \n",
      "2012-05-12 12:00:00  172.0  37.0  -57.0 -151.0 -48.0  80.0  -86.0   11.0   \n",
      "2012-05-12 13:00:00   -5.0 -14.0   -9.0 -261.0 -93.0 -74.0   30.0  -77.0   \n",
      "2012-05-12 14:00:00  131.0  37.0  -78.0 -321.0 -50.0  69.0 -273.0 -173.0   \n",
      "2012-05-12 15:00:00  121.0 -43.0  -28.0 -297.0  -4.0  37.0 -159.0  -20.0   \n",
      "2012-05-12 16:00:00   74.0  24.0 -166.0 -285.0 -90.0  19.0 -270.0  -66.0   \n",
      "\n",
      "                        48   186   142    107   163    68   239   164    141  \\\n",
      "date                                                                           \n",
      "2012-05-12 12:00:00   97.0  -3.0  -5.0  -79.0 -83.0  40.0  -2.0  17.0  -54.0   \n",
      "2012-05-12 13:00:00   18.0   2.0 -74.0 -234.0 -17.0 -18.0 -30.0 -87.0  -25.0   \n",
      "2012-05-12 14:00:00   14.0 -14.0 -35.0  -76.0 -15.0  91.0   4.0 -23.0 -160.0   \n",
      "2012-05-12 15:00:00   27.0   3.0 -71.0  -78.0  34.0  24.0   2.0 -48.0  -48.0   \n",
      "2012-05-12 16:00:00 -122.0  98.0  12.0  -49.0  63.0  -9.0  46.0  13.0 -209.0   \n",
      "\n",
      "                       249    138    90  \n",
      "date                                     \n",
      "2012-05-12 12:00:00    7.0    9.0   1.0  \n",
      "2012-05-12 13:00:00 -152.0  -17.0  27.0  \n",
      "2012-05-12 14:00:00  -44.0 -135.0  -4.0  \n",
      "2012-05-12 15:00:00  -57.0  190.0 -63.0  \n",
      "2012-05-12 16:00:00 -148.0  -10.0 -77.0  \n",
      "## Tail of streaming_df:                         237    161   230    79    236    162    170    234  \\\n",
      "date                                                                        \n",
      "2012-05-19 08:00:00  -65.0  -53.0   3.0 -30.0  -93.0   23.0  -40.0    4.0   \n",
      "2012-05-19 09:00:00  -19.0   60.0  24.0  94.0  -16.0   33.0    4.0  -52.0   \n",
      "2012-05-19 10:00:00   32.0  -21.0 -22.0 -21.0   42.0  -87.0 -104.0  -92.0   \n",
      "2012-05-19 11:00:00   91.0  -55.0   3.0 -38.0  -27.0 -170.0   42.0  -57.0   \n",
      "2012-05-19 12:00:00 -317.0 -251.0  42.0   2.0 -190.0 -241.0  -44.0 -164.0   \n",
      "\n",
      "                        48    186    142   107   163     68   239   164  \\\n",
      "date                                                                      \n",
      "2012-05-19 08:00:00   -7.0  -44.0    1.0  60.0 -35.0  -10.0 -50.0  40.0   \n",
      "2012-05-19 09:00:00 -241.0  102.0   26.0 -12.0  15.0  -60.0 -29.0  47.0   \n",
      "2012-05-19 10:00:00 -141.0   24.0   80.0  73.0 -45.0   41.0  72.0   5.0   \n",
      "2012-05-19 11:00:00  -97.0 -103.0   -6.0  28.0  -9.0 -101.0  81.0  12.0   \n",
      "2012-05-19 12:00:00 -304.0  -66.0  107.0  15.0 -50.0  -35.0 -19.0 -24.0   \n",
      "\n",
      "                       141   249   138     90  \n",
      "date                                           \n",
      "2012-05-19 08:00:00  -65.0   6.0   6.0  -44.0  \n",
      "2012-05-19 09:00:00 -105.0 -35.0  17.0  -78.0  \n",
      "2012-05-19 10:00:00 -102.0 -31.0   6.0 -110.0  \n",
      "2012-05-19 11:00:00  -84.0  10.0  27.0  -47.0  \n",
      "2012-05-19 12:00:00  -97.0 -33.0  43.0 -210.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-05-19 13:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 62us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[62.401121686827494]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230    79    236    162   170    234  \\\n",
      "date                                                                        \n",
      "2012-05-19 13:00:00   99.0  102.0   25.0 -34.0    0.0  -18.0 -71.0 -133.0   \n",
      "2012-05-19 14:00:00  -86.0   10.0   63.0   0.0   82.0   23.0  82.0 -148.0   \n",
      "2012-05-19 15:00:00 -102.0 -111.0 -129.0 -35.0 -146.0   82.0 -19.0 -138.0   \n",
      "2012-05-19 16:00:00   11.0 -111.0    9.0 -47.0 -172.0  -53.0  58.0 -197.0   \n",
      "2012-05-19 17:00:00   63.0 -144.0  107.0 -65.0 -144.0  253.0  -6.0 -273.0   \n",
      "\n",
      "                        48    186   142    107    163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-05-19 13:00:00 -139.0 -142.0  36.0   49.0   68.0 -42.0   15.0  23.0   \n",
      "2012-05-19 14:00:00 -155.0 -145.0 -37.0   -6.0   67.0   3.0 -121.0 -45.0   \n",
      "2012-05-19 15:00:00 -274.0 -103.0  -8.0    4.0  -91.0 -96.0 -112.0  33.0   \n",
      "2012-05-19 16:00:00 -135.0 -203.0 -49.0 -110.0  -76.0 -53.0 -137.0 -89.0   \n",
      "2012-05-19 17:00:00 -296.0 -334.0 -78.0  -65.0  192.0 -70.0  -95.0 -94.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-05-19 13:00:00 -180.0  118.0   24.0 -136.0  \n",
      "2012-05-19 14:00:00  -25.0  -96.0  123.0 -212.0  \n",
      "2012-05-19 15:00:00  -11.0  -71.0 -121.0 -115.0  \n",
      "2012-05-19 16:00:00   49.0    3.0  -55.0 -127.0  \n",
      "2012-05-19 17:00:00  -85.0   38.0   20.0 -392.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-05-26 09:00:00  -91.0    9.0  145.0 -196.0 -336.0  -97.0  -95.0  -54.0   \n",
      "2012-05-26 10:00:00 -199.0   30.0   50.0 -119.0 -374.0 -111.0  -32.0    2.0   \n",
      "2012-05-26 11:00:00 -237.0   69.0   36.0  -69.0 -292.0  -75.0 -107.0   72.0   \n",
      "2012-05-26 12:00:00   99.0  261.0  -42.0 -173.0 -132.0 -104.0 -160.0   -1.0   \n",
      "2012-05-26 13:00:00 -261.0 -125.0  -13.0  -75.0 -173.0 -191.0 -155.0  104.0   \n",
      "\n",
      "                        48    186    142    107   163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-05-26 09:00:00  180.0 -104.0 -204.0 -195.0  19.0 -50.0 -181.0 -52.0   \n",
      "2012-05-26 10:00:00   90.0  107.0 -170.0 -155.0 -12.0 -97.0 -254.0  52.0   \n",
      "2012-05-26 11:00:00   74.0  141.0 -166.0 -179.0 -26.0 -45.0 -330.0 -10.0   \n",
      "2012-05-26 12:00:00  142.0  -48.0 -304.0 -215.0  24.0 -83.0 -243.0  14.0   \n",
      "2012-05-26 13:00:00   33.0   13.0  -81.0 -153.0 -41.0 -85.0 -109.0 -64.0   \n",
      "\n",
      "                       141    249    138    90  \n",
      "date                                            \n",
      "2012-05-26 09:00:00 -120.0 -134.0  133.0 -69.0  \n",
      "2012-05-26 10:00:00 -117.0  -96.0  224.0 -13.0  \n",
      "2012-05-26 11:00:00 -128.0 -170.0  -32.0 -96.0  \n",
      "2012-05-26 12:00:00   -3.0 -188.0  -38.0  28.0  \n",
      "2012-05-26 13:00:00   31.0 -191.0  107.0 -98.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-05-26 14:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 62us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[60.84799623479]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-05-26 14:00:00 -160.0  -88.0  -12.0   17.0 -136.0 -212.0 -209.0  104.0   \n",
      "2012-05-26 15:00:00  -80.0  230.0  217.0   85.0  -14.0 -132.0 -135.0   90.0   \n",
      "2012-05-26 16:00:00 -144.0  149.0  253.0  184.0  126.0 -340.0  -78.0  131.0   \n",
      "2012-05-26 17:00:00 -159.0 -209.0  280.0    7.0   96.0 -412.0 -164.0  103.0   \n",
      "2012-05-26 18:00:00   11.0 -257.0  344.0  -43.0  178.0 -341.0   13.0  224.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239   164  \\\n",
      "date                                                                         \n",
      "2012-05-26 14:00:00   13.0    9.0 -127.0 -230.0 -130.0 -191.0  -14.0 -44.0   \n",
      "2012-05-26 15:00:00  158.0  -33.0    7.0 -159.0   67.0  -34.0   53.0 -15.0   \n",
      "2012-05-26 16:00:00  269.0  104.0  174.0  -75.0   81.0  -16.0  170.0  70.0   \n",
      "2012-05-26 17:00:00  370.0  202.0   49.0   -7.0 -108.0   10.0  -42.0  46.0   \n",
      "2012-05-26 18:00:00  135.0 -132.0  160.0  133.0   75.0  -89.0    6.0  20.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-05-26 14:00:00 -109.0  -78.0  -27.0   40.0  \n",
      "2012-05-26 15:00:00 -162.0  -51.0  -86.0   54.0  \n",
      "2012-05-26 16:00:00 -108.0  -10.0   -9.0   56.0  \n",
      "2012-05-26 17:00:00  -18.0  -95.0  120.0  263.0  \n",
      "2012-05-26 18:00:00    2.0 -107.0  -50.0  236.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-06-02 10:00:00   56.0 -152.0  -92.0  132.0  111.0  122.0   49.0  103.0   \n",
      "2012-06-02 11:00:00  206.0    9.0 -101.0   30.0   97.0   43.0   20.0  -62.0   \n",
      "2012-06-02 12:00:00   36.0 -194.0  -84.0  142.0  221.0   79.0   75.0   82.0   \n",
      "2012-06-02 13:00:00   22.0  -90.0 -147.0  108.0   74.0  108.0  164.0   17.0   \n",
      "2012-06-02 14:00:00   66.0   20.0 -205.0  109.0  -38.0  136.0  131.0  146.0   \n",
      "\n",
      "                       48    186   142    107    163    68    239   164   141  \\\n",
      "date                                                                            \n",
      "2012-06-02 10:00:00   7.0  -75.0  36.0  135.0   -9.0  45.0  181.0 -80.0  10.0   \n",
      "2012-06-02 11:00:00  64.0 -140.0 -15.0  103.0   30.0  42.0  175.0 -37.0  49.0   \n",
      "2012-06-02 12:00:00  18.0   30.0  45.0  124.0  -43.0   2.0  175.0  -4.0 -75.0   \n",
      "2012-06-02 13:00:00  18.0   15.0 -21.0  113.0 -124.0  12.0  -12.0  10.0  19.0   \n",
      "2012-06-02 14:00:00  64.0  153.0  41.0  132.0  -19.0  74.0  -67.0  23.0  41.0   \n",
      "\n",
      "                       249    138     90  \n",
      "date                                      \n",
      "2012-06-02 10:00:00  133.0   88.0   61.0  \n",
      "2012-06-02 11:00:00  102.0   67.0  119.0  \n",
      "2012-06-02 12:00:00   87.0 -107.0  103.0  \n",
      "2012-06-02 13:00:00   59.0  -72.0  176.0  \n",
      "2012-06-02 14:00:00   55.0  175.0   78.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-06-02 15:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 24us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 60us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.60476815930733]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237    161    230     79    236    162    170    234  \\\n",
      "date                                                                         \n",
      "2012-06-02 15:00:00  63.0  -63.0 -228.0   72.0   53.0   63.0  136.0   93.0   \n",
      "2012-06-02 16:00:00 -28.0  -14.0 -255.0  -93.0  -87.0  284.0   -7.0   41.0   \n",
      "2012-06-02 17:00:00 -20.0  193.0 -554.0  -49.0  -39.0   56.0  -40.0  -62.0   \n",
      "2012-06-02 18:00:00  23.0  221.0 -387.0   51.0 -152.0  169.0   25.0    7.0   \n",
      "2012-06-02 19:00:00  73.0 -189.0 -342.0 -134.0   88.0 -146.0   35.0 -166.0   \n",
      "\n",
      "                        48    186    142    107    163    68    239    164  \\\n",
      "date                                                                         \n",
      "2012-06-02 15:00:00   15.0   85.0   16.0  113.0  -80.0  97.0   30.0   56.0   \n",
      "2012-06-02 16:00:00 -190.0   72.0 -118.0  119.0  -82.0  40.0  -16.0   -8.0   \n",
      "2012-06-02 17:00:00 -244.0  -69.0  -56.0   51.0 -122.0 -46.0  102.0  -38.0   \n",
      "2012-06-02 18:00:00  -46.0  267.0  -78.0 -122.0 -344.0  78.0   20.0  116.0   \n",
      "2012-06-02 19:00:00  -57.0  -11.0 -135.0 -107.0 -222.0  15.0  -36.0  -33.0   \n",
      "\n",
      "                       141   249    138     90  \n",
      "date                                            \n",
      "2012-06-02 15:00:00  107.0  88.0  170.0  118.0  \n",
      "2012-06-02 16:00:00  108.0  14.0  217.0  132.0  \n",
      "2012-06-02 17:00:00  -15.0  44.0  -50.0   51.0  \n",
      "2012-06-02 18:00:00   55.0  32.0   41.0   23.0  \n",
      "2012-06-02 19:00:00   98.0  26.0  168.0  -34.0  \n",
      "## Tail of streaming_df:                        237    161    230     79    236    162   170    234  \\\n",
      "date                                                                        \n",
      "2012-06-09 11:00:00 -77.0   44.0   97.0  104.0  118.0  118.0  20.0  107.0   \n",
      "2012-06-09 12:00:00 -18.0  196.0   61.0  114.0  -18.0   51.0  57.0   43.0   \n",
      "2012-06-09 13:00:00 -21.0   62.0   73.0   85.0  -59.0   52.0  28.0   21.0   \n",
      "2012-06-09 14:00:00  77.0    8.0  243.0  -27.0   87.0  -30.0  47.0   42.0   \n",
      "2012-06-09 15:00:00  37.0   46.0  261.0  -41.0  -30.0  -17.0  17.0   88.0   \n",
      "\n",
      "                        48    186    142   107    163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-06-09 11:00:00   17.0  100.0  118.0  88.0   12.0 -11.0   32.0  75.0   \n",
      "2012-06-09 12:00:00  117.0   78.0  258.0   1.0  102.0  45.0   34.0   6.0   \n",
      "2012-06-09 13:00:00   19.0    8.0  108.0  28.0    8.0  33.0  101.0  64.0   \n",
      "2012-06-09 14:00:00   54.0  -76.0   79.0  64.0    3.0  31.0   81.0  75.0   \n",
      "2012-06-09 15:00:00   -6.0  106.0  108.0  58.0  137.0  20.0  -18.0  22.0   \n",
      "\n",
      "                      141    249    138    90  \n",
      "date                                           \n",
      "2012-06-09 11:00:00  84.0    8.0  -12.0 -36.0  \n",
      "2012-06-09 12:00:00  41.0  105.0  116.0 -18.0  \n",
      "2012-06-09 13:00:00 -12.0   -4.0  -32.0 -83.0  \n",
      "2012-06-09 14:00:00 -17.0   20.0 -113.0  81.0  \n",
      "2012-06-09 15:00:00  52.0   -2.0 -118.0 -17.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-06-09 16:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Avg. RMSE of recent predictions: \n",
      "[57.31865315065353]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-06-09 16:00:00   78.0  165.0  212.0   68.0  -49.0   94.0  152.0  121.0   \n",
      "2012-06-09 17:00:00  -14.0  230.0  287.0  -28.0   32.0   98.0  166.0   97.0   \n",
      "2012-06-09 18:00:00   27.0  108.0  202.0   31.0  -50.0  181.0   13.0   63.0   \n",
      "2012-06-09 19:00:00  -80.0   49.0  209.0  178.0 -141.0 -109.0   43.0  147.0   \n",
      "2012-06-09 20:00:00 -372.0  -65.0  114.0 -180.0   59.0  -35.0  -54.0   84.0   \n",
      "\n",
      "                        48    186    142   107    163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-06-09 16:00:00  112.0   18.0   71.0  40.0  182.0   68.0   11.0   60.0   \n",
      "2012-06-09 17:00:00   90.0  362.0  191.0 -36.0   65.0  163.0 -157.0  109.0   \n",
      "2012-06-09 18:00:00  278.0  279.0   30.0  22.0  236.0   -5.0   -3.0   63.0   \n",
      "2012-06-09 19:00:00  103.0  468.0   83.0  92.0   95.0   32.0   28.0   88.0   \n",
      "2012-06-09 20:00:00   42.0  154.0  130.0 -23.0   -8.0  -53.0  -97.0   36.0   \n",
      "\n",
      "                       141   249    138    90  \n",
      "date                                           \n",
      "2012-06-09 16:00:00   19.0  69.0  -48.0  25.0  \n",
      "2012-06-09 17:00:00  -23.0 -96.0   75.0 -43.0  \n",
      "2012-06-09 18:00:00   10.0 -14.0 -225.0  27.0  \n",
      "2012-06-09 19:00:00  -97.0  60.0 -158.0 -28.0  \n",
      "2012-06-09 20:00:00 -180.0  56.0  -70.0 -37.0  \n",
      "## Tail of streaming_df:                         237    161   230     79    236   162   170    234  \\\n",
      "date                                                                       \n",
      "2012-06-16 12:00:00   -6.0  -93.0 -16.0 -187.0   49.0  83.0 -16.0 -114.0   \n",
      "2012-06-16 13:00:00   -1.0  -29.0 -16.0  -63.0   80.0 -42.0 -38.0  -44.0   \n",
      "2012-06-16 14:00:00 -106.0  -24.0 -94.0  -54.0   57.0  14.0 -26.0 -150.0   \n",
      "2012-06-16 15:00:00  -47.0  -76.0 -54.0    1.0   90.0  44.0  36.0  -57.0   \n",
      "2012-06-16 16:00:00 -102.0 -163.0 -62.0 -184.0  175.0 -40.0   5.0  -46.0   \n",
      "\n",
      "                        48   186    142    107    163    68    239   164  \\\n",
      "date                                                                       \n",
      "2012-06-16 12:00:00  -80.0  -9.0 -179.0   66.0  -74.0 -12.0 -141.0 -76.0   \n",
      "2012-06-16 13:00:00   13.0  42.0  -56.0  126.0  133.0 -56.0 -151.0 -93.0   \n",
      "2012-06-16 14:00:00 -105.0 -77.0  -46.0   15.0   21.0 -10.0  -11.0 -50.0   \n",
      "2012-06-16 15:00:00  136.0 -47.0  -84.0  -76.0  -15.0   4.0  -50.0 -57.0   \n",
      "2012-06-16 16:00:00  -10.0  22.0    6.0   -8.0 -140.0 -50.0  -43.0  24.0   \n",
      "\n",
      "                       141   249    138     90  \n",
      "date                                            \n",
      "2012-06-16 12:00:00  121.0 -96.0 -147.0  -70.0  \n",
      "2012-06-16 13:00:00  117.0 -21.0   77.0  -26.0  \n",
      "2012-06-16 14:00:00  105.0 -14.0   21.0 -119.0  \n",
      "2012-06-16 15:00:00  -37.0 -30.0  -62.0  -13.0  \n",
      "2012-06-16 16:00:00  -32.0 -53.0 -180.0  -29.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-06-16 17:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.13787760480439]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230     79    236    162    170   234  \\\n",
      "date                                                                        \n",
      "2012-06-16 17:00:00   76.0  17.0 -137.0   16.0   -8.0   31.0  -15.0  66.0   \n",
      "2012-06-16 18:00:00  -82.0 -79.0   73.0  -77.0  105.0  -95.0  156.0  17.0   \n",
      "2012-06-16 19:00:00   -7.0 -17.0    4.0   31.0  124.0  135.0 -150.0 -56.0   \n",
      "2012-06-16 20:00:00   14.0 -65.0   64.0   12.0 -108.0  110.0    3.0 -65.0   \n",
      "2012-06-16 21:00:00  175.0  -2.0   -5.0 -158.0  103.0  128.0   46.0 -81.0   \n",
      "\n",
      "                        48    186    142   107   163    68    239   164   141  \\\n",
      "date                                                                            \n",
      "2012-06-16 17:00:00   28.0 -285.0 -195.0 -44.0  75.0 -89.0   94.0 -94.0  40.0   \n",
      "2012-06-16 18:00:00 -119.0 -213.0   94.0 -42.0 -15.0  14.0   53.0 -31.0 -11.0   \n",
      "2012-06-16 19:00:00  -74.0 -532.0   47.0 -68.0  64.0 -68.0   57.0 -81.0 -28.0   \n",
      "2012-06-16 20:00:00    2.0 -322.0   30.0 -24.0 -31.0  30.0   69.0 -67.0  61.0   \n",
      "2012-06-16 21:00:00   35.0   33.0 -175.0 -54.0  38.0  28.0  111.0 -18.0  57.0   \n",
      "\n",
      "                       249    138     90  \n",
      "date                                      \n",
      "2012-06-16 17:00:00   65.0   22.0   33.0  \n",
      "2012-06-16 18:00:00   79.0  252.0 -117.0  \n",
      "2012-06-16 19:00:00  -36.0   -4.0   34.0  \n",
      "2012-06-16 20:00:00  -76.0    4.0  -55.0  \n",
      "2012-06-16 21:00:00  194.0  -43.0  -56.0  \n",
      "## Tail of streaming_df:                         237   161    230     79    236    162   170    234  \\\n",
      "date                                                                        \n",
      "2012-06-23 13:00:00   43.0  79.0  -26.0   -2.0 -156.0   86.0  52.0  172.0   \n",
      "2012-06-23 14:00:00  132.0   5.0   78.0  145.0  125.0   22.0  59.0  134.0   \n",
      "2012-06-23 15:00:00  -26.0 -54.0 -196.0   -6.0  -11.0  -52.0  -8.0    6.0   \n",
      "2012-06-23 16:00:00 -135.0 -61.0 -167.0   97.0 -157.0  -71.0   1.0 -113.0   \n",
      "2012-06-23 17:00:00   21.0  93.0   83.0 -201.0 -113.0  144.0 -13.0 -113.0   \n",
      "\n",
      "                        48    186    142    107   163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-06-23 13:00:00  -18.0  -35.0  -87.0 -214.0  76.0  107.0  -27.0   93.0   \n",
      "2012-06-23 14:00:00  199.0  135.0   61.0    1.0  80.0  142.0   74.0   36.0   \n",
      "2012-06-23 15:00:00  -71.0  -73.0  -31.0   60.0 -45.0    1.0   10.0    2.0   \n",
      "2012-06-23 16:00:00  -54.0 -211.0 -185.0 -125.0   6.0   25.0  -65.0 -180.0   \n",
      "2012-06-23 17:00:00 -288.0  -89.0 -219.0   74.0 -13.0  -59.0 -227.0   35.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-06-23 13:00:00 -229.0  140.0  263.0  172.0  \n",
      "2012-06-23 14:00:00  -28.0   37.0  211.0  158.0  \n",
      "2012-06-23 15:00:00   48.0   16.0  335.0   32.0  \n",
      "2012-06-23 16:00:00  -80.0   72.0  268.0   27.0  \n",
      "2012-06-23 17:00:00 -117.0 -151.0  -48.0   64.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-06-23 18:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 65us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.3330726615189]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170   234  \\\n",
      "date                                                                         \n",
      "2012-06-23 18:00:00  134.0   70.0 -182.0   85.0   41.0   20.0 -144.0  38.0   \n",
      "2012-06-23 19:00:00   94.0  -48.0  -81.0 -134.0  111.0  150.0   96.0 -24.0   \n",
      "2012-06-23 20:00:00   -4.0  153.0 -205.0 -273.0   57.0  -74.0   65.0 -59.0   \n",
      "2012-06-23 21:00:00  -32.0  -45.0 -164.0  -62.0   -3.0    2.0  129.0 -76.0   \n",
      "2012-06-23 22:00:00   71.0  160.0 -114.0   -8.0   89.0  144.0  405.0 -39.0   \n",
      "\n",
      "                        48   186    142    107    163    68    239    164  \\\n",
      "date                                                                        \n",
      "2012-06-23 18:00:00 -142.0 -61.0  -37.0   86.0   69.0 -30.0   26.0   -5.0   \n",
      "2012-06-23 19:00:00   64.0 -65.0    5.0  -70.0   12.0  49.0  -21.0   35.0   \n",
      "2012-06-23 20:00:00  -38.0   4.0 -134.0  -55.0  -70.0  22.0  -58.0   56.0   \n",
      "2012-06-23 21:00:00  -67.0  25.0  -59.0    9.0 -105.0  31.0  -11.0   13.0   \n",
      "2012-06-23 22:00:00 -109.0 -64.0 -170.0  208.0  -74.0  15.0  314.0  179.0   \n",
      "\n",
      "                      141   249    138     90  \n",
      "date                                           \n",
      "2012-06-23 18:00:00 -14.0 -59.0  107.0  123.0  \n",
      "2012-06-23 19:00:00  29.0  34.0  113.0  -12.0  \n",
      "2012-06-23 20:00:00  -8.0 -16.0  184.0   43.0  \n",
      "2012-06-23 21:00:00 -83.0 -88.0  165.0  143.0  \n",
      "2012-06-23 22:00:00 -20.0 -83.0   -4.0  -16.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162   170    234  \\\n",
      "date                                                                         \n",
      "2012-06-30 14:00:00  -87.0 -104.0  -94.0  -69.0  -93.0  -83.0  -4.0  -48.0   \n",
      "2012-06-30 15:00:00   94.0   59.0  217.0   18.0   -3.0   41.0  24.0   32.0   \n",
      "2012-06-30 16:00:00  206.0  172.0  122.0  -27.0    8.0  158.0   2.0  175.0   \n",
      "2012-06-30 17:00:00 -175.0 -180.0   -5.0  150.0  111.0 -134.0 -46.0  -42.0   \n",
      "2012-06-30 18:00:00 -165.0 -181.0   41.0 -132.0   18.0  -90.0 -94.0 -163.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-06-30 14:00:00  -71.0  -66.0  -10.0   13.0  -66.0 -172.0  -36.0    0.0   \n",
      "2012-06-30 15:00:00   44.0  -16.0   87.0    7.0   63.0 -119.0   21.0   69.0   \n",
      "2012-06-30 16:00:00  122.0  299.0  195.0   15.0  182.0  -23.0   22.0  186.0   \n",
      "2012-06-30 17:00:00  195.0  132.0  205.0 -120.0  -54.0  -64.0  119.0    5.0   \n",
      "2012-06-30 18:00:00  -45.0   30.0  -79.0  -97.0  -89.0  -91.0  -55.0 -120.0   \n",
      "\n",
      "                       141    249    138    90  \n",
      "date                                            \n",
      "2012-06-30 14:00:00  -66.0  -14.0 -294.0 -81.0  \n",
      "2012-06-30 15:00:00   39.0   18.0 -239.0 -27.0  \n",
      "2012-06-30 16:00:00  -16.0 -139.0 -196.0  50.0  \n",
      "2012-06-30 17:00:00   55.0  121.0   71.0 -78.0  \n",
      "2012-06-30 18:00:00 -115.0  -76.0 -349.0 -54.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-06-30 19:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 68us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[71.3018936894146]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-06-30 19:00:00  -85.0    4.0  -50.0   -2.0 -145.0 -149.0  -83.0   15.0   \n",
      "2012-06-30 20:00:00  -12.0 -124.0  102.0  367.0  -18.0   37.0 -154.0  102.0   \n",
      "2012-06-30 21:00:00 -100.0    4.0  259.0   52.0 -107.0   -7.0 -197.0    0.0   \n",
      "2012-06-30 22:00:00  -96.0 -105.0   17.0   27.0 -114.0 -118.0 -343.0    8.0   \n",
      "2012-06-30 23:00:00  -98.0  -95.0   63.0   14.0  -35.0 -181.0 -362.0  -15.0   \n",
      "\n",
      "                        48    186    142    107   163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-06-30 19:00:00 -119.0  -57.0 -139.0   26.0 -88.0 -140.0  -71.0  -47.0   \n",
      "2012-06-30 20:00:00 -126.0  -33.0  -11.0   34.0  43.0  -25.0 -102.0  -82.0   \n",
      "2012-06-30 21:00:00  -59.0  -52.0   56.0  -41.0  67.0  -70.0  -93.0  -11.0   \n",
      "2012-06-30 22:00:00   68.0  107.0   69.0 -231.0 -55.0  -74.0 -257.0 -131.0   \n",
      "2012-06-30 23:00:00 -217.0 -134.0  -41.0 -140.0 -73.0 -130.0  -89.0 -103.0   \n",
      "\n",
      "                       141    249    138    90  \n",
      "date                                            \n",
      "2012-06-30 19:00:00 -124.0 -130.0  -25.0 -50.0  \n",
      "2012-06-30 20:00:00  -10.0  111.0 -213.0  30.0  \n",
      "2012-06-30 21:00:00   -9.0    7.0 -101.0 -87.0  \n",
      "2012-06-30 22:00:00 -110.0   63.0 -164.0 -34.0  \n",
      "2012-06-30 23:00:00  -67.0  169.0  309.0 -54.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-07-07 15:00:00 -204.0 -232.0 -183.0  -37.0   22.0 -280.0 -172.0 -256.0   \n",
      "2012-07-07 16:00:00 -137.0 -432.0  -89.0  -64.0  118.0 -553.0 -303.0 -339.0   \n",
      "2012-07-07 17:00:00  192.0 -587.0  -64.0   36.0  108.0 -184.0  -54.0  -62.0   \n",
      "2012-07-07 18:00:00  205.0 -174.0  149.0  121.0   78.0  -78.0   98.0   58.0   \n",
      "2012-07-07 19:00:00   76.0  181.0   91.0  164.0  121.0   88.0  197.0   80.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-07-07 15:00:00 -114.0  -14.0  -66.0 -155.0 -147.0  -31.0   -2.0 -161.0   \n",
      "2012-07-07 16:00:00 -188.0 -224.0  -17.0  -37.0 -312.0 -178.0   30.0 -190.0   \n",
      "2012-07-07 17:00:00   30.0 -234.0   96.0  133.0  -53.0   49.0  215.0 -223.0   \n",
      "2012-07-07 18:00:00  172.0   80.0   53.0  157.0  105.0  121.0   91.0   13.0   \n",
      "2012-07-07 19:00:00  163.0  227.0  185.0  184.0   13.0  109.0   77.0  -29.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-07-07 15:00:00 -168.0  -92.0  289.0 -118.0  \n",
      "2012-07-07 16:00:00   26.0    3.0  127.0 -167.0  \n",
      "2012-07-07 17:00:00  124.0   38.0  208.0 -127.0  \n",
      "2012-07-07 18:00:00  131.0  211.0  539.0   42.0  \n",
      "2012-07-07 19:00:00  211.0  177.0  139.0  150.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-07-07 20:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 69us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[58.52238096955201]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-07-07 20:00:00  160.0   77.0   78.0 -114.0   36.0  159.0  234.0   55.0   \n",
      "2012-07-07 21:00:00   97.0   76.0 -260.0   24.0  126.0   70.0   78.0    7.0   \n",
      "2012-07-07 22:00:00  158.0  110.0   46.0   60.0  156.0  135.0  216.0   37.0   \n",
      "2012-07-07 23:00:00   52.0  117.0  255.0   97.0   27.0  188.0  137.0  116.0   \n",
      "2012-07-08 00:00:00    8.0   88.0  -60.0 -207.0   19.0  124.0  204.0   55.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-07-07 20:00:00  266.0  225.0   67.0  115.0   98.0   63.0  122.0   67.0   \n",
      "2012-07-07 21:00:00  114.0   13.0 -150.0  186.0   33.0 -109.0   35.0  -69.0   \n",
      "2012-07-07 22:00:00  293.0   39.0  323.0  234.0  175.0   66.0  204.0  169.0   \n",
      "2012-07-07 23:00:00  364.0  147.0  200.0  151.0  167.0    5.0   87.0  177.0   \n",
      "2012-07-08 00:00:00   11.0   59.0   74.0  190.0   26.0 -189.0   39.0   88.0   \n",
      "\n",
      "                       141    249    138    90  \n",
      "date                                            \n",
      "2012-07-07 20:00:00   76.0  -36.0  287.0 -49.0  \n",
      "2012-07-07 21:00:00   54.0  -36.0  284.0  56.0  \n",
      "2012-07-07 22:00:00  224.0 -197.0  329.0  56.0  \n",
      "2012-07-07 23:00:00  191.0 -191.0  101.0   9.0  \n",
      "2012-07-08 00:00:00  133.0 -118.0   38.0  37.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2012-07-14 16:00:00 -129.0  402.0   67.0   32.0    8.0  159.0  176.0  262.0   \n",
      "2012-07-14 17:00:00   17.0  257.0  -73.0   -5.0  -36.0 -125.0  -14.0  -34.0   \n",
      "2012-07-14 18:00:00  -88.0  -43.0 -150.0  -91.0  -56.0 -135.0 -164.0   32.0   \n",
      "2012-07-14 19:00:00   99.0 -173.0  -59.0    0.0  -34.0 -113.0  -89.0  -86.0   \n",
      "2012-07-14 20:00:00 -197.0   71.0  -87.0  150.0 -165.0 -178.0 -132.0 -100.0   \n",
      "\n",
      "                        48    186    142   107    163    68    239    164  \\\n",
      "date                                                                        \n",
      "2012-07-14 16:00:00   21.0   43.0 -169.0  45.0  101.0  86.0   29.0  166.0   \n",
      "2012-07-14 17:00:00  -90.0   66.0  -37.0 -85.0 -148.0   8.0 -157.0  195.0   \n",
      "2012-07-14 18:00:00 -149.0 -216.0 -111.0  18.0  -94.0 -32.0  -57.0   31.0   \n",
      "2012-07-14 19:00:00  -94.0 -140.0  -36.0 -20.0   60.0  15.0  -22.0    0.0   \n",
      "2012-07-14 20:00:00  -81.0 -161.0  -50.0 -64.0  -73.0 -21.0 -208.0  -36.0   \n",
      "\n",
      "                      141    249    138    90  \n",
      "date                                           \n",
      "2012-07-14 16:00:00  29.0   88.0  -25.0  74.0  \n",
      "2012-07-14 17:00:00 -51.0  -55.0 -190.0  20.0  \n",
      "2012-07-14 18:00:00 -31.0 -173.0 -411.0 -90.0  \n",
      "2012-07-14 19:00:00 -56.0  -80.0 -113.0 -71.0  \n",
      "2012-07-14 20:00:00 -76.0   -6.0 -289.0  17.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-07-14 21:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 81us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[61.85375616951668]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                         237   161    230     79    236    162    170   234  \\\n",
      "date                                                                        \n",
      "2012-07-14 21:00:00  -34.0 -27.0   -4.0  275.0 -288.0   37.0  -13.0  52.0   \n",
      "2012-07-14 22:00:00 -118.0 -55.0  -89.0  232.0 -263.0 -112.0 -103.0 -89.0   \n",
      "2012-07-14 23:00:00   -5.0 -39.0 -203.0  126.0  -82.0 -107.0  -63.0 -30.0   \n",
      "2012-07-15 00:00:00   27.0 -54.0   58.0  362.0  -16.0 -120.0 -156.0  83.0   \n",
      "2012-07-15 01:00:00   -1.0  13.0  168.0  318.0   -5.0   44.0   -9.0 -49.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-07-14 21:00:00  -26.0  -12.0  253.0 -138.0   25.0  252.0 -214.0   45.0   \n",
      "2012-07-14 22:00:00 -297.0 -127.0 -288.0 -189.0 -135.0   46.0 -206.0 -103.0   \n",
      "2012-07-14 23:00:00 -380.0  -45.0 -183.0 -118.0 -116.0  226.0 -202.0 -109.0   \n",
      "2012-07-15 00:00:00    0.0 -129.0  -85.0 -118.0   -2.0  280.0  -42.0   97.0   \n",
      "2012-07-15 01:00:00   98.0  -34.0  -41.0 -119.0   11.0  300.0   36.0  120.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-07-14 21:00:00   20.0   37.0 -196.0   23.0  \n",
      "2012-07-14 22:00:00  -29.0   70.0 -152.0 -122.0  \n",
      "2012-07-14 23:00:00 -175.0 -113.0 -110.0  131.0  \n",
      "2012-07-15 00:00:00 -106.0   37.0  -29.0   54.0  \n",
      "2012-07-15 01:00:00   12.0  192.0    6.0  102.0  \n",
      "## Tail of streaming_df:                        237    161    230     79    236    162   170   234  \\\n",
      "date                                                                       \n",
      "2012-07-21 17:00:00 -60.0  254.0  185.0   28.0  -41.0  142.0 -69.0  73.0   \n",
      "2012-07-21 18:00:00 -16.0  350.0  -29.0   20.0   17.0  216.0  14.0 -17.0   \n",
      "2012-07-21 19:00:00 -72.0  301.0   29.0 -137.0  -30.0  277.0  37.0   9.0   \n",
      "2012-07-21 20:00:00  87.0  -43.0  -77.0  -48.0  220.0  -10.0 -16.0  93.0   \n",
      "2012-07-21 21:00:00  68.0  -59.0  -68.0 -126.0  330.0  -38.0 -24.0  57.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-07-21 17:00:00  184.0  181.0  -46.0  -30.0  228.0  105.0   84.0  -57.0   \n",
      "2012-07-21 18:00:00 -123.0    6.0  -81.0 -169.0  -13.0   83.0  -26.0  -32.0   \n",
      "2012-07-21 19:00:00    6.0   15.0  -68.0  -51.0  -58.0   17.0   38.0  156.0   \n",
      "2012-07-21 20:00:00   -8.0   47.0   53.0    4.0   78.0   94.0  237.0   63.0   \n",
      "2012-07-21 21:00:00  -34.0  -81.0 -193.0  129.0   23.0  -23.0  266.0   34.0   \n",
      "\n",
      "                      141   249    138    90  \n",
      "date                                          \n",
      "2012-07-21 17:00:00  76.0 -65.0   18.0  53.0  \n",
      "2012-07-21 18:00:00 -35.0  25.0   54.0  56.0  \n",
      "2012-07-21 19:00:00  29.0  60.0  117.0  27.0  \n",
      "2012-07-21 20:00:00  35.0 -20.0  -60.0  -9.0  \n",
      "2012-07-21 21:00:00  37.0  -3.0 -196.0   6.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-07-21 22:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 27us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 75us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[63.283880894999186]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161    230     79    236    162   170    234  \\\n",
      "date                                                                       \n",
      "2012-07-21 22:00:00  52.0 -66.0 -118.0 -142.0  272.0  -42.0  45.0  167.0   \n",
      "2012-07-21 23:00:00 -26.0  36.0  -49.0  -76.0   39.0  101.0  30.0    1.0   \n",
      "2012-07-22 00:00:00   4.0  58.0  -87.0 -354.0   11.0   52.0  15.0   79.0   \n",
      "2012-07-22 01:00:00  11.0 -16.0  -53.0 -117.0  -11.0  -41.0 -16.0   99.0   \n",
      "2012-07-22 02:00:00 -11.0  32.0  -11.0 -190.0    3.0    6.0  31.0  -25.0   \n",
      "\n",
      "                        48    186   142    107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2012-07-21 22:00:00 -116.0  141.0 -19.0  139.0  45.0    6.0  177.0  -28.0   \n",
      "2012-07-21 23:00:00   82.0  -74.0 -40.0   57.0  57.0  -66.0  131.0   97.0   \n",
      "2012-07-22 00:00:00  -33.0   86.0  58.0   81.0  29.0    9.0   10.0  112.0   \n",
      "2012-07-22 01:00:00   -6.0   22.0  -5.0   26.0 -40.0  -46.0  -24.0   54.0   \n",
      "2012-07-22 02:00:00   86.0   21.0 -56.0   36.0  18.0 -190.0   31.0  101.0   \n",
      "\n",
      "                      141    249    138    90  \n",
      "date                                           \n",
      "2012-07-21 22:00:00 -45.0    2.0 -269.0  92.0  \n",
      "2012-07-21 23:00:00  85.0  165.0   65.0 -54.0  \n",
      "2012-07-22 00:00:00  38.0  105.0  -46.0 -14.0  \n",
      "2012-07-22 01:00:00  28.0  -52.0  -36.0 -22.0  \n",
      "2012-07-22 02:00:00 -44.0 -159.0    6.0   3.0  \n",
      "## Tail of streaming_df:                        237    161    230     79    236    162    170    234  \\\n",
      "date                                                                         \n",
      "2012-07-28 18:00:00  24.0 -411.0   69.0  -87.0   45.0 -128.0  177.0  -41.0   \n",
      "2012-07-28 19:00:00 -16.0    4.0   18.0   13.0  101.0  -52.0  -27.0   67.0   \n",
      "2012-07-28 20:00:00  89.0   65.0  126.0   77.0   59.0   74.0   18.0   50.0   \n",
      "2012-07-28 21:00:00  30.0   44.0 -107.0   18.0   19.0   55.0   17.0    9.0   \n",
      "2012-07-28 22:00:00 -22.0  -10.0   21.0 -264.0  -15.0   85.0  -39.0 -180.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2012-07-28 18:00:00  257.0  129.0  190.0    8.0   52.0 -156.0 -119.0   54.0   \n",
      "2012-07-28 19:00:00   56.0  337.0   62.0  -18.0  103.0  -23.0  -24.0 -126.0   \n",
      "2012-07-28 20:00:00   82.0   24.0  197.0    1.0  -71.0  -14.0  -28.0  -89.0   \n",
      "2012-07-28 21:00:00   18.0  -72.0   88.0  -97.0  -18.0 -122.0  -89.0 -225.0   \n",
      "2012-07-28 22:00:00  124.0 -207.0 -121.0 -117.0  -20.0 -124.0  -75.0 -155.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2012-07-28 18:00:00   66.0   10.0   67.0  -12.0  \n",
      "2012-07-28 19:00:00  -34.0  -57.0  108.0  -22.0  \n",
      "2012-07-28 20:00:00  100.0  107.0  282.0   68.0  \n",
      "2012-07-28 21:00:00  -43.0 -115.0  317.0 -133.0  \n",
      "2012-07-28 22:00:00  -84.0  102.0  184.0  -55.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-07-28 23:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "3380/3380 [==============================] - 0s 26us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 67us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[56.36810571139578]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237    161   230     79   236    162    170   234  \\\n",
      "date                                                                      \n",
      "2012-07-28 23:00:00 -50.0 -164.0  14.0 -194.0 -22.0 -147.0  -58.0  84.0   \n",
      "2012-07-29 00:00:00 -43.0  -82.0 -25.0   49.0 -66.0  -75.0 -191.0 -69.0   \n",
      "2012-07-29 01:00:00 -58.0   51.0  16.0  -33.0 -10.0   -9.0  -16.0  17.0   \n",
      "2012-07-29 02:00:00 -24.0  -25.0  28.0  -78.0   8.0  -30.0  -25.0  17.0   \n",
      "2012-07-29 03:00:00  -4.0  -35.0 -71.0  -91.0  12.0  -41.0  -11.0 -37.0   \n",
      "\n",
      "                        48   186    142    107    163     68    239    164  \\\n",
      "date                                                                         \n",
      "2012-07-28 23:00:00 -154.0 -19.0 -119.0 -182.0 -127.0 -318.0 -101.0 -225.0   \n",
      "2012-07-29 00:00:00 -175.0 -38.0  -78.0 -135.0    7.0 -299.0  -83.0 -238.0   \n",
      "2012-07-29 01:00:00 -133.0 -24.0   22.0    2.0   27.0 -365.0   -9.0  -99.0   \n",
      "2012-07-29 02:00:00  -63.0   9.0   15.0   13.0  -57.0 -114.0  -16.0 -127.0   \n",
      "2012-07-29 03:00:00  -24.0  19.0  -39.0    6.0   18.0  -97.0  -26.0  -68.0   \n",
      "\n",
      "                      141    249   138     90  \n",
      "date                                           \n",
      "2012-07-28 23:00:00 -90.0  231.0 -64.0 -186.0  \n",
      "2012-07-29 00:00:00 -74.0  -49.0  28.0 -110.0  \n",
      "2012-07-29 01:00:00 -84.0  -75.0  34.0 -136.0  \n",
      "2012-07-29 02:00:00   4.0   -2.0  -9.0  -79.0  \n",
      "2012-07-29 03:00:00 -25.0 -127.0  -4.0   45.0  \n",
      "## Tail of streaming_df:                        237    161    230     79   236   162    170    234  \\\n",
      "date                                                                       \n",
      "2012-08-04 19:00:00  41.0   -6.0   56.0  -26.0  55.0  90.0   13.0    8.0   \n",
      "2012-08-04 20:00:00 -63.0 -107.0  -12.0 -143.0 -76.0  94.0  -59.0  -49.0   \n",
      "2012-08-04 21:00:00 -10.0   31.0   70.0  -99.0 -52.0   5.0    7.0  -76.0   \n",
      "2012-08-04 22:00:00  18.0   54.0  145.0  173.0 -36.0  -5.0 -150.0  163.0   \n",
      "2012-08-04 23:00:00  81.0  134.0  -28.0   49.0  23.0   3.0  -74.0 -121.0   \n",
      "\n",
      "                        48    186    142   107   163     68    239    164  \\\n",
      "date                                                                        \n",
      "2012-08-04 19:00:00    2.0 -203.0  -51.0 -38.0   2.0   -8.0   76.0   30.0   \n",
      "2012-08-04 20:00:00   -1.0  -60.0 -156.0 -52.0  93.0  -58.0    1.0   31.0   \n",
      "2012-08-04 21:00:00  115.0  123.0   37.0  16.0  -3.0  105.0   56.0  191.0   \n",
      "2012-08-04 22:00:00   49.0  186.0   68.0   8.0 -13.0    3.0 -142.0  151.0   \n",
      "2012-08-04 23:00:00  136.0   12.0  153.0  88.0  64.0  163.0   71.0  117.0   \n",
      "\n",
      "                      141    249    138     90  \n",
      "date                                            \n",
      "2012-08-04 19:00:00  23.0   43.0 -339.0  -26.0  \n",
      "2012-08-04 20:00:00 -71.0 -161.0 -230.0  -35.0  \n",
      "2012-08-04 21:00:00  62.0   89.0 -117.0   51.0  \n",
      "2012-08-04 22:00:00  24.0   27.0  -19.0    2.0  \n",
      "2012-08-04 23:00:00  24.0 -164.0  151.0  138.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      " ++ Number of days contained in train_set used for scaling:  759\n",
      "selected years for training:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00')]\n",
      "year_list given:  [Timestamp('2009-08-12 22:00:00'), Timestamp('2011-09-09 23:00:00'), Timestamp('2012-08-05 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3380/3380 [==============================] - 0s 25us/step\n",
      "Shape of org. dataset after shift:  (169, 20)\n",
      "20/20 [==============================] - 0s 66us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[51.79386427960477]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (169, 20)\n",
      "## Head of streaming_df:                        237   161   230     79   236   162    170   234     48  \\\n",
      "date                                                                           \n",
      "2012-08-05 00:00:00  -4.0  55.0   0.0 -179.0  52.0  25.0  192.0  99.0  173.0   \n",
      "2012-08-05 01:00:00  14.0 -85.0 -33.0 -248.0   2.0 -54.0   26.0  20.0  110.0   \n",
      "2012-08-05 02:00:00  14.0  -1.0   4.0  -72.0  -7.0  18.0  -22.0  48.0   49.0   \n",
      "2012-08-05 03:00:00 -23.0  -8.0  76.0   37.0  -8.0  27.0  -23.0   1.0   88.0   \n",
      "2012-08-05 04:00:00  -7.0 -13.0 -23.0  -65.0  17.0  -7.0  -50.0 -25.0  -11.0   \n",
      "\n",
      "                      186   142   107   163     68   239   164   141   249  \\\n",
      "date                                                                         \n",
      "2012-08-05 00:00:00  42.0  50.0  82.0 -41.0  228.0  83.0  68.0  70.0 -21.0   \n",
      "2012-08-05 01:00:00 -65.0  32.0  50.0 -12.0  313.0 -23.0  -7.0  35.0  23.0   \n",
      "2012-08-05 02:00:00 -71.0  25.0  12.0 -12.0  245.0 -47.0  24.0  -2.0  45.0   \n",
      "2012-08-05 03:00:00 -38.0  -1.0  -8.0  40.0   45.0   9.0 -46.0  -7.0  45.0   \n",
      "2012-08-05 04:00:00  19.0  -1.0 -32.0  -7.0  -34.0 -18.0  31.0 -22.0  -9.0   \n",
      "\n",
      "                      138     90  \n",
      "date                              \n",
      "2012-08-05 00:00:00  37.0  117.0  \n",
      "2012-08-05 01:00:00  -8.0  171.0  \n",
      "2012-08-05 02:00:00  -3.0   45.0  \n",
      "2012-08-05 03:00:00 -13.0  -18.0  \n",
      "2012-08-05 04:00:00  -1.0  -32.0  \n",
      "## Tail of streaming_df:                         237   161    230     79   236   162    170    234  \\\n",
      "date                                                                       \n",
      "2012-08-11 20:00:00  -34.0  55.0  -20.0   86.0 -12.0 -68.0   54.0    7.0   \n",
      "2012-08-11 21:00:00 -101.0  -3.0  -91.0   80.0  28.0   3.0 -105.0  -71.0   \n",
      "2012-08-11 22:00:00  -33.0  -7.0 -201.0 -140.0  15.0 -42.0   30.0 -208.0   \n",
      "2012-08-11 23:00:00   -6.0 -36.0  -60.0  -49.0  64.0   0.0  158.0  -57.0   \n",
      "2012-08-12 00:00:00   25.0 -69.0    7.0  108.0  15.0  41.0  112.0 -217.0   \n",
      "\n",
      "                       48    186    142   107   163     68    239   164   141  \\\n",
      "date                                                                            \n",
      "2012-08-11 20:00:00 -51.0  -87.0   46.0 -50.0 -69.0  -22.0  308.0 -18.0  38.0   \n",
      "2012-08-11 21:00:00 -98.0 -135.0   26.0  36.0 -87.0  -55.0  281.0 -78.0 -58.0   \n",
      "2012-08-11 22:00:00  43.0   35.0  136.0 -38.0 -14.0   91.0  356.0 -51.0  -1.0   \n",
      "2012-08-11 23:00:00 -36.0  197.0  141.0 -43.0  -5.0  -98.0  157.0 -37.0  47.0   \n",
      "2012-08-12 00:00:00  -2.0   25.0   53.0  82.0 -24.0 -177.0   32.0   2.0 -50.0   \n",
      "\n",
      "                       249    138     90  \n",
      "date                                      \n",
      "2012-08-11 20:00:00   -7.0  -17.0   33.0  \n",
      "2012-08-11 21:00:00  -86.0 -116.0   57.0  \n",
      "2012-08-11 22:00:00 -148.0 -158.0    5.0  \n",
      "2012-08-11 23:00:00 -172.0 -260.0 -128.0  \n",
      "2012-08-12 00:00:00  -96.0 -140.0 -115.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "current detectors are re-used since no drifts were detected...\n",
      "pre-defined detectors are used...\n",
      "## Change detected in area 170, index: 143\n",
      "date: 2012-08-10 23:00:00\n",
      "Drift detected at:  2012-08-10 23:00:00\n",
      ">> Current Time:  23/01/2020 13:49:05\n",
      " ->> update_weights_flag set to \"False\" , delta of drift dates: -336\n",
      " >> delta of last start trainset & current drift:  -1318\n",
      "## ++ previous detected dates:  [Timestamp('2011-09-09 23:00:00'), Timestamp('2012-08-10 23:00:00')]\n",
      "## ++ last training dates:  [Timestamp('2009-01-01 00:00:00')]\n",
      " ++ Number of days contained in train_set used for scaling/retraining:  731\n",
      "#### Current dates: \n",
      "#### training_start_date:  2010-08-10 23:00:00\n",
      "#### start_valid_set:  None\n",
      "#### start_test_set:  None\n",
      "### ### New Model is trained\n",
      "selected years for training:  [Timestamp('2010-08-10 23:00:00'), Timestamp('2012-08-10 23:00:00')]\n",
      "year_list given:  [Timestamp('2010-08-10 23:00:00'), Timestamp('2012-08-10 23:00:00'), None, None]\n",
      "#### Train model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff_count39__trainsize731_s8_2010_e8_2012__stepsize1__p12_2012 ####\n",
      ">> Dates are assigned...\n",
      ">date_valid:  None\n",
      ">date_test:  None\n",
      "No predictions are made, model is only retrained or weights are updated\n",
      ">> No preds are returned, only training history & model\n",
      ">start_train_year:  2010-08-10 23:00:00\n",
      ">last_train_set_year:  2012-08-10 23:00:00\n",
      "start_validation_set_year:  2012-08-10 23:00:00\n",
      "end_validation_set_year:  2012-08-10 23:00:00\n",
      "start_test_set_year:  2012-08-10 23:00:00\n",
      "end_test_set_year:  2012-08-10 23:00:00\n",
      "#params are overwritten\n",
      "## New Model is created, old model is discarded..\n",
      "generate data..\n",
      "start_train_year:  2010-08-10 23:00:00\n",
      "last_train_set_year:  2012-08-10 23:00:00\n",
      "start_validation_set_year:  2012-08-10 23:00:00\n",
      "start_test_set_year:  2012-08-10 23:00:00\n",
      "end_validation_set_year:  2012-08-10 23:00:00\n",
      "end_test_set_year:  2012-08-10 23:00:00\n",
      "# adjusted dates..\n",
      "start_train_year:  2010-08-10 23:00:00\n",
      "last_train_set_year:  2012-08-10 23:00:00\n",
      "start_validation_set_year:  2012-08-10 23:00:00\n",
      "start_test_set_year:  2012-08-10 23:00:00\n",
      "end_validation_set_year:  2012-08-10 23:00:00\n",
      "end_test_set_year:  2012-08-10 23:00:00\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "Data is scaled...\n",
      "Standardizing used...\n",
      "X_train shape of area237 before concat with other areas:  (16872, 203)\n",
      "X_valid shape of area237 before concat with other areas:  (1, 203)\n",
      "X_test shape of area237 before concat with other areas:  (1, 203)\n",
      "y_train shape of area237 before concat with other areas:  (16872,)\n",
      "y_valid shape of area237 before concat with other areas:  (1,)\n",
      "y_test shape of area237 before concat with other areas:  (1,)\n",
      "final concatenated shape of X_train :  (337440, 203)\n",
      "create MLP Model:\n",
      "#Dropout applied\n",
      "#Clipping Norm applied\n",
      "Train on 337440 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "337440/337440 [==============================] - 7s 19us/step - loss: 0.3469 - mean_absolute_error: 0.4259 - val_loss: 0.2001 - val_mean_absolute_error: 0.3635\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 2/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2770 - mean_absolute_error: 0.3792 - val_loss: 0.1580 - val_mean_absolute_error: 0.3042\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 3/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2623 - mean_absolute_error: 0.3679 - val_loss: 0.1621 - val_mean_absolute_error: 0.3160\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 4/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2516 - mean_absolute_error: 0.3600 - val_loss: 0.1842 - val_mean_absolute_error: 0.3461\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 5/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2454 - mean_absolute_error: 0.3550 - val_loss: 0.1582 - val_mean_absolute_error: 0.3178\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 6/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2385 - mean_absolute_error: 0.3501 - val_loss: 0.1963 - val_mean_absolute_error: 0.3586\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 7/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2337 - mean_absolute_error: 0.3464 - val_loss: 0.1838 - val_mean_absolute_error: 0.3440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Current LearningRate:  0.001\n",
      "Epoch 8/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2311 - mean_absolute_error: 0.3445 - val_loss: 0.1604 - val_mean_absolute_error: 0.3154\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 9/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2282 - mean_absolute_error: 0.3425 - val_loss: 0.1667 - val_mean_absolute_error: 0.3191\n",
      "#Current LearningRate:  0.001\n",
      "Epoch 10/10\n",
      "337440/337440 [==============================] - 5s 14us/step - loss: 0.2255 - mean_absolute_error: 0.3406 - val_loss: 0.1805 - val_mean_absolute_error: 0.3311\n",
      "#Current LearningRate:  0.001\n",
      "## Only training history & model are returned\n",
      "## Predictions with retrained model are made..\n",
      ">> Current Number of weight updates based on Switching Scheme:  1\n",
      ">> Current Number of retrainings:  1\n",
      "# Very first predictions are made for next 168 days..\n",
      "## Assigned Dates are double checked..\n",
      "# >> end of dataset is reached with preds of valid_set --> get last predictions with model\n",
      "current valid date:  2012-08-11 00:00:00\n",
      "current test date:  None\n",
      " ++ Number of days contained in train_set used for scaling:  731\n",
      "selected years for training:  [Timestamp('2010-08-10 23:00:00'), Timestamp('2012-08-10 23:00:00')]\n",
      "year_list given:  [Timestamp('2010-08-10 23:00:00'), Timestamp('2012-08-10 23:00:00'), Timestamp('2012-08-11 00:00:00'), None]\n",
      "#### Make predictions model: complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff ####\n",
      "68640/68640 [==============================] - 3s 40us/step\n",
      "Shape of org. dataset after shift:  (3432, 20)\n",
      "20/20 [==============================] - 0s 69us/step\n",
      "Shape of org. dataset after shift:  (1, 20)\n",
      "## Avg. RMSE of recent predictions: \n",
      "[65.23846766977306]\n",
      "## converted_stream_flag used:  False\n",
      "## Shape of streaming_df:  (4033, 20)\n",
      "## Head of streaming_df:                        237   161   230     79   236   162   170    234    48  \\\n",
      "date                                                                          \n",
      "2012-08-11 00:00:00 -33.0   5.0 -48.0 -163.0 -35.0 -92.0 -18.0  185.0 -28.0   \n",
      "2012-08-11 01:00:00  -7.0   2.0  27.0 -176.0 -12.0 -40.0   7.0  225.0  61.0   \n",
      "2012-08-11 02:00:00   8.0  32.0  61.0  -71.0  -3.0  14.0 -48.0   97.0  98.0   \n",
      "2012-08-11 03:00:00  -8.0  16.0   8.0   17.0 -32.0  34.0 -12.0   39.0  11.0   \n",
      "2012-08-11 04:00:00 -26.0  -5.0 -18.0  -13.0  14.0 -43.0 -18.0    8.0  17.0   \n",
      "\n",
      "                      186   142    107   163     68   239   164   141   249  \\\n",
      "date                                                                          \n",
      "2012-08-11 00:00:00 -29.0 -40.0 -188.0   8.0  -32.0 -42.0  10.0  25.0  70.0   \n",
      "2012-08-11 01:00:00 -11.0  -7.0  -46.0 -28.0   90.0  32.0 -52.0  59.0  95.0   \n",
      "2012-08-11 02:00:00   6.0  45.0   36.0   2.0  161.0  11.0  -9.0  22.0  38.0   \n",
      "2012-08-11 03:00:00 -56.0 -12.0  -33.0  46.0  -12.0 -13.0   1.0  10.0  23.0   \n",
      "2012-08-11 04:00:00 -16.0  20.0    9.0  25.0   12.0  12.0  26.0 -19.0  42.0   \n",
      "\n",
      "                       138    90  \n",
      "date                              \n",
      "2012-08-11 00:00:00 -329.0  64.0  \n",
      "2012-08-11 01:00:00 -502.0   7.0  \n",
      "2012-08-11 02:00:00  -32.0  -2.0  \n",
      "2012-08-11 03:00:00   -9.0  47.0  \n",
      "2012-08-11 04:00:00  -11.0   7.0  \n",
      "## Tail of streaming_df:                         237    161    230     79    236    162    170    234  \\\n",
      "date                                                                          \n",
      "2013-01-25 20:00:00 -432.0 -332.0 -205.0  -76.0 -119.0 -538.0 -367.0 -268.0   \n",
      "2013-01-25 21:00:00 -284.0  -58.0    6.0 -156.0  -58.0 -134.0 -229.0 -207.0   \n",
      "2013-01-25 22:00:00 -264.0    9.0 -291.0   54.0  -91.0  -21.0 -224.0 -125.0   \n",
      "2013-01-25 23:00:00  -55.0  -19.0  -22.0 -170.0   12.0   18.0   -9.0 -337.0   \n",
      "2013-01-26 00:00:00   37.0   -5.0   25.0 -143.0   47.0  -58.0  -92.0  -43.0   \n",
      "\n",
      "                        48    186    142    107    163     68    239    164  \\\n",
      "date                                                                          \n",
      "2013-01-25 20:00:00 -134.0 -307.0 -121.0  -95.0  -94.0  -69.0   90.0 -295.0   \n",
      "2013-01-25 21:00:00  -76.0 -104.0 -157.0  -86.0  -87.0 -115.0 -175.0 -113.0   \n",
      "2013-01-25 22:00:00 -145.0 -266.0  -98.0 -132.0  -25.0 -158.0 -137.0  -81.0   \n",
      "2013-01-25 23:00:00   20.0 -113.0  -19.0  154.0  140.0 -198.0 -129.0 -130.0   \n",
      "2013-01-26 00:00:00   88.0  -44.0  -51.0  -64.0  -26.0  -36.0  -56.0  -57.0   \n",
      "\n",
      "                       141    249    138     90  \n",
      "date                                             \n",
      "2013-01-25 20:00:00 -119.0  -94.0 -268.0 -150.0  \n",
      "2013-01-25 21:00:00 -134.0 -121.0 -215.0  -40.0  \n",
      "2013-01-25 22:00:00  -65.0 -196.0 -299.0   37.0  \n",
      "2013-01-25 23:00:00   -4.0 -112.0  -71.0   73.0  \n",
      "2013-01-26 00:00:00   36.0 -442.0  349.0   35.0  \n",
      "## Start Drift Detection\n",
      "# sensitivity for detection:  1\n",
      "# sensitivity_type for detection:  monthly\n",
      "New drift detectors applied...\n",
      "new detectors are created for each area...\n",
      "> No drifts detected!\n",
      "Make preds for next 7 days...\n",
      "## Assigned Dates are double checked..\n",
      "# >>> End of dataset reached! Stop Predictions\n",
      "Stop streaming >> end of data set or end of predictions are reached\n",
      ">> Total Number of weight updates based on Switching Scheme:  1\n",
      ">> Total Number of retrainings:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/home/vincent/anaconda3/envs/deeplearning_gpu/lib/python3.6/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#set model_name based on used params:\n",
    "model_name = 'complex_MLP_2H_128_32_batch512_drop03_clip_norm_shuffle_scaling_std_W168_20largest_areas_live_drift_switch_backshift2_weight_range_w_update_r1__4_5_hdddm_diff'\n",
    "\n",
    "\n",
    "\n",
    "#create instance of class:\n",
    "cplxMLP_model_new = create_model_instance('ComplexMLP')\n",
    "\n",
    "#update instance with model from disk:\n",
    "prediction_model = load_pretrained_model_from_disk('ComplexMLP')\n",
    "cplxMLP_model_new.load_model(prediction_model)\n",
    "\n",
    "#set dataset for slicing:\n",
    "ts_series_input = ts_20largest.copy()\n",
    "\n",
    "\n",
    "\n",
    "#call function for drift detection & retraining:\n",
    "results_tuple_switch_backshift2_weight_range_w_updater1__4_5_hdddm_diff = dft.drift_detection_retraining(model_instance = cplxMLP_model_new, org_ts_series=ts_series_input, \n",
    "                                                    model_name = model_name, detector_type = 'HDDDM_diff', \n",
    "                                                    use_differenced_ts=True,\n",
    "                                                    update_retrain_switch=True, first_forecast_range_days=168,\n",
    "                                                    make_preds_with_weight_range = True,\n",
    "                                                    weight_update_backshift=2,\n",
    "                                                    weight_update_range = [3,4.5],\n",
    "                                                    n_epochs_retrain = 10, n_epochs_weight = 10,\n",
    "                                                    overwrite_params = True,\n",
    "                                                    end_of_dataset_date = '2012-12-31 23:00:00', \n",
    "                                                    verbosity = 2)\n",
    "                               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T15:00:39.371537Z",
     "start_time": "2019-09-29T15:00:35.696300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions stored on disk!\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n",
      "Saved model to disk\n",
      "Save history_df on disk done\n"
     ]
    }
   ],
   "source": [
    "#call function to store results:\n",
    "df_save_PATH = 'media/...'\n",
    "model_save_PATH = 'media/...'\n",
    "\n",
    "#call function to store results:\n",
    "_ = sv_files.store_retrained_drift_detection_results(results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[1], \n",
    "                                            results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[0], \n",
    "                                            df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "#call function to store models & history:\n",
    "#call function to store model:\n",
    "_ = sv_files.store_model_and_history_on_disk(results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[0], \n",
    "                                model_save_PATH, df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "## store dates:\n",
    "#call function to store dates at which change was detected:\n",
    "dates_df = sv_files.store_detected_change_dates(results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[2],\n",
    "                                           df_save_PATH)\n",
    "\n",
    "\n",
    "\n",
    "## store dates with dates as index:\n",
    "_ = sv_files.store_detected_change_dates_with_index(results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[2],\n",
    "                                           results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[4],\n",
    "                                           switch_updating_dates_list=results_tuple_switch_backshift2_weight_range_w_updater5__4_5_hdddm_diff[5],\n",
    "                                           df_store_PATH=df_save_PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
